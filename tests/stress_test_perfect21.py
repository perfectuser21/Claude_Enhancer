#!/usr/bin/env python3
"""
Perfect21 ç³»ç»Ÿå‹åŠ›æµ‹è¯•è„šæœ¬
å…¨é¢æµ‹è¯•Perfect21çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½å’Œæ€§èƒ½
"""

import os
import sys
import time
import json
import subprocess
import concurrent.futures
from typing import Dict, Any, List, Tuple
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥Perfect21ç»„ä»¶
from features.git_workflow.hooks_manager import GitHooksManager
from features.version_manager import get_global_version_manager, get_global_version_advisor
from features.git_workflow.plugins.plugin_manager import PluginManager

class Perfect21StressTest:
    """Perfect21ç³»ç»Ÿå‹åŠ›æµ‹è¯•å™¨"""

    def __init__(self):
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.test_results = {
            "start_time": time.time(),
            "tests": {},
            "summary": {},
            "errors": []
        }

        print("ğŸš€ Perfect21 ç³»ç»Ÿå‹åŠ›æµ‹è¯•å¯åŠ¨")
        print("=" * 60)

    def run_command_test(self, description: str, command: str, timeout: int = 30) -> Dict[str, Any]:
        """è¿è¡Œå‘½ä»¤æµ‹è¯•"""
        print(f"ğŸ§ª {description}")
        start_time = time.time()

        try:
            result = subprocess.run(
                command.split(),
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=timeout
            )

            execution_time = time.time() - start_time
            success = result.returncode == 0

            test_result = {
                "success": success,
                "execution_time": execution_time,
                "returncode": result.returncode,
                "stdout": result.stdout[:500],  # é™åˆ¶è¾“å‡ºé•¿åº¦
                "stderr": result.stderr[:500] if result.stderr else "",
                "timeout": timeout
            }

            status = "âœ…" if success else "âŒ"
            print(f"   {status} è€—æ—¶: {execution_time:.2f}s | çŠ¶æ€: {result.returncode}")

            return test_result

        except subprocess.TimeoutExpired:
            execution_time = time.time() - start_time
            print(f"   â° è¶…æ—¶: {timeout}s")
            return {
                "success": False,
                "execution_time": execution_time,
                "timeout": True,
                "error": f"Command timeout after {timeout}s"
            }
        except Exception as e:
            execution_time = time.time() - start_time
            print(f"   ğŸ’¥ å¼‚å¸¸: {str(e)}")
            return {
                "success": False,
                "execution_time": execution_time,
                "error": str(e)
            }

    def test_perfect21_commands(self):
        """æµ‹è¯•Perfect21ä¸»å‘½ä»¤"""
        print("\nğŸ“‹ æµ‹è¯•Perfect21ä¸»å‘½ä»¤")
        print("-" * 40)

        commands = [
            ("Perfect21å¸®åŠ©ä¿¡æ¯", "./perfect21 --help"),
            ("Perfect21ç³»ç»ŸçŠ¶æ€", "./perfect21 --status"),
            ("Git HooksçŠ¶æ€", "python3 main/cli.py hooks status"),
            ("å·¥ä½œæµçŠ¶æ€", "python3 main/cli.py workflow list"),
            ("åˆ†æ”¯çŠ¶æ€", "python3 main/cli.py branch status"),
        ]

        results = {}
        for description, command in commands:
            results[description] = self.run_command_test(description, command)

        self.test_results["tests"]["perfect21_commands"] = results

    def test_hooks_system(self):
        """æµ‹è¯•Git Hooksç³»ç»Ÿ"""
        print("\nğŸ”§ æµ‹è¯•Git Hooksç³»ç»Ÿ")
        print("-" * 40)

        try:
            # æµ‹è¯•GitHooksManageråˆå§‹åŒ–
            print("ğŸ§ª GitHooksManageråˆå§‹åŒ–æµ‹è¯•")
            start_time = time.time()

            hm = GitHooksManager()
            init_time = time.time() - start_time

            print(f"   âœ… åˆå§‹åŒ–æˆåŠŸ: {init_time:.2f}s")

            # æµ‹è¯•é…ç½®åŠ è½½
            print("ğŸ§ª é…ç½®ç³»ç»Ÿæµ‹è¯•")
            config_summary = hm.get_config_summary()
            print(f"   âœ… é…ç½®åŠ è½½æˆåŠŸ: {len(config_summary)} å­—ç¬¦")

            # æµ‹è¯•æ’ä»¶ç³»ç»Ÿ
            print("ğŸ§ª æ’ä»¶ç³»ç»Ÿæµ‹è¯•")
            plugin_status = hm.get_plugin_status()
            plugin_count = plugin_status['total_plugins']
            enabled_count = len(plugin_status['enabled_plugins'])

            print(f"   âœ… æ’ä»¶ç³»ç»Ÿ: {enabled_count}/{plugin_count} æ’ä»¶å¯ç”¨")

            # æµ‹è¯•HookçŠ¶æ€
            print("ğŸ§ª HookçŠ¶æ€æµ‹è¯•")
            hook_status = hm.get_hook_status()
            total_hooks = hook_status['summary']['total_hooks']
            installed_hooks = hook_status['summary']['installed_hooks']

            print(f"   âœ… HookçŠ¶æ€: {installed_hooks}/{total_hooks} é’©å­å®‰è£…")

            self.test_results["tests"]["hooks_system"] = {
                "success": True,
                "init_time": init_time,
                "total_plugins": plugin_count,
                "enabled_plugins": enabled_count,
                "total_hooks": total_hooks,
                "installed_hooks": installed_hooks,
                "config_size": len(config_summary)
            }

        except Exception as e:
            print(f"   ğŸ’¥ Hooksç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
            self.test_results["tests"]["hooks_system"] = {
                "success": False,
                "error": str(e)
            }
            self.test_results["errors"].append(f"Hooks system: {e}")

    def test_version_manager(self):
        """æµ‹è¯•ç‰ˆæœ¬ç®¡ç†ç³»ç»Ÿ"""
        print("\nğŸ“Š æµ‹è¯•ç‰ˆæœ¬ç®¡ç†ç³»ç»Ÿ")
        print("-" * 40)

        try:
            # æµ‹è¯•ç‰ˆæœ¬ç®¡ç†å™¨
            print("ğŸ§ª ç‰ˆæœ¬ç®¡ç†å™¨åˆå§‹åŒ–æµ‹è¯•")
            start_time = time.time()

            vm = get_global_version_manager()
            va = get_global_version_advisor()

            init_time = time.time() - start_time
            print(f"   âœ… ç‰ˆæœ¬ç®¡ç†å™¨åˆå§‹åŒ–: {init_time:.2f}s")

            # æµ‹è¯•ç‰ˆæœ¬è·å–
            print("ğŸ§ª ç‰ˆæœ¬ä¿¡æ¯æµ‹è¯•")
            current_version = vm.get_current_version()
            version_info = vm.get_version_info()

            print(f"   âœ… å½“å‰ç‰ˆæœ¬: {current_version}")
            print(f"   âœ… ç‰ˆæœ¬ä¿¡æ¯: {len(json.dumps(version_info))} å­—ç¬¦")

            # æµ‹è¯•ç‰ˆæœ¬å»ºè®®
            print("ğŸ§ª ç‰ˆæœ¬å†³ç­–æµ‹è¯•")
            suggestion = va.suggest_version_bump(current_version)

            print(f"   âœ… ç‰ˆæœ¬å»ºè®®: {suggestion.get('suggested_version', 'N/A')}")

            self.test_results["tests"]["version_manager"] = {
                "success": True,
                "init_time": init_time,
                "current_version": current_version,
                "version_info_size": len(json.dumps(version_info)),
                "suggestion": suggestion
            }

        except Exception as e:
            print(f"   ğŸ’¥ ç‰ˆæœ¬ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
            self.test_results["tests"]["version_manager"] = {
                "success": False,
                "error": str(e)
            }
            self.test_results["errors"].append(f"Version manager: {e}")

    def test_plugin_performance(self):
        """æµ‹è¯•æ’ä»¶ç³»ç»Ÿæ€§èƒ½"""
        print("\nâš¡ æµ‹è¯•æ’ä»¶ç³»ç»Ÿæ€§èƒ½")
        print("-" * 40)

        try:
            # åˆ›å»ºæ’ä»¶ç®¡ç†å™¨
            plugins_dir = os.path.join(self.project_root, 'features/git_workflow/plugins')
            pm = PluginManager(plugins_dir=plugins_dir)

            # æµ‹è¯•æ’ä»¶åŠ è½½æ€§èƒ½
            print("ğŸ§ª æ’ä»¶åŠ è½½æ€§èƒ½æµ‹è¯•")
            load_times = []

            for i in range(5):  # é‡å¤5æ¬¡æµ‹è¯•
                start_time = time.time()
                results = pm.load_all_plugins()
                load_time = time.time() - start_time
                load_times.append(load_time)

                # æ¸…ç†æ’ä»¶ä»¥ä¾¿ä¸‹æ¬¡æµ‹è¯•
                pm.cleanup()

            avg_load_time = sum(load_times) / len(load_times)
            min_load_time = min(load_times)
            max_load_time = max(load_times)

            print(f"   âœ… å¹³å‡åŠ è½½æ—¶é—´: {avg_load_time:.3f}s")
            print(f"   âœ… æœ€å¿«åŠ è½½æ—¶é—´: {min_load_time:.3f}s")
            print(f"   âœ… æœ€æ…¢åŠ è½½æ—¶é—´: {max_load_time:.3f}s")

            # é‡æ–°åŠ è½½æ’ä»¶ç”¨äºæ‰§è¡Œæµ‹è¯•
            pm.load_all_plugins()

            # æµ‹è¯•æ’ä»¶æ‰§è¡Œæ€§èƒ½
            print("ğŸ§ª æ’ä»¶æ‰§è¡Œæ€§èƒ½æµ‹è¯•")
            context = {
                'hook_name': 'performance_test',
                'project_root': self.project_root,
                'staged_files': ['test_file.py'],
                'dry_run': True
            }

            execution_times = {}
            for plugin_name in pm.get_enabled_plugins():
                start_time = time.time()
                result = pm.execute_plugin(plugin_name, context)
                exec_time = time.time() - start_time
                execution_times[plugin_name] = {
                    'time': exec_time,
                    'status': result.status.value,
                    'success': result.status.value in ['success', 'skipped']
                }

                print(f"   âœ… {plugin_name}: {exec_time:.3f}s ({result.status.value})")

            pm.cleanup()

            self.test_results["tests"]["plugin_performance"] = {
                "success": True,
                "load_times": {
                    "average": avg_load_time,
                    "min": min_load_time,
                    "max": max_load_time,
                    "all_times": load_times
                },
                "execution_times": execution_times
            }

        except Exception as e:
            print(f"   ğŸ’¥ æ’ä»¶æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            self.test_results["tests"]["plugin_performance"] = {
                "success": False,
                "error": str(e)
            }
            self.test_results["errors"].append(f"Plugin performance: {e}")

    def test_concurrent_operations(self):
        """æµ‹è¯•å¹¶å‘æ“ä½œ"""
        print("\nğŸš€ æµ‹è¯•å¹¶å‘æ“ä½œ")
        print("-" * 40)

        try:
            # å¹¶å‘æ‰§è¡Œå¤šä¸ªPerfect21æ“ä½œ
            commands = [
                "./perfect21 --status",
                "python3 main/cli.py hooks status",
                "python3 main/cli.py workflow list",
                "python3 main/cli.py branch status"
            ]

            print(f"ğŸ§ª å¹¶å‘æ‰§è¡Œ {len(commands)} ä¸ªæ“ä½œ")
            start_time = time.time()

            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                futures = [
                    executor.submit(
                        self.run_command_test,
                        f"å¹¶å‘ä»»åŠ¡{i+1}",
                        cmd,
                        30
                    )
                    for i, cmd in enumerate(commands)
                ]

                results = []
                for future in concurrent.futures.as_completed(futures):
                    results.append(future.result())

            total_time = time.time() - start_time
            success_count = sum(1 for r in results if r['success'])

            print(f"   âœ… å¹¶å‘æ‰§è¡Œå®Œæˆ: {total_time:.2f}s")
            print(f"   âœ… æˆåŠŸç‡: {success_count}/{len(results)} ({success_count/len(results)*100:.1f}%)")

            self.test_results["tests"]["concurrent_operations"] = {
                "success": success_count == len(results),
                "total_time": total_time,
                "success_count": success_count,
                "total_count": len(results),
                "success_rate": success_count / len(results) * 100,
                "individual_results": results
            }

        except Exception as e:
            print(f"   ğŸ’¥ å¹¶å‘æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
            self.test_results["tests"]["concurrent_operations"] = {
                "success": False,
                "error": str(e)
            }
            self.test_results["errors"].append(f"Concurrent operations: {e}")

    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("\nğŸ’¾ æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ")
        print("-" * 40)

        try:
            import psutil
            import gc

            # è·å–åˆå§‹å†…å­˜
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB

            print("ğŸ§ª åˆ›å»ºå¤šä¸ªPerfect21ç»„ä»¶æµ‹è¯•å†…å­˜")

            # åˆ›å»ºå¤šä¸ªç»„ä»¶å®ä¾‹
            components = []
            memory_snapshots = [initial_memory]

            for i in range(10):
                hm = GitHooksManager()
                components.append(hm)

                current_memory = process.memory_info().rss / 1024 / 1024
                memory_snapshots.append(current_memory)

                if i % 2 == 0:  # æ¯ä¸¤æ¬¡æ˜¾ç¤ºä¸€æ¬¡
                    print(f"   ğŸ“Š åˆ›å»º {i+1} ä¸ªç»„ä»¶: {current_memory:.1f}MB (+{current_memory-initial_memory:.1f}MB)")

            peak_memory = max(memory_snapshots)

            # æ¸…ç†ç»„ä»¶
            components.clear()
            gc.collect()

            final_memory = process.memory_info().rss / 1024 / 1024
            memory_freed = peak_memory - final_memory

            print(f"   âœ… åˆå§‹å†…å­˜: {initial_memory:.1f}MB")
            print(f"   âœ… å³°å€¼å†…å­˜: {peak_memory:.1f}MB")
            print(f"   âœ… æœ€ç»ˆå†…å­˜: {final_memory:.1f}MB")
            print(f"   âœ… é‡Šæ”¾å†…å­˜: {memory_freed:.1f}MB")

            self.test_results["tests"]["memory_usage"] = {
                "success": True,
                "initial_memory_mb": initial_memory,
                "peak_memory_mb": peak_memory,
                "final_memory_mb": final_memory,
                "memory_freed_mb": memory_freed,
                "memory_snapshots": memory_snapshots
            }

        except ImportError:
            print("   âš ï¸  psutilæœªå®‰è£…ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
            self.test_results["tests"]["memory_usage"] = {
                "success": False,
                "error": "psutil not available"
            }
        except Exception as e:
            print(f"   ğŸ’¥ å†…å­˜æµ‹è¯•å¤±è´¥: {e}")
            self.test_results["tests"]["memory_usage"] = {
                "success": False,
                "error": str(e)
            }
            self.test_results["errors"].append(f"Memory usage: {e}")

    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆå‹åŠ›æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)

        end_time = time.time()
        total_time = end_time - self.test_results["start_time"]

        # ç»Ÿè®¡ç»“æœ
        total_tests = len(self.test_results["tests"])
        successful_tests = sum(
            1 for test_data in self.test_results["tests"].values()
            if test_data.get("success", False)
        )

        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0

        # æ›´æ–°æ‘˜è¦
        self.test_results["summary"] = {
            "total_time": total_time,
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "failed_tests": total_tests - successful_tests,
            "success_rate": success_rate,
            "end_time": end_time
        }

        # æ‰“å°æŠ¥å‘Š
        print(f"ğŸ•’ æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
        print(f"ğŸ“‹ æµ‹è¯•é¡¹ç›®: {total_tests}")
        print(f"âœ… æˆåŠŸ: {successful_tests}")
        print(f"âŒ å¤±è´¥: {total_tests - successful_tests}")
        print(f"ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")

        if self.test_results["errors"]:
            print(f"\nğŸš¨ é”™è¯¯è¯¦æƒ…:")
            for error in self.test_results["errors"]:
                print(f"   â€¢ {error}")

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = os.path.join(self.project_root, "tests/stress_test_report.json")
        os.makedirs(os.path.dirname(report_file), exist_ok=True)

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        # æ€§èƒ½è¯„çº§
        if success_rate >= 95:
            grade = "ğŸ† ä¼˜ç§€"
        elif success_rate >= 85:
            grade = "ğŸ¥‰ è‰¯å¥½"
        elif success_rate >= 70:
            grade = "âš ï¸  éœ€è¦æ”¹è¿›"
        else:
            grade = "ğŸš¨ ä¸¥é‡é—®é¢˜"

        print(f"\nğŸ¯ Perfect21ç³»ç»Ÿè¯„çº§: {grade} ({success_rate:.1f}%)")

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        try:
            self.test_perfect21_commands()
            self.test_hooks_system()
            self.test_version_manager()
            self.test_plugin_performance()
            self.test_concurrent_operations()
            self.test_memory_usage()
        finally:
            self.generate_report()


if __name__ == "__main__":
    print("ğŸ”¥ Perfect21ç³»ç»Ÿè‡ªæˆ‘å‹åŠ›æµ‹è¯•")
    print("Testing all components: Commands, Hooks, Plugins, Version Manager, Performance")
    print()

    tester = Perfect21StressTest()
    tester.run_all_tests()