#!/usr/bin/env python3
"""
Perfect21æ€§èƒ½ä¼˜åŒ–éªŒè¯æµ‹è¯•
å…¨é¢æµ‹è¯•å’ŒéªŒè¯æ€§èƒ½ä¼˜åŒ–æ•ˆæœ
"""

import os
import sys
import asyncio
import time
import statistics
import psutil
import json
from typing import Dict, Any, List, Tuple
from datetime import datetime
import concurrent.futures
import threading

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from modules.enhanced_performance_optimizer import (
    enhanced_performance_optimizer,
    optimize_agent_execution,
    batch_git_operations,
    optimize_memory,
    start_performance_optimization,
    get_performance_report,
    run_benchmark,
    optimized_execution
)
from modules.enhanced_git_cache import (
    enhanced_git_cache,
    git_batch_processor,
    queue_git_operation,
    get_git_cache_stats
)
from modules.performance_cache import performance_cache
from modules.logger import log_info, log_error

class PerformanceValidator:
    """æ€§èƒ½ä¼˜åŒ–éªŒè¯å™¨"""

    def __init__(self):
        self.test_results = {}
        self.baseline_metrics = {}
        self.optimized_metrics = {}

        log_info("æ€§èƒ½éªŒè¯å™¨åˆå§‹åŒ–å®Œæˆ")

    async def run_comprehensive_validation(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢æ€§èƒ½éªŒè¯"""
        print("ğŸš€ å¼€å§‹Perfect21æ€§èƒ½ä¼˜åŒ–éªŒè¯")
        print("=" * 60)

        validation_results = {
            'timestamp': datetime.now().isoformat(),
            'tests': {},
            'summary': {},
            'recommendations': []
        }

        # 1. åŸºçº¿æ€§èƒ½æµ‹è¯•
        print("ğŸ“Š å»ºç«‹æ€§èƒ½åŸºçº¿...")
        baseline_results = await self._establish_baseline()
        validation_results['tests']['baseline'] = baseline_results

        # 2. Agentæ‰§è¡Œä¼˜åŒ–æµ‹è¯•
        print("ğŸ¤– æµ‹è¯•Agentæ‰§è¡Œä¼˜åŒ–...")
        agent_results = await self._test_agent_optimization()
        validation_results['tests']['agent_optimization'] = agent_results

        # 3. ç¼“å­˜ç³»ç»Ÿæµ‹è¯•
        print("ğŸ“¦ æµ‹è¯•ç¼“å­˜ç³»ç»Ÿ...")
        cache_results = await self._test_cache_system()
        validation_results['tests']['cache_system'] = cache_results

        # 4. Gitæ“ä½œä¼˜åŒ–æµ‹è¯•
        print("ğŸ”§ æµ‹è¯•Gitæ“ä½œä¼˜åŒ–...")
        git_results = await self._test_git_optimization()
        validation_results['tests']['git_optimization'] = git_results

        # 5. å†…å­˜ä¼˜åŒ–æµ‹è¯•
        print("ğŸ’¾ æµ‹è¯•å†…å­˜ä¼˜åŒ–...")
        memory_results = await self._test_memory_optimization()
        validation_results['tests']['memory_optimization'] = memory_results

        # 6. å¹¶å‘æ€§èƒ½æµ‹è¯•
        print("âš¡ æµ‹è¯•å¹¶å‘æ€§èƒ½...")
        concurrency_results = await self._test_concurrency_performance()
        validation_results['tests']['concurrency'] = concurrency_results

        # 7. èµ„æºæ± æµ‹è¯•
        print("ğŸ”„ æµ‹è¯•èµ„æºæ± ...")
        pool_results = await self._test_resource_pool()
        validation_results['tests']['resource_pool'] = pool_results

        # ç”Ÿæˆæ€»ç»“å’Œå»ºè®®
        validation_results['summary'] = self._generate_summary(validation_results['tests'])
        validation_results['recommendations'] = self._generate_recommendations(validation_results['tests'])

        print("\nâœ… æ€§èƒ½éªŒè¯å®Œæˆ!")
        return validation_results

    async def _establish_baseline(self) -> Dict[str, Any]:
        """å»ºç«‹æ€§èƒ½åŸºçº¿"""
        baseline = {}

        # ç³»ç»Ÿèµ„æºåŸºçº¿
        process = psutil.Process()
        baseline['initial_memory_mb'] = process.memory_info().rss / 1024 / 1024
        baseline['initial_cpu_percent'] = process.cpu_percent()

        # æ‰§è¡ŒåŸºçº¿æµ‹è¯•
        execution_times = []
        for i in range(10):
            start_time = time.perf_counter()
            await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿ10msæ“ä½œ
            execution_time = time.perf_counter() - start_time
            execution_times.append(execution_time * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

        baseline['avg_execution_time_ms'] = statistics.mean(execution_times)
        baseline['p95_execution_time_ms'] = statistics.quantiles(execution_times, n=20)[18]

        return baseline

    async def _test_agent_optimization(self) -> Dict[str, Any]:
        """æµ‹è¯•Agentæ‰§è¡Œä¼˜åŒ–"""
        results = {}

        # æµ‹è¯•ç¼“å­˜å‘½ä¸­
        agent_type = 'test-agent'
        params = {'test_param': 'test_value'}

        # ç¬¬ä¸€æ¬¡æ‰§è¡Œ (ç¼“å­˜æœªå‘½ä¸­)
        start_time = time.perf_counter()
        result1 = await optimize_agent_execution(agent_type, params)
        first_execution_time = time.perf_counter() - start_time

        # ç¬¬äºŒæ¬¡æ‰§è¡Œ (ç¼“å­˜å‘½ä¸­)
        start_time = time.perf_counter()
        result2 = await optimize_agent_execution(agent_type, params)
        second_execution_time = time.perf_counter() - start_time

        # è®¡ç®—ç¼“å­˜æ•ˆæœ
        cache_improvement = ((first_execution_time - second_execution_time) / first_execution_time) * 100

        results['first_execution_time_ms'] = first_execution_time * 1000
        results['second_execution_time_ms'] = second_execution_time * 1000
        results['cache_improvement_percent'] = cache_improvement
        results['cache_working'] = second_execution_time < first_execution_time * 0.5  # è‡³å°‘50%æå‡

        # æµ‹è¯•ä¸åŒAgentç±»å‹
        agent_types = ['agent-1', 'agent-2', 'agent-3']
        concurrent_times = []

        async def test_agent(agent_type):
            start_time = time.perf_counter()
            await optimize_agent_execution(agent_type, {'id': agent_type})
            return time.perf_counter() - start_time

        # å¹¶å‘æ‰§è¡Œ
        start_time = time.perf_counter()
        concurrent_results = await asyncio.gather(*[test_agent(agent) for agent in agent_types])
        total_concurrent_time = time.perf_counter() - start_time

        # é¡ºåºæ‰§è¡Œå¯¹æ¯”
        sequential_times = []
        for agent_type in agent_types:
            start_time = time.perf_counter()
            await optimize_agent_execution(agent_type, {'id': agent_type})
            sequential_times.append(time.perf_counter() - start_time)

        total_sequential_time = sum(sequential_times)

        results['concurrent_execution_time_ms'] = total_concurrent_time * 1000
        results['sequential_execution_time_ms'] = total_sequential_time * 1000
        results['concurrency_improvement_percent'] = ((total_sequential_time - total_concurrent_time) / total_sequential_time) * 100

        return results

    async def _test_cache_system(self) -> Dict[str, Any]:
        """æµ‹è¯•ç¼“å­˜ç³»ç»Ÿ"""
        results = {}

        # æµ‹è¯•å¤šå±‚ç¼“å­˜
        cache_operations = []

        # å¼‚æ­¥ç¼“å­˜æµ‹è¯•
        async_test_data = {'test': 'async_data', 'timestamp': time.time()}
        await enhanced_performance_optimizer.cache_system.async_set('async_test_key', async_test_data)
        cached_data = await enhanced_performance_optimizer.cache_system.async_get('async_test_key')

        results['async_cache_working'] = cached_data == async_test_data

        # Gitç¼“å­˜æµ‹è¯•
        git_cache_stats_before = get_git_cache_stats()

        # æ¨¡æ‹ŸGitæ“ä½œ
        test_operations = [
            ('status', []),
            ('branch', []),
            ('log', ['--oneline', '-10']),
            ('status', []),  # é‡å¤æ“ä½œæµ‹è¯•ç¼“å­˜
            ('log', ['--oneline', '-10'])  # é‡å¤æ“ä½œæµ‹è¯•ç¼“å­˜
        ]

        git_execution_times = []
        for command, args in test_operations:
            start_time = time.perf_counter()
            # è¿™é‡Œæ¨¡æ‹ŸGitç¼“å­˜è°ƒç”¨
            cached_result = enhanced_git_cache.get_cached_result(command, args)
            if cached_result is None:
                # æ¨¡æ‹ŸGitæ‰§è¡Œå¹¶ç¼“å­˜
                mock_result = f"Mock {command} result"
                enhanced_git_cache.cache_result(command, args, mock_result)
            execution_time = time.perf_counter() - start_time
            git_execution_times.append(execution_time * 1000)

        git_cache_stats_after = get_git_cache_stats()

        results['git_cache_hit_rate'] = git_cache_stats_after['hit_rate']
        results['git_cache_entries'] = git_cache_stats_after['total_entries']
        results['avg_git_operation_time_ms'] = statistics.mean(git_execution_times)

        # é€šç”¨ç¼“å­˜æµ‹è¯•
        cache_hit_times = []
        cache_miss_times = []

        # æµ‹è¯•ç¼“å­˜æœªå‘½ä¸­
        for i in range(5):
            start_time = time.perf_counter()
            with performance_cache.get_dict() as test_dict:
                test_dict[f'key_{i}'] = f'value_{i}'
                # æ¨¡æ‹Ÿå¤„ç†
                sum(range(100))
            miss_time = time.perf_counter() - start_time
            cache_miss_times.append(miss_time * 1000)

        results['avg_cache_miss_time_ms'] = statistics.mean(cache_miss_times)

        # å†…å­˜æ± æ•ˆç‡æµ‹è¯•
        pool_stats = enhanced_performance_optimizer.resource_manager.get_pool_stats()
        results['resource_pool_stats'] = pool_stats

        return results

    async def _test_git_optimization(self) -> Dict[str, Any]:
        """æµ‹è¯•Gitæ“ä½œä¼˜åŒ–"""
        results = {}

        # æµ‹è¯•æ‰¹é‡Gitæ“ä½œ
        batch_operations = [
            ('status', []),
            ('status', ['--short']),
            ('branch', []),
            ('branch', ['-r']),
            ('log', ['--oneline', '-5']),
            ('log', ['--oneline', '-10'])
        ]

        # æ‰¹é‡æ‰§è¡Œ
        batch_start_time = time.perf_counter()
        operation_callbacks = []
        operation_results = []

        def create_callback(op_id):
            def callback(result):
                operation_results.append((op_id, result, time.perf_counter()))
            return callback

        # é˜Ÿåˆ—æ‰€æœ‰æ“ä½œ
        for i, (command, args) in enumerate(batch_operations):
            callback = create_callback(i)
            operation_callbacks.append(callback)
            queue_git_operation(command, args, callback)

        # ç­‰å¾…æ‰¹é‡å¤„ç†å®Œæˆ
        await asyncio.sleep(3)  # ç­‰å¾…æ‰¹é‡å¤„ç†

        batch_total_time = time.perf_counter() - batch_start_time

        # å•ç‹¬æ‰§è¡Œå¯¹æ¯”
        individual_start_time = time.perf_counter()
        for command, args in batch_operations:
            # æ¨¡æ‹Ÿå•ç‹¬Gitæ‰§è¡Œ
            start_time = time.perf_counter()
            cached_result = enhanced_git_cache.get_cached_result(command, args)
            if cached_result is None:
                # æ¨¡æ‹ŸGitå‘½ä»¤æ‰§è¡Œå»¶è¿Ÿ
                await asyncio.sleep(0.05)  # 50msæ¨¡æ‹ŸGitå‘½ä»¤
                enhanced_git_cache.cache_result(command, args, f"Individual {command} result")
            individual_time = time.perf_counter() - start_time

        individual_total_time = time.perf_counter() - individual_start_time

        # è®¡ç®—æ‰¹é‡ä¼˜åŒ–æ•ˆæœ
        batch_improvement = ((individual_total_time - batch_total_time) / individual_total_time) * 100

        results['batch_execution_time_ms'] = batch_total_time * 1000
        results['individual_execution_time_ms'] = individual_total_time * 1000
        results['batch_improvement_percent'] = batch_improvement
        results['operations_processed'] = len(batch_operations)
        results['batch_optimization_working'] = batch_improvement > 20  # è‡³å°‘20%æå‡

        # Gitç¼“å­˜ç»Ÿè®¡
        git_stats = get_git_cache_stats()
        results['git_cache_final_stats'] = git_stats

        return results

    async def _test_memory_optimization(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä¼˜åŒ–"""
        results = {}

        # å†…å­˜ä½¿ç”¨åŸºçº¿
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024

        # åˆ›å»ºå†…å­˜å‹åŠ›
        memory_hogs = []
        for i in range(100):
            # åˆ›å»ºå¤§å¯¹è±¡
            memory_hogs.append([f'data_{j}' for j in range(1000)])

        before_optimization_memory = process.memory_info().rss / 1024 / 1024

        # æ‰§è¡Œå†…å­˜ä¼˜åŒ–
        optimization_result = await optimize_memory()

        after_optimization_memory = process.memory_info().rss / 1024 / 1024

        # æ¸…ç†æµ‹è¯•å¯¹è±¡
        del memory_hogs

        final_memory = process.memory_info().rss / 1024 / 1024

        results['initial_memory_mb'] = initial_memory
        results['before_optimization_memory_mb'] = before_optimization_memory
        results['after_optimization_memory_mb'] = after_optimization_memory
        results['final_memory_mb'] = final_memory
        results['memory_freed_by_optimization_mb'] = before_optimization_memory - after_optimization_memory
        results['optimization_effectiveness'] = optimization_result
        results['memory_optimization_working'] = before_optimization_memory > after_optimization_memory

        return results

    async def _test_concurrency_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        results = {}

        # å¹¶å‘Agentæ‰§è¡Œæµ‹è¯•
        concurrent_tasks = 20

        async def concurrent_agent_task(task_id: int):
            start_time = time.perf_counter()
            result = await optimize_agent_execution(f'concurrent-agent-{task_id}', {'task_id': task_id})
            execution_time = time.perf_counter() - start_time
            return task_id, execution_time, result

        # å¹¶å‘æ‰§è¡Œ
        concurrent_start_time = time.perf_counter()
        concurrent_results = await asyncio.gather(*[
            concurrent_agent_task(i) for i in range(concurrent_tasks)
        ])
        concurrent_total_time = time.perf_counter() - concurrent_start_time

        # é¡ºåºæ‰§è¡Œå¯¹æ¯”
        sequential_start_time = time.perf_counter()
        sequential_results = []
        for i in range(concurrent_tasks):
            task_result = await concurrent_agent_task(i)
            sequential_results.append(task_result)
        sequential_total_time = time.perf_counter() - sequential_start_time

        # åˆ†æç»“æœ
        concurrent_individual_times = [result[1] * 1000 for result in concurrent_results]
        sequential_individual_times = [result[1] * 1000 for result in sequential_results]

        results['concurrent_tasks'] = concurrent_tasks
        results['concurrent_total_time_ms'] = concurrent_total_time * 1000
        results['sequential_total_time_ms'] = sequential_total_time * 1000
        results['concurrency_speedup'] = sequential_total_time / concurrent_total_time
        results['avg_concurrent_task_time_ms'] = statistics.mean(concurrent_individual_times)
        results['avg_sequential_task_time_ms'] = statistics.mean(sequential_individual_times)
        results['concurrency_efficiency'] = (concurrent_tasks / (concurrent_total_time / (sequential_total_time / concurrent_tasks))) * 100

        return results

    async def _test_resource_pool(self) -> Dict[str, Any]:
        """æµ‹è¯•èµ„æºæ± """
        results = {}

        # èµ„æºæ± ä½¿ç”¨æµ‹è¯•
        pool_usage_times = []
        direct_creation_times = []

        # æµ‹è¯•èµ„æºæ± ä½¿ç”¨
        for i in range(50):
            start_time = time.perf_counter()
            with enhanced_performance_optimizer.resource_manager.get_resource('dict') as test_dict:
                test_dict['test_key'] = f'test_value_{i}'
                # æ¨¡æ‹Ÿä½¿ç”¨
                len(test_dict)
            pool_time = time.perf_counter() - start_time
            pool_usage_times.append(pool_time * 1000)

        # æµ‹è¯•ç›´æ¥åˆ›å»º
        for i in range(50):
            start_time = time.perf_counter()
            test_dict = {}
            test_dict['test_key'] = f'test_value_{i}'
            len(test_dict)
            del test_dict
            direct_time = time.perf_counter() - start_time
            direct_creation_times.append(direct_time * 1000)

        # åˆ†ææ•ˆæœ
        avg_pool_time = statistics.mean(pool_usage_times)
        avg_direct_time = statistics.mean(direct_creation_times)
        pool_efficiency = ((avg_direct_time - avg_pool_time) / avg_direct_time) * 100

        results['avg_pool_usage_time_ms'] = avg_pool_time
        results['avg_direct_creation_time_ms'] = avg_direct_time
        results['pool_efficiency_percent'] = pool_efficiency
        results['pool_working'] = avg_pool_time < avg_direct_time

        # è·å–èµ„æºæ± ç»Ÿè®¡
        pool_stats = enhanced_performance_optimizer.resource_manager.get_pool_stats()
        results['pool_stats'] = pool_stats

        return results

    def _generate_summary(self, tests: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ€»ç»“"""
        summary = {
            'total_tests': len(tests),
            'passed_tests': 0,
            'failed_tests': 0,
            'performance_improvements': {},
            'issues_found': []
        }

        # åˆ†æå„é¡¹æµ‹è¯•
        test_status = {}

        # Agentä¼˜åŒ–æµ‹è¯•
        if 'agent_optimization' in tests:
            agent_test = tests['agent_optimization']
            agent_passed = (agent_test.get('cache_working', False) and
                          agent_test.get('cache_improvement_percent', 0) > 10)
            test_status['agent_optimization'] = agent_passed

            if agent_passed:
                summary['performance_improvements']['agent_cache'] = f"{agent_test.get('cache_improvement_percent', 0):.1f}%"
            else:
                summary['issues_found'].append("Agentç¼“å­˜ä¼˜åŒ–æ•ˆæœä¸æ˜æ˜¾")

        # ç¼“å­˜ç³»ç»Ÿæµ‹è¯•
        if 'cache_system' in tests:
            cache_test = tests['cache_system']
            cache_passed = cache_test.get('async_cache_working', False)
            test_status['cache_system'] = cache_passed

            if not cache_passed:
                summary['issues_found'].append("å¼‚æ­¥ç¼“å­˜ç³»ç»Ÿå·¥ä½œå¼‚å¸¸")

        # Gitä¼˜åŒ–æµ‹è¯•
        if 'git_optimization' in tests:
            git_test = tests['git_optimization']
            git_passed = (git_test.get('batch_optimization_working', False) and
                         git_test.get('batch_improvement_percent', 0) > 20)
            test_status['git_optimization'] = git_passed

            if git_passed:
                summary['performance_improvements']['git_batch'] = f"{git_test.get('batch_improvement_percent', 0):.1f}%"
            else:
                summary['issues_found'].append("Gitæ‰¹é‡ä¼˜åŒ–æ•ˆæœä¸è¶³")

        # å†…å­˜ä¼˜åŒ–æµ‹è¯•
        if 'memory_optimization' in tests:
            memory_test = tests['memory_optimization']
            memory_passed = memory_test.get('memory_optimization_working', False)
            test_status['memory_optimization'] = memory_passed

            if memory_passed:
                memory_freed = memory_test.get('memory_freed_by_optimization_mb', 0)
                summary['performance_improvements']['memory_freed'] = f"{memory_freed:.2f}MB"
            else:
                summary['issues_found'].append("å†…å­˜ä¼˜åŒ–æ•ˆæœä¸æ˜æ˜¾")

        # å¹¶å‘æ€§èƒ½æµ‹è¯•
        if 'concurrency' in tests:
            concurrency_test = tests['concurrency']
            speedup = concurrency_test.get('concurrency_speedup', 1)
            concurrency_passed = speedup > 1.5  # è‡³å°‘1.5å€åŠ é€Ÿ
            test_status['concurrency'] = concurrency_passed

            if concurrency_passed:
                summary['performance_improvements']['concurrency_speedup'] = f"{speedup:.1f}x"
            else:
                summary['issues_found'].append("å¹¶å‘æ€§èƒ½æå‡ä¸è¶³")

        # èµ„æºæ± æµ‹è¯•
        if 'resource_pool' in tests:
            pool_test = tests['resource_pool']
            pool_passed = pool_test.get('pool_working', False)
            test_status['resource_pool'] = pool_passed

            if pool_passed:
                pool_efficiency = pool_test.get('pool_efficiency_percent', 0)
                summary['performance_improvements']['resource_pool'] = f"{pool_efficiency:.1f}%"
            else:
                summary['issues_found'].append("èµ„æºæ± æ•ˆç‡ä¸ä½³")

        # ç»Ÿè®¡é€šè¿‡/å¤±è´¥æµ‹è¯•
        summary['passed_tests'] = sum(1 for passed in test_status.values() if passed)
        summary['failed_tests'] = len(test_status) - summary['passed_tests']
        summary['test_status'] = test_status

        return summary

    def _generate_recommendations(self, tests: Dict[str, Any]) -> List[Dict[str, str]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        if 'agent_optimization' in tests:
            agent_test = tests['agent_optimization']
            if agent_test.get('cache_improvement_percent', 0) < 20:
                recommendations.append({
                    'category': 'agent_cache',
                    'priority': 'medium',
                    'recommendation': 'å¢åŠ Agentç¼“å­˜TTLæˆ–ä¼˜åŒ–ç¼“å­˜é”®ç”Ÿæˆç­–ç•¥',
                    'expected_benefit': 'æå‡Agentæ‰§è¡Œæ•ˆç‡20-40%'
                })

        if 'git_optimization' in tests:
            git_test = tests['git_optimization']
            if git_test.get('batch_improvement_percent', 0) < 30:
                recommendations.append({
                    'category': 'git_batch',
                    'priority': 'high',
                    'recommendation': 'è°ƒæ•´Gitæ‰¹é‡å¤„ç†é—´éš”å’Œæ‰¹é‡å¤§å°',
                    'expected_benefit': 'å‡å°‘Gitæ“ä½œå»¶è¿Ÿ30-50%'
                })

        if 'memory_optimization' in tests:
            memory_test = tests['memory_optimization']
            if memory_test.get('memory_freed_by_optimization_mb', 0) < 10:
                recommendations.append({
                    'category': 'memory',
                    'priority': 'medium',
                    'recommendation': 'å¯ç”¨æ›´ç§¯æçš„åƒåœ¾å›æ”¶ç­–ç•¥',
                    'expected_benefit': 'é™ä½å†…å­˜ä½¿ç”¨10-20MB'
                })

        if 'concurrency' in tests:
            concurrency_test = tests['concurrency']
            if concurrency_test.get('concurrency_speedup', 1) < 2:
                recommendations.append({
                    'category': 'concurrency',
                    'priority': 'high',
                    'recommendation': 'ä¼˜åŒ–å¼‚æ­¥æ‰§è¡Œå’Œçº¿ç¨‹æ± é…ç½®',
                    'expected_benefit': 'æå‡å¹¶å‘æ€§èƒ½2-3å€'
                })

        return recommendations

    def print_validation_report(self, results: Dict[str, Any]) -> None:
        """æ‰“å°éªŒè¯æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ Perfect21æ€§èƒ½ä¼˜åŒ–éªŒè¯æŠ¥å‘Š")
        print("=" * 60)

        summary = results.get('summary', {})

        # æ€»ä½“çŠ¶å†µ
        total_tests = summary.get('total_tests', 0)
        passed_tests = summary.get('passed_tests', 0)
        failed_tests = summary.get('failed_tests', 0)

        success_rate = (passed_tests / max(1, total_tests)) * 100

        print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed_tests}/{total_tests} é€šè¿‡ ({success_rate:.1f}%)")

        if success_rate >= 80:
            print("ğŸŸ¢ æ•´ä½“æ€§èƒ½: ä¼˜ç§€")
        elif success_rate >= 60:
            print("ğŸŸ¡ æ•´ä½“æ€§èƒ½: è‰¯å¥½")
        else:
            print("ğŸ”´ æ•´ä½“æ€§èƒ½: éœ€è¦æ”¹è¿›")

        # æ€§èƒ½æå‡
        improvements = summary.get('performance_improvements', {})
        if improvements:
            print("\nâœ¨ æ€§èƒ½æå‡:")
            for category, improvement in improvements.items():
                print(f"  â€¢ {category}: {improvement}")

        # é—®é¢˜å’Œå»ºè®®
        issues = summary.get('issues_found', [])
        if issues:
            print("\nâš ï¸ å‘ç°çš„é—®é¢˜:")
            for issue in issues:
                print(f"  â€¢ {issue}")

        recommendations = results.get('recommendations', [])
        if recommendations:
            print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(recommendations[:3], 1):
                priority_icon = {'high': 'ğŸ”´', 'medium': 'ğŸŸ ', 'low': 'ğŸŸ¡'}.get(rec['priority'], 'âšª')
                print(f"  {i}. {priority_icon} {rec['recommendation']}")
                print(f"     é¢„æœŸæ”¶ç›Š: {rec['expected_benefit']}")

        print("\n" + "=" * 60)

async def main():
    """ä¸»å‡½æ•°"""
    validator = PerformanceValidator()

    try:
        # è¿è¡Œå…¨é¢éªŒè¯
        results = await validator.run_comprehensive_validation()

        # æ‰“å°æŠ¥å‘Š
        validator.print_validation_report(results)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        output_file = f"performance_validation_{int(time.time())}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {output_file}")

        return results

    except Exception as e:
        log_error(f"æ€§èƒ½éªŒè¯å¤±è´¥: {e}")
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    import platform

    print(f"ğŸš€ Perfect21æ€§èƒ½éªŒè¯")
    print(f"Pythonç‰ˆæœ¬: {platform.python_version()}")
    print(f"å¹³å°: {platform.platform()}")

    results = asyncio.run(main())

    if results:
        summary = results.get('summary', {})
        success_rate = (summary.get('passed_tests', 0) / max(1, summary.get('total_tests', 1))) * 100

        if success_rate >= 80:
            print("\nğŸ‰ æ€§èƒ½ä¼˜åŒ–éªŒè¯é€šè¿‡ï¼Perfect21è¿è¡ŒçŠ¶æ€ä¼˜ç§€ã€‚")
            exit(0)
        else:
            print("\nâš ï¸ æ€§èƒ½ä¼˜åŒ–éªŒè¯éƒ¨åˆ†å¤±è´¥ï¼Œå»ºè®®æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Šã€‚")
            exit(1)
    else:
        print("\nâŒ æ€§èƒ½éªŒè¯å¤±è´¥")
        exit(1)