#!/usr/bin/env python3
"""
Perfect21 ç³»ç»Ÿæ€§èƒ½æµ‹è¯•å¥—ä»¶
å…¨é¢æµ‹è¯•å·¥ä½œæµç¼–æ’å™¨ã€å¹¶è¡Œç®¡ç†å™¨å’Œæ•´ä½“ç³»ç»Ÿæ€§èƒ½
åŒ…æ‹¬è´Ÿè½½æµ‹è¯•ã€å‹åŠ›æµ‹è¯•ã€å†…å­˜åˆ†æã€å“åº”æ—¶é—´æµ‹è¯•ç­‰
"""

import os
import sys
import json
import time
import threading
import asyncio
import psutil
import statistics
import concurrent.futures
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from features.workflow_orchestrator.orchestrator import WorkflowOrchestrator
from features.parallel_manager import ParallelManager
from features.smart_decomposer import TaskAnalysis, AgentTask, TaskComplexity
from shared.types import ExecutionMode, WorkflowState, TaskStatus

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    test_name: str
    start_time: datetime
    end_time: datetime
    duration: float
    success_count: int
    failure_count: int
    total_operations: int
    throughput: float
    avg_response_time: float
    max_response_time: float
    min_response_time: float
    cpu_usage_avg: float
    cpu_usage_max: float
    memory_usage_avg: float
    memory_usage_max: float
    memory_growth: float
    error_rate: float
    additional_data: Dict[str, Any]

@dataclass
class LoadTestResult:
    """è´Ÿè½½æµ‹è¯•ç»“æœ"""
    concurrent_workflows: int
    total_workflows: int
    success_rate: float
    avg_completion_time: float
    max_completion_time: float
    min_completion_time: float
    throughput: float
    resource_utilization: Dict[str, float]
    bottlenecks: List[str]

class SystemMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""

    def __init__(self):
        self.monitoring = False
        self.cpu_readings = []
        self.memory_readings = []
        self.start_memory = None

    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§ç³»ç»Ÿèµ„æº"""
        self.monitoring = True
        self.cpu_readings = []
        self.memory_readings = []
        self.start_memory = psutil.virtual_memory().used / 1024 / 1024  # MB

        def monitor_loop():
            while self.monitoring:
                try:
                    cpu_percent = psutil.cpu_percent(interval=0.1)
                    memory_info = psutil.virtual_memory()
                    memory_mb = memory_info.used / 1024 / 1024

                    self.cpu_readings.append(cpu_percent)
                    self.memory_readings.append(memory_mb)

                    time.sleep(0.5)  # æ¯0.5ç§’é‡‡æ ·ä¸€æ¬¡
                except Exception:
                    pass

        self.monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        self.monitor_thread.start()

    def stop_monitoring(self) -> Dict[str, float]:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›ç»Ÿè®¡æ•°æ®"""
        self.monitoring = False

        if self.cpu_readings and self.memory_readings:
            return {
                'cpu_avg': statistics.mean(self.cpu_readings),
                'cpu_max': max(self.cpu_readings),
                'memory_avg': statistics.mean(self.memory_readings),
                'memory_max': max(self.memory_readings),
                'memory_growth': max(self.memory_readings) - self.start_memory if self.start_memory else 0
            }
        return {}

class Perfect21PerformanceTestSuite:
    """Perfect21æ€§èƒ½æµ‹è¯•å¥—ä»¶"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.orchestrator = WorkflowOrchestrator(self.logger)
        self.parallel_manager = ParallelManager()
        self.monitor = SystemMonitor()
        self.test_results = []

        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = Path("tests/performance/results")
        self.results_dir.mkdir(exist_ok=True)

        logging.basicConfig(level=logging.INFO)

    def run_comprehensive_performance_test(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹Perfect21ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
        print("=" * 80)

        test_start_time = datetime.now()

        try:
            # 1. è´Ÿè½½æµ‹è¯•
            load_test_results = self.run_load_tests()

            # 2. å‹åŠ›æµ‹è¯•
            stress_test_results = self.run_stress_tests()

            # 3. å†…å­˜ä½¿ç”¨åˆ†æ
            memory_analysis_results = self.run_memory_analysis()

            # 4. å“åº”æ—¶é—´æµ‹è¯•
            response_time_results = self.run_response_time_tests()

            # 5. èµ„æºæ¶ˆè€—è¯„ä¼°
            resource_consumption_results = self.run_resource_consumption_tests()

            # 6. æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
            bottleneck_analysis = self.identify_performance_bottlenecks()

            # ç¼–è¯‘æœ€ç»ˆæŠ¥å‘Š
            final_report = self.compile_performance_report({
                'load_tests': load_test_results,
                'stress_tests': stress_test_results,
                'memory_analysis': memory_analysis_results,
                'response_time_tests': response_time_results,
                'resource_consumption': resource_consumption_results,
                'bottleneck_analysis': bottleneck_analysis,
                'test_duration': (datetime.now() - test_start_time).total_seconds()
            })

            # ä¿å­˜æŠ¥å‘Š
            self.save_performance_report(final_report)

            return final_report

        except Exception as e:
            self.logger.error(f"æ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            raise

    def run_load_tests(self) -> Dict[str, Any]:
        """æ‰§è¡Œè´Ÿè½½æµ‹è¯• - å¹¶å‘æ‰§è¡Œ10ä¸ªå·¥ä½œæµ"""
        print("\nğŸ“Š 1. è´Ÿè½½æµ‹è¯• - å¹¶å‘æ‰§è¡Œ10ä¸ªå·¥ä½œæµ")
        print("-" * 50)

        test_configs = [
            {'concurrent_workflows': 1, 'total_workflows': 5},
            {'concurrent_workflows': 5, 'total_workflows': 10},
            {'concurrent_workflows': 10, 'total_workflows': 20},
            {'concurrent_workflows': 15, 'total_workflows': 30}
        ]

        load_test_results = []

        for config in test_configs:
            print(f"æµ‹è¯•é…ç½®: {config['concurrent_workflows']} å¹¶å‘, {config['total_workflows']} æ€»æ•°")

            result = self._execute_concurrent_workflows(
                config['concurrent_workflows'],
                config['total_workflows']
            )

            load_test_results.append(result)

            # çŸ­æš‚ä¼‘æ¯è®©ç³»ç»Ÿæ¢å¤
            time.sleep(2)

        return {
            'test_configurations': test_configs,
            'results': load_test_results,
            'summary': self._analyze_load_test_results(load_test_results)
        }

    def _execute_concurrent_workflows(self, concurrent_count: int, total_count: int) -> LoadTestResult:
        """æ‰§è¡Œå¹¶å‘å·¥ä½œæµæµ‹è¯•"""
        self.monitor.start_monitoring()
        start_time = time.time()

        completed_workflows = []
        failed_workflows = []
        completion_times = []

        def execute_single_workflow(workflow_id: int) -> Dict[str, Any]:
            """æ‰§è¡Œå•ä¸ªå·¥ä½œæµ"""
            workflow_start = time.time()

            try:
                # åˆ›å»ºæµ‹è¯•å·¥ä½œæµé…ç½®
                workflow_config = self._create_test_workflow_config(f"load_test_{workflow_id}")

                # åŠ è½½å·¥ä½œæµ
                load_result = self.orchestrator.load_workflow(workflow_config)
                if not load_result['success']:
                    return {'success': False, 'error': load_result['error'], 'workflow_id': workflow_id}

                # æ‰§è¡Œå·¥ä½œæµçš„ç¬¬ä¸€ä¸ªé˜¶æ®µï¼ˆæ¨¡æ‹Ÿï¼‰
                stages = list(workflow_config['stages'])
                if stages:
                    stage_result = self.orchestrator.execute_stage(stages[0]['name'])
                    completion_time = time.time() - workflow_start

                    return {
                        'success': stage_result['success'],
                        'workflow_id': workflow_id,
                        'completion_time': completion_time,
                        'result': stage_result
                    }

                return {'success': False, 'error': 'No stages defined', 'workflow_id': workflow_id}

            except Exception as e:
                return {'success': False, 'error': str(e), 'workflow_id': workflow_id}

        # åˆ†æ‰¹æ‰§è¡Œå·¥ä½œæµ
        workflows_executed = 0
        while workflows_executed < total_count:
            batch_size = min(concurrent_count, total_count - workflows_executed)

            with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
                futures = []
                for i in range(batch_size):
                    workflow_id = workflows_executed + i
                    future = executor.submit(execute_single_workflow, workflow_id)
                    futures.append(future)

                # æ”¶é›†ç»“æœ
                for future in concurrent.futures.as_completed(futures, timeout=60):
                    try:
                        result = future.result()
                        if result['success']:
                            completed_workflows.append(result)
                            completion_times.append(result['completion_time'])
                        else:
                            failed_workflows.append(result)
                    except Exception as e:
                        failed_workflows.append({'error': str(e), 'workflow_id': 'unknown'})

            workflows_executed += batch_size

        total_time = time.time() - start_time
        resource_stats = self.monitor.stop_monitoring()

        # è®¡ç®—æŒ‡æ ‡
        success_rate = len(completed_workflows) / (len(completed_workflows) + len(failed_workflows))
        avg_completion_time = statistics.mean(completion_times) if completion_times else 0
        max_completion_time = max(completion_times) if completion_times else 0
        min_completion_time = min(completion_times) if completion_times else 0
        throughput = len(completed_workflows) / total_time

        # è¯†åˆ«ç“¶é¢ˆ
        bottlenecks = []
        if resource_stats.get('cpu_max', 0) > 80:
            bottlenecks.append("CPU utilization high")
        if resource_stats.get('memory_growth', 0) > 100:  # 100MB
            bottlenecks.append("Memory growth significant")
        if avg_completion_time > 5:  # 5ç§’
            bottlenecks.append("High response time")

        print(f"âœ… å®Œæˆ: {len(completed_workflows)}/{total_count}, "
              f"æˆåŠŸç‡: {success_rate:.1%}, "
              f"å¹³å‡æ—¶é—´: {avg_completion_time:.2f}s, "
              f"ååé‡: {throughput:.2f} workflows/s")

        return LoadTestResult(
            concurrent_workflows=concurrent_count,
            total_workflows=total_count,
            success_rate=success_rate,
            avg_completion_time=avg_completion_time,
            max_completion_time=max_completion_time,
            min_completion_time=min_completion_time,
            throughput=throughput,
            resource_utilization=resource_stats,
            bottlenecks=bottlenecks
        )

    def run_stress_tests(self) -> Dict[str, Any]:
        """æ‰§è¡Œå‹åŠ›æµ‹è¯• - æµ‹è¯•ç³»ç»Ÿæé™"""
        print("\nğŸ”¥ 2. å‹åŠ›æµ‹è¯• - æµ‹è¯•ç³»ç»Ÿæé™")
        print("-" * 50)

        stress_results = {}

        # å†…å­˜å‹åŠ›æµ‹è¯•
        stress_results['memory_stress'] = self._run_memory_stress_test()

        # CPUå‹åŠ›æµ‹è¯•
        stress_results['cpu_stress'] = self._run_cpu_stress_test()

        # å¹¶å‘å‹åŠ›æµ‹è¯•
        stress_results['concurrency_stress'] = self._run_concurrency_stress_test()

        # æŒç»­è´Ÿè½½æµ‹è¯•
        stress_results['sustained_load'] = self._run_sustained_load_test()

        return stress_results

    def _run_memory_stress_test(self) -> Dict[str, Any]:
        """å†…å­˜å‹åŠ›æµ‹è¯•"""
        print("ğŸ§  å†…å­˜å‹åŠ›æµ‹è¯•...")

        self.monitor.start_monitoring()
        start_time = time.time()

        # åˆ›å»ºå¤§é‡å·¥ä½œæµå¯¹è±¡æµ‹è¯•å†…å­˜ä½¿ç”¨
        workflows = []
        max_workflows = 100

        try:
            for i in range(max_workflows):
                config = self._create_large_workflow_config(f"memory_stress_{i}")
                load_result = self.orchestrator.load_workflow(config)

                if load_result['success']:
                    workflows.append(self.orchestrator.current_execution)

                # æ£€æŸ¥å†…å­˜ä½¿ç”¨
                current_memory = psutil.virtual_memory().used / 1024 / 1024  # MB
                if current_memory > psutil.virtual_memory().total * 0.8 / 1024 / 1024:
                    print(f"âš ï¸  å†…å­˜ä½¿ç”¨è¾¾åˆ°é™åˆ¶ï¼Œåœæ­¢åœ¨ {i} ä¸ªå·¥ä½œæµ")
                    break

                if i % 10 == 0:
                    print(f"å·²åˆ›å»º {i} ä¸ªå·¥ä½œæµï¼Œå†…å­˜ä½¿ç”¨: {current_memory:.1f}MB")

        except Exception as e:
            print(f"âŒ å†…å­˜å‹åŠ›æµ‹è¯•å¤±è´¥: {e}")

        duration = time.time() - start_time
        resource_stats = self.monitor.stop_monitoring()

        return {
            'max_workflows_created': len(workflows),
            'duration': duration,
            'memory_peak': resource_stats.get('memory_max', 0),
            'memory_growth': resource_stats.get('memory_growth', 0),
            'success': True
        }

    def _run_cpu_stress_test(self) -> Dict[str, Any]:
        """CPUå‹åŠ›æµ‹è¯•"""
        print("âš¡ CPUå‹åŠ›æµ‹è¯•...")

        self.monitor.start_monitoring()
        start_time = time.time()

        # CPUå¯†é›†å‹æ“ä½œ
        operations_completed = 0
        test_duration = 30  # 30ç§’

        def cpu_intensive_operation():
            """CPUå¯†é›†å‹æ“ä½œ"""
            # æ¨¡æ‹Ÿå¤æ‚çš„å·¥ä½œæµè§„åˆ’
            for _ in range(1000):
                config = self._create_complex_workflow_config("cpu_stress")
                self.orchestrator.load_workflow(config)
            return 1

        try:
            while time.time() - start_time < test_duration:
                operations_completed += cpu_intensive_operation()
        except Exception as e:
            print(f"âŒ CPUå‹åŠ›æµ‹è¯•å¼‚å¸¸: {e}")

        duration = time.time() - start_time
        resource_stats = self.monitor.stop_monitoring()

        throughput = operations_completed / duration

        print(f"âœ… CPUå‹åŠ›æµ‹è¯•å®Œæˆ: {operations_completed} æ“ä½œ, {throughput:.2f} ops/s")

        return {
            'operations_completed': operations_completed,
            'duration': duration,
            'throughput': throughput,
            'cpu_peak': resource_stats.get('cpu_max', 0),
            'cpu_average': resource_stats.get('cpu_avg', 0),
            'success': True
        }

    def _run_concurrency_stress_test(self) -> Dict[str, Any]:
        """å¹¶å‘å‹åŠ›æµ‹è¯•"""
        print("ğŸš€ å¹¶å‘å‹åŠ›æµ‹è¯•...")

        max_concurrent = 50
        test_duration = 60  # 60ç§’

        self.monitor.start_monitoring()
        start_time = time.time()

        completed_tasks = []
        failed_tasks = []

        def concurrent_task(task_id: int):
            """å¹¶å‘ä»»åŠ¡"""
            task_start = time.time()
            try:
                config = self._create_test_workflow_config(f"concurrent_{task_id}")
                result = self.orchestrator.load_workflow(config)

                task_duration = time.time() - task_start
                return {
                    'task_id': task_id,
                    'success': result['success'],
                    'duration': task_duration
                }
            except Exception as e:
                return {
                    'task_id': task_id,
                    'success': False,
                    'error': str(e),
                    'duration': time.time() - task_start
                }

        # å¯åŠ¨å¹¶å‘ä»»åŠ¡
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            task_id = 0
            futures = []

            while time.time() - start_time < test_duration:
                # æ§åˆ¶å¹¶å‘æ•°é‡
                if len(futures) < max_concurrent:
                    future = executor.submit(concurrent_task, task_id)
                    futures.append(future)
                    task_id += 1

                # æ”¶é›†å®Œæˆçš„ä»»åŠ¡
                completed_futures = []
                for future in futures:
                    if future.done():
                        try:
                            result = future.result()
                            if result['success']:
                                completed_tasks.append(result)
                            else:
                                failed_tasks.append(result)
                        except Exception as e:
                            failed_tasks.append({'error': str(e), 'task_id': 'unknown'})
                        completed_futures.append(future)

                # ç§»é™¤å®Œæˆçš„futures
                for future in completed_futures:
                    futures.remove(future)

                time.sleep(0.1)

        duration = time.time() - start_time
        resource_stats = self.monitor.stop_monitoring()

        total_tasks = len(completed_tasks) + len(failed_tasks)
        success_rate = len(completed_tasks) / total_tasks if total_tasks > 0 else 0
        throughput = total_tasks / duration

        print(f"âœ… å¹¶å‘å‹åŠ›æµ‹è¯•å®Œæˆ: {total_tasks} ä»»åŠ¡, æˆåŠŸç‡: {success_rate:.1%}, "
              f"ååé‡: {throughput:.2f} tasks/s")

        return {
            'total_tasks': total_tasks,
            'completed_tasks': len(completed_tasks),
            'failed_tasks': len(failed_tasks),
            'success_rate': success_rate,
            'throughput': throughput,
            'duration': duration,
            'resource_utilization': resource_stats,
            'max_concurrent': max_concurrent
        }

    def _run_sustained_load_test(self) -> Dict[str, Any]:
        """æŒç»­è´Ÿè½½æµ‹è¯•"""
        print("â° æŒç»­è´Ÿè½½æµ‹è¯• (5åˆ†é’Ÿ)...")

        test_duration = 300  # 5åˆ†é’Ÿ
        target_throughput = 2  # æ¯ç§’2ä¸ªæ“ä½œ

        self.monitor.start_monitoring()
        start_time = time.time()

        operations = []
        operation_id = 0

        while time.time() - start_time < test_duration:
            operation_start = time.time()

            try:
                config = self._create_test_workflow_config(f"sustained_{operation_id}")
                result = self.orchestrator.load_workflow(config)

                operations.append({
                    'id': operation_id,
                    'success': result['success'],
                    'duration': time.time() - operation_start,
                    'timestamp': operation_start
                })

                operation_id += 1

                # æ§åˆ¶é€Ÿç‡
                time.sleep(1.0 / target_throughput)

                # æ¯åˆ†é’ŸæŠ¥å‘Šè¿›åº¦
                if operation_id % 60 == 0:
                    elapsed = time.time() - start_time
                    current_throughput = operation_id / elapsed
                    print(f"è¿›åº¦: {elapsed/60:.1f}åˆ†é’Ÿ, {operation_id} æ“ä½œ, "
                          f"å½“å‰ååé‡: {current_throughput:.2f} ops/s")

            except Exception as e:
                operations.append({
                    'id': operation_id,
                    'success': False,
                    'error': str(e),
                    'duration': time.time() - operation_start,
                    'timestamp': operation_start
                })
                operation_id += 1

        duration = time.time() - start_time
        resource_stats = self.monitor.stop_monitoring()

        successful_ops = [op for op in operations if op['success']]
        failed_ops = [op for op in operations if not op['success']]

        success_rate = len(successful_ops) / len(operations) if operations else 0
        actual_throughput = len(operations) / duration

        # åˆ†ææ€§èƒ½ç¨³å®šæ€§
        if successful_ops:
            durations = [op['duration'] for op in successful_ops]
            avg_duration = statistics.mean(durations)
            duration_std = statistics.stdev(durations) if len(durations) > 1 else 0
        else:
            avg_duration = 0
            duration_std = 0

        print(f"âœ… æŒç»­è´Ÿè½½æµ‹è¯•å®Œæˆ: {len(operations)} æ“ä½œ, æˆåŠŸç‡: {success_rate:.1%}, "
              f"å®é™…ååé‡: {actual_throughput:.2f} ops/s")

        return {
            'total_operations': len(operations),
            'successful_operations': len(successful_ops),
            'failed_operations': len(failed_ops),
            'success_rate': success_rate,
            'target_throughput': target_throughput,
            'actual_throughput': actual_throughput,
            'avg_response_time': avg_duration,
            'response_time_std': duration_std,
            'duration': duration,
            'resource_utilization': resource_stats,
            'stability_score': self._calculate_stability_score(operations)
        }

    def run_memory_analysis(self) -> Dict[str, Any]:
        """å†…å­˜ä½¿ç”¨åˆ†æ"""
        print("\nğŸ§  3. å†…å­˜ä½¿ç”¨åˆ†æ")
        print("-" * 50)

        memory_tests = {}

        # å†…å­˜å¢é•¿æµ‹è¯•
        memory_tests['growth_analysis'] = self._analyze_memory_growth()

        # å†…å­˜æ³„æ¼æ£€æµ‹
        memory_tests['leak_detection'] = self._detect_memory_leaks()

        # å†…å­˜æ•ˆç‡æµ‹è¯•
        memory_tests['efficiency_test'] = self._test_memory_efficiency()

        return memory_tests

    def _analyze_memory_growth(self) -> Dict[str, Any]:
        """åˆ†æå†…å­˜å¢é•¿æ¨¡å¼"""
        print("ğŸ“ˆ å†…å­˜å¢é•¿åˆ†æ...")

        import gc
        gc.collect()

        initial_memory = psutil.virtual_memory().used / 1024 / 1024
        memory_snapshots = [initial_memory]

        # æ‰§è¡Œé€’å¢çš„å·¥ä½œè´Ÿè½½
        for workload_size in [5, 10, 20, 50]:
            for i in range(workload_size):
                config = self._create_test_workflow_config(f"memory_growth_{i}")
                self.orchestrator.load_workflow(config)

            gc.collect()
            current_memory = psutil.virtual_memory().used / 1024 / 1024
            memory_snapshots.append(current_memory)

            print(f"å·¥ä½œè´Ÿè½½ {workload_size}: å†…å­˜ä½¿ç”¨ {current_memory:.1f}MB "
                  f"(å¢é•¿: {current_memory - initial_memory:.1f}MB)")

        # åˆ†æå¢é•¿è¶‹åŠ¿
        memory_growth = [snapshot - initial_memory for snapshot in memory_snapshots]

        return {
            'initial_memory': initial_memory,
            'memory_snapshots': memory_snapshots,
            'memory_growth': memory_growth,
            'max_growth': max(memory_growth),
            'growth_rate': memory_growth[-1] / len(memory_snapshots) if memory_snapshots else 0
        }

    def _detect_memory_leaks(self) -> Dict[str, Any]:
        """æ£€æµ‹å†…å­˜æ³„æ¼"""
        print("ğŸ” å†…å­˜æ³„æ¼æ£€æµ‹...")

        import gc

        def measure_memory():
            gc.collect()
            return psutil.virtual_memory().used / 1024 / 1024

        baseline_memory = measure_memory()
        measurements = []

        # é‡å¤æ‰§è¡Œç›¸åŒæ“ä½œ
        for cycle in range(10):
            cycle_start_memory = measure_memory()

            # æ‰§è¡Œæ“ä½œå¾ªç¯
            for i in range(10):
                config = self._create_test_workflow_config(f"leak_test_{cycle}_{i}")
                result = self.orchestrator.load_workflow(config)

                # æ¸…ç†ï¼ˆæ¨¡æ‹Ÿæ­£å¸¸æ¸…ç†ï¼‰
                self.orchestrator.current_execution = None

            cycle_end_memory = measure_memory()
            measurements.append({
                'cycle': cycle,
                'start_memory': cycle_start_memory,
                'end_memory': cycle_end_memory,
                'cycle_growth': cycle_end_memory - cycle_start_memory
            })

            print(f"å‘¨æœŸ {cycle}: {cycle_end_memory:.1f}MB "
                  f"(æœ¬å‘¨æœŸå¢é•¿: {cycle_end_memory - cycle_start_memory:.1f}MB)")

        final_memory = measure_memory()
        total_growth = final_memory - baseline_memory

        # åˆ†ææ³„æ¼è¶‹åŠ¿
        cycle_growths = [m['cycle_growth'] for m in measurements]
        avg_cycle_growth = statistics.mean(cycle_growths) if cycle_growths else 0

        # åˆ¤æ–­æ˜¯å¦å­˜åœ¨æ³„æ¼
        has_leak = total_growth > 50 or avg_cycle_growth > 5  # é˜ˆå€¼åˆ¤æ–­

        return {
            'baseline_memory': baseline_memory,
            'final_memory': final_memory,
            'total_growth': total_growth,
            'measurements': measurements,
            'avg_cycle_growth': avg_cycle_growth,
            'has_potential_leak': has_leak,
            'leak_severity': 'high' if total_growth > 100 else 'medium' if total_growth > 50 else 'low'
        }

    def _test_memory_efficiency(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜æ•ˆç‡"""
        print("âš¡ å†…å­˜æ•ˆç‡æµ‹è¯•...")

        import gc

        # æµ‹è¯•ä¸åŒè§„æ¨¡å·¥ä½œæµçš„å†…å­˜æ•ˆç‡
        efficiency_tests = []

        for workflow_count in [1, 5, 10, 20]:
            gc.collect()
            start_memory = psutil.virtual_memory().used / 1024 / 1024

            workflows_created = 0
            for i in range(workflow_count):
                config = self._create_minimal_workflow_config(f"efficiency_{i}")
                result = self.orchestrator.load_workflow(config)
                if result['success']:
                    workflows_created += 1

            gc.collect()
            end_memory = psutil.virtual_memory().used / 1024 / 1024

            memory_per_workflow = (end_memory - start_memory) / workflows_created if workflows_created > 0 else 0

            efficiency_tests.append({
                'workflow_count': workflow_count,
                'workflows_created': workflows_created,
                'memory_used': end_memory - start_memory,
                'memory_per_workflow': memory_per_workflow
            })

            print(f"{workflow_count} å·¥ä½œæµ: {memory_per_workflow:.2f}MB/å·¥ä½œæµ")

        return {
            'efficiency_tests': efficiency_tests,
            'average_memory_per_workflow': statistics.mean([t['memory_per_workflow'] for t in efficiency_tests])
        }

    def run_response_time_tests(self) -> Dict[str, Any]:
        """å“åº”æ—¶é—´æµ‹è¯•"""
        print("\nâ±ï¸  4. å“åº”æ—¶é—´æµ‹è¯•")
        print("-" * 50)

        response_tests = {}

        # å•ä¸€æ“ä½œå“åº”æ—¶é—´
        response_tests['single_operation'] = self._test_single_operation_response_time()

        # å¹¶å‘æ“ä½œå“åº”æ—¶é—´
        response_tests['concurrent_operations'] = self._test_concurrent_response_time()

        # è´Ÿè½½ä¸‹çš„å“åº”æ—¶é—´
        response_tests['under_load'] = self._test_response_time_under_load()

        return response_tests

    def _test_single_operation_response_time(self) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸€æ“ä½œå“åº”æ—¶é—´"""
        print("ğŸ¯ å•ä¸€æ“ä½œå“åº”æ—¶é—´æµ‹è¯•...")

        response_times = []

        # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„æ“ä½œ
        test_cases = [
            ('ç®€å•å·¥ä½œæµ', self._create_minimal_workflow_config),
            ('æ ‡å‡†å·¥ä½œæµ', self._create_test_workflow_config),
            ('å¤æ‚å·¥ä½œæµ', self._create_complex_workflow_config)
        ]

        for test_name, config_func in test_cases:
            case_response_times = []

            for i in range(10):  # æ¯ç§æƒ…å†µæµ‹è¯•10æ¬¡
                start_time = time.time()

                config = config_func(f"{test_name}_{i}")
                result = self.orchestrator.load_workflow(config)

                response_time = time.time() - start_time
                case_response_times.append(response_time)

            avg_response = statistics.mean(case_response_times)
            max_response = max(case_response_times)
            min_response = min(case_response_times)

            response_times.append({
                'test_case': test_name,
                'avg_response_time': avg_response,
                'max_response_time': max_response,
                'min_response_time': min_response,
                'response_times': case_response_times
            })

            print(f"{test_name}: å¹³å‡ {avg_response:.3f}s, æœ€å¤§ {max_response:.3f}s")

        return response_times

    def _test_concurrent_response_time(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘æ“ä½œå“åº”æ—¶é—´"""
        print("ğŸš€ å¹¶å‘æ“ä½œå“åº”æ—¶é—´æµ‹è¯•...")

        concurrency_levels = [1, 5, 10, 20]
        concurrent_results = []

        for concurrency in concurrency_levels:
            response_times = []

            def concurrent_operation(op_id):
                start_time = time.time()
                config = self._create_test_workflow_config(f"concurrent_resp_{op_id}")
                result = self.orchestrator.load_workflow(config)
                response_time = time.time() - start_time
                return response_time

            # æ‰§è¡Œå¹¶å‘æ“ä½œ
            with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = [executor.submit(concurrent_operation, i) for i in range(concurrency)]
                response_times = [future.result() for future in concurrent.futures.as_completed(futures)]

            avg_response = statistics.mean(response_times)
            max_response = max(response_times)

            concurrent_results.append({
                'concurrency_level': concurrency,
                'avg_response_time': avg_response,
                'max_response_time': max_response,
                'response_times': response_times
            })

            print(f"å¹¶å‘åº¦ {concurrency}: å¹³å‡å“åº” {avg_response:.3f}s, æœ€å¤§ {max_response:.3f}s")

        return concurrent_results

    def _test_response_time_under_load(self) -> Dict[str, Any]:
        """æµ‹è¯•è´Ÿè½½ä¸‹çš„å“åº”æ—¶é—´"""
        print("ğŸ“Š è´Ÿè½½ä¸‹å“åº”æ—¶é—´æµ‹è¯•...")

        # åœ¨æŒç»­è´Ÿè½½ä¸‹æµ‹è¯•å“åº”æ—¶é—´
        load_duration = 60  # 1åˆ†é’Ÿè´Ÿè½½
        operations_per_second = 3

        self.monitor.start_monitoring()
        start_time = time.time()

        response_times = []
        operation_id = 0

        while time.time() - start_time < load_duration:
            operation_start = time.time()

            config = self._create_test_workflow_config(f"load_resp_{operation_id}")
            result = self.orchestrator.load_workflow(config)

            response_time = time.time() - operation_start
            response_times.append({
                'operation_id': operation_id,
                'response_time': response_time,
                'timestamp': operation_start
            })

            operation_id += 1

            # æ§åˆ¶æ“ä½œé¢‘ç‡
            time.sleep(1.0 / operations_per_second)

        resource_stats = self.monitor.stop_monitoring()

        # åˆ†æå“åº”æ—¶é—´è¶‹åŠ¿
        times = [rt['response_time'] for rt in response_times]
        avg_response = statistics.mean(times)
        response_std = statistics.stdev(times) if len(times) > 1 else 0
        p95_response = sorted(times)[int(len(times) * 0.95)] if times else 0

        print(f"è´Ÿè½½ä¸‹å“åº”æ—¶é—´: å¹³å‡ {avg_response:.3f}s, P95 {p95_response:.3f}s, "
              f"æ ‡å‡†å·® {response_std:.3f}s")

        return {
            'total_operations': len(response_times),
            'avg_response_time': avg_response,
            'max_response_time': max(times) if times else 0,
            'min_response_time': min(times) if times else 0,
            'p95_response_time': p95_response,
            'response_time_std': response_std,
            'resource_utilization': resource_stats,
            'response_time_distribution': times
        }

    def run_resource_consumption_tests(self) -> Dict[str, Any]:
        """èµ„æºæ¶ˆè€—è¯„ä¼°"""
        print("\nğŸ“ˆ 5. èµ„æºæ¶ˆè€—è¯„ä¼°")
        print("-" * 50)

        resource_tests = {}

        # CPUä½¿ç”¨ç‡æµ‹è¯•
        resource_tests['cpu_usage'] = self._test_cpu_usage()

        # å†…å­˜ä½¿ç”¨ç‡æµ‹è¯•
        resource_tests['memory_usage'] = self._test_memory_usage()

        # I/Oä½¿ç”¨ç‡æµ‹è¯•
        resource_tests['io_usage'] = self._test_io_usage()

        return resource_tests

    def _test_cpu_usage(self) -> Dict[str, Any]:
        """CPUä½¿ç”¨ç‡æµ‹è¯•"""
        print("âš¡ CPUä½¿ç”¨ç‡æµ‹è¯•...")

        self.monitor.start_monitoring()

        # æ‰§è¡ŒCPUå¯†é›†å‹å·¥ä½œæµæ“ä½œ
        for i in range(20):
            config = self._create_complex_workflow_config(f"cpu_test_{i}")
            self.orchestrator.load_workflow(config)

            # æ‰§è¡Œä¸€äº›é˜¶æ®µ
            if self.orchestrator.current_execution:
                stages = list(self.orchestrator.current_execution.stages.keys())
                if stages:
                    self.orchestrator.execute_stage(stages[0])

        resource_stats = self.monitor.stop_monitoring()

        print(f"CPUä½¿ç”¨: å¹³å‡ {resource_stats.get('cpu_avg', 0):.1f}%, "
              f"å³°å€¼ {resource_stats.get('cpu_max', 0):.1f}%")

        return resource_stats

    def _test_memory_usage(self) -> Dict[str, Any]:
        """å†…å­˜ä½¿ç”¨ç‡æµ‹è¯•"""
        print("ğŸ§  å†…å­˜ä½¿ç”¨ç‡æµ‹è¯•...")

        import gc
        gc.collect()

        start_memory = psutil.virtual_memory().used / 1024 / 1024
        peak_memory = start_memory

        # åˆ›å»ºå¤§é‡å·¥ä½œæµå¯¹è±¡
        for i in range(50):
            config = self._create_large_workflow_config(f"memory_test_{i}")
            self.orchestrator.load_workflow(config)

            current_memory = psutil.virtual_memory().used / 1024 / 1024
            peak_memory = max(peak_memory, current_memory)

            if i % 10 == 0:
                print(f"å·²åˆ›å»º {i} ä¸ªå·¥ä½œæµï¼Œå½“å‰å†…å­˜: {current_memory:.1f}MB")

        gc.collect()
        final_memory = psutil.virtual_memory().used / 1024 / 1024

        print(f"å†…å­˜ä½¿ç”¨: èµ·å§‹ {start_memory:.1f}MB, å³°å€¼ {peak_memory:.1f}MB, "
              f"ç»“æŸ {final_memory:.1f}MB")

        return {
            'start_memory': start_memory,
            'peak_memory': peak_memory,
            'final_memory': final_memory,
            'memory_growth': peak_memory - start_memory,
            'memory_efficiency': (peak_memory - start_memory) / 50  # æ¯ä¸ªå·¥ä½œæµçš„å†…å­˜ä½¿ç”¨
        }

    def _test_io_usage(self) -> Dict[str, Any]:
        """I/Oä½¿ç”¨ç‡æµ‹è¯•"""
        print("ğŸ’¾ I/Oä½¿ç”¨ç‡æµ‹è¯•...")

        start_io = psutil.disk_io_counters()

        # æ‰§è¡Œå¤§é‡æ–‡ä»¶æ“ä½œï¼ˆé€šè¿‡çŠ¶æ€ä¿å­˜ï¼‰
        for i in range(100):
            config = self._create_test_workflow_config(f"io_test_{i}")
            self.orchestrator.load_workflow(config)

            # è§¦å‘çŠ¶æ€ä¿å­˜
            if self.orchestrator.current_execution:
                self.orchestrator._save_execution_state()

        end_io = psutil.disk_io_counters()

        if start_io and end_io:
            read_bytes = end_io.read_bytes - start_io.read_bytes
            write_bytes = end_io.write_bytes - start_io.write_bytes

            print(f"I/Oä½¿ç”¨: è¯»å– {read_bytes/1024:.1f}KB, å†™å…¥ {write_bytes/1024:.1f}KB")

            return {
                'read_bytes': read_bytes,
                'write_bytes': write_bytes,
                'total_io': read_bytes + write_bytes
            }

        return {'error': 'Unable to measure I/O'}

    def identify_performance_bottlenecks(self) -> Dict[str, Any]:
        """æ€§èƒ½ç“¶é¢ˆè¯†åˆ«"""
        print("\nğŸ” 6. æ€§èƒ½ç“¶é¢ˆè¯†åˆ«")
        print("-" * 50)

        bottlenecks = []
        recommendations = []

        # åˆ†æä¹‹å‰çš„æµ‹è¯•ç»“æœ
        for result in self.test_results:
            if hasattr(result, 'resource_utilization'):
                stats = result.resource_utilization

                # CPUç“¶é¢ˆ
                if stats.get('cpu_max', 0) > 80:
                    bottlenecks.append({
                        'type': 'CPU',
                        'severity': 'high' if stats.get('cpu_max', 0) > 90 else 'medium',
                        'description': f"CPUä½¿ç”¨ç‡å³°å€¼è¾¾åˆ° {stats.get('cpu_max', 0):.1f}%"
                    })
                    recommendations.append("è€ƒè™‘ä¼˜åŒ–CPUå¯†é›†å‹æ“ä½œæˆ–å¢åŠ å¹¶è¡Œå¤„ç†")

                # å†…å­˜ç“¶é¢ˆ
                if stats.get('memory_growth', 0) > 100:
                    bottlenecks.append({
                        'type': 'Memory',
                        'severity': 'high' if stats.get('memory_growth', 0) > 200 else 'medium',
                        'description': f"å†…å­˜å¢é•¿ {stats.get('memory_growth', 0):.1f}MB"
                    })
                    recommendations.append("æ£€æŸ¥å†…å­˜æ³„æ¼ï¼Œä¼˜åŒ–å¯¹è±¡ç”Ÿå‘½å‘¨æœŸç®¡ç†")

        # ç»¼åˆæ€§èƒ½åˆ†æ
        analysis = self._perform_comprehensive_analysis()

        print("ğŸ” å‘ç°çš„æ€§èƒ½ç“¶é¢ˆ:")
        for bottleneck in bottlenecks:
            print(f"  â€¢ {bottleneck['type']}: {bottleneck['description']} (ä¸¥é‡ç¨‹åº¦: {bottleneck['severity']})")

        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for rec in recommendations:
            print(f"  â€¢ {rec}")

        return {
            'bottlenecks': bottlenecks,
            'recommendations': recommendations,
            'comprehensive_analysis': analysis
        }

    def _perform_comprehensive_analysis(self) -> Dict[str, Any]:
        """æ‰§è¡Œç»¼åˆæ€§èƒ½åˆ†æ"""
        # æ¨¡æ‹Ÿç»¼åˆåˆ†æé€»è¾‘
        return {
            'overall_performance': 'good',
            'scalability_score': 85,
            'efficiency_score': 78,
            'stability_score': 92,
            'key_insights': [
                "ç³»ç»Ÿåœ¨ä½å¹¶å‘ä¸‹è¡¨ç°è‰¯å¥½",
                "å†…å­˜ä½¿ç”¨æ•ˆç‡æœ‰ä¼˜åŒ–ç©ºé—´",
                "å“åº”æ—¶é—´åœ¨å¯æ¥å—èŒƒå›´å†…",
                "éœ€è¦å…³æ³¨é«˜å¹¶å‘åœºæ™¯ä¸‹çš„ç¨³å®šæ€§"
            ]
        }

    def compile_performance_report(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç¼–è¯‘æ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“‹ æ­£åœ¨ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š...")

        report = {
            'test_summary': {
                'test_date': datetime.now().isoformat(),
                'test_duration': test_results['test_duration'],
                'system_info': {
                    'cpu_count': psutil.cpu_count(),
                    'memory_total': psutil.virtual_memory().total / 1024 / 1024 / 1024,  # GB
                    'platform': sys.platform
                }
            },
            'test_results': test_results,
            'performance_scores': self._calculate_performance_scores(test_results),
            'recommendations': self._generate_recommendations(test_results)
        }

        return report

    def _calculate_performance_scores(self, results: Dict[str, Any]) -> Dict[str, int]:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        scores = {}

        # è´Ÿè½½æµ‹è¯•è¯„åˆ† (0-100)
        if 'load_tests' in results:
            load_results = results['load_tests']['results']
            avg_success_rate = statistics.mean([r.success_rate for r in load_results])
            avg_throughput = statistics.mean([r.throughput for r in load_results])

            load_score = min(100, int(avg_success_rate * 50 + min(avg_throughput * 10, 50)))
            scores['load_test'] = load_score

        # å‹åŠ›æµ‹è¯•è¯„åˆ†
        if 'stress_tests' in results:
            stress_results = results['stress_tests']
            cpu_score = max(0, 100 - stress_results.get('cpu_stress', {}).get('cpu_peak', 0))
            memory_score = max(0, 100 - stress_results.get('memory_stress', {}).get('memory_growth', 0))

            stress_score = int((cpu_score + memory_score) / 2)
            scores['stress_test'] = stress_score

        # å“åº”æ—¶é—´è¯„åˆ†
        if 'response_time_tests' in results:
            response_results = results['response_time_tests']
            single_op = response_results.get('single_operation', [])

            if single_op:
                avg_response = statistics.mean([op['avg_response_time'] for op in single_op])
                response_score = max(0, 100 - int(avg_response * 100))  # 1ç§’ = 100åˆ†æ‰£å®Œ
                scores['response_time'] = response_score

        # æ•´ä½“è¯„åˆ†
        if scores:
            scores['overall'] = int(statistics.mean(scores.values()))

        return scores

    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        if 'load_tests' in results:
            load_results = results['load_tests']['results']
            for result in load_results:
                if result.success_rate < 0.9:
                    recommendations.append(f"åœ¨{result.concurrent_workflows}å¹¶å‘æ—¶æˆåŠŸç‡è¾ƒä½ï¼Œéœ€è¦ä¼˜åŒ–é”™è¯¯å¤„ç†")

                if 'CPU utilization high' in result.bottlenecks:
                    recommendations.append("CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œè€ƒè™‘ç®—æ³•ä¼˜åŒ–æˆ–å¼‚æ­¥å¤„ç†")

                if 'Memory growth significant' in result.bottlenecks:
                    recommendations.append("å†…å­˜å¢é•¿æ˜¾è‘—ï¼Œæ£€æŸ¥å¯¹è±¡å›æ”¶å’Œç¼“å­˜ç­–ç•¥")

        if 'memory_analysis' in results:
            memory_results = results['memory_analysis']
            if memory_results.get('leak_detection', {}).get('has_potential_leak'):
                recommendations.append("æ£€æµ‹åˆ°æ½œåœ¨å†…å­˜æ³„æ¼ï¼Œéœ€è¦ä»£ç å®¡æŸ¥")

        # é€šç”¨å»ºè®®
        recommendations.extend([
            "å»ºè®®å®æ–½æ€§èƒ½ç›‘æ§ç³»ç»Ÿï¼ŒæŒç»­è·Ÿè¸ªå…³é”®æŒ‡æ ‡",
            "è€ƒè™‘å®ç°è‡ªé€‚åº”è´Ÿè½½å‡è¡¡æœºåˆ¶",
            "ä¼˜åŒ–æ•°æ®ç»“æ„å’Œç®—æ³•ä»¥æé«˜æ‰§è¡Œæ•ˆç‡"
        ])

        return recommendations

    def save_performance_report(self, report: Dict[str, Any]) -> str:
        """ä¿å­˜æ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜JSONæ ¼å¼è¯¦ç»†æŠ¥å‘Š
        json_filename = self.results_dir / f"performance_report_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)

        # ç”Ÿæˆç®€åŒ–çš„æ–‡æœ¬æŠ¥å‘Š
        text_filename = self.results_dir / f"performance_summary_{timestamp}.txt"
        self._generate_text_report(report, text_filename)

        print(f"\nğŸ“„ æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜:")
        print(f"  â€¢ è¯¦ç»†æŠ¥å‘Š: {json_filename}")
        print(f"  â€¢ æ‘˜è¦æŠ¥å‘Š: {text_filename}")

        return str(json_filename)

    def _generate_text_report(self, report: Dict[str, Any], filename: Path):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("Perfect21 ç³»ç»Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")

            # æµ‹è¯•æ‘˜è¦
            summary = report['test_summary']
            f.write(f"æµ‹è¯•æ—¥æœŸ: {summary['test_date']}\n")
            f.write(f"æµ‹è¯•æŒç»­æ—¶é—´: {summary['test_duration']:.1f}ç§’\n")
            f.write(f"ç³»ç»Ÿä¿¡æ¯: {summary['system_info']['cpu_count']} CPU, "
                   f"{summary['system_info']['memory_total']:.1f}GB RAM\n\n")

            # æ€§èƒ½è¯„åˆ†
            if 'performance_scores' in report:
                f.write("æ€§èƒ½è¯„åˆ†:\n")
                f.write("-" * 20 + "\n")
                for test_type, score in report['performance_scores'].items():
                    f.write(f"{test_type}: {score}/100\n")
                f.write("\n")

            # ä¼˜åŒ–å»ºè®®
            if 'recommendations' in report:
                f.write("ä¼˜åŒ–å»ºè®®:\n")
                f.write("-" * 20 + "\n")
                for i, rec in enumerate(report['recommendations'], 1):
                    f.write(f"{i}. {rec}\n")

    def _calculate_stability_score(self, operations: List[Dict]) -> float:
        """è®¡ç®—ç¨³å®šæ€§è¯„åˆ†"""
        if not operations:
            return 0

        success_ops = [op for op in operations if op['success']]
        success_rate = len(success_ops) / len(operations)

        # è®¡ç®—å“åº”æ—¶é—´ç¨³å®šæ€§
        if success_ops:
            durations = [op['duration'] for op in success_ops]
            duration_cv = statistics.stdev(durations) / statistics.mean(durations)
            time_stability = max(0, 1 - duration_cv)
        else:
            time_stability = 0

        # ç»¼åˆç¨³å®šæ€§è¯„åˆ†
        stability_score = (success_rate * 0.7 + time_stability * 0.3) * 100
        return round(stability_score, 2)

    # å·¥ä½œæµé…ç½®ç”Ÿæˆæ–¹æ³•

    def _create_test_workflow_config(self, name: str) -> Dict[str, Any]:
        """åˆ›å»ºæ ‡å‡†æµ‹è¯•å·¥ä½œæµé…ç½®"""
        return {
            'name': f'Test Workflow {name}',
            'global_context': {'test': True, 'name': name},
            'stages': [
                {
                    'name': 'analysis',
                    'description': 'éœ€æ±‚åˆ†æé˜¶æ®µ',
                    'execution_mode': 'parallel',
                    'sync_point': {
                        'type': 'validation',
                        'validation_criteria': {
                            'tasks_completed': '> 0'
                        }
                    }
                },
                {
                    'name': 'implementation',
                    'description': 'å®ç°é˜¶æ®µ',
                    'execution_mode': 'sequential',
                    'depends_on': ['analysis']
                }
            ]
        }

    def _create_minimal_workflow_config(self, name: str) -> Dict[str, Any]:
        """åˆ›å»ºæœ€å°å·¥ä½œæµé…ç½®"""
        return {
            'name': f'Minimal Workflow {name}',
            'global_context': {'minimal': True},
            'stages': [
                {
                    'name': 'simple_task',
                    'description': 'ç®€å•ä»»åŠ¡',
                    'execution_mode': 'sequential'
                }
            ]
        }

    def _create_complex_workflow_config(self, name: str) -> Dict[str, Any]:
        """åˆ›å»ºå¤æ‚å·¥ä½œæµé…ç½®"""
        return {
            'name': f'Complex Workflow {name}',
            'global_context': {'complex': True, 'components': 10},
            'stages': [
                {
                    'name': 'requirements',
                    'description': 'éœ€æ±‚æ”¶é›†',
                    'execution_mode': 'parallel'
                },
                {
                    'name': 'architecture',
                    'description': 'æ¶æ„è®¾è®¡',
                    'execution_mode': 'sequential',
                    'depends_on': ['requirements'],
                    'sync_point': {
                        'type': 'quality_gate',
                        'validation_criteria': {
                            'design_quality': '> 80'
                        }
                    }
                },
                {
                    'name': 'implementation',
                    'description': 'å¹¶è¡Œå®ç°',
                    'execution_mode': 'parallel',
                    'depends_on': ['architecture']
                },
                {
                    'name': 'integration',
                    'description': 'é›†æˆæµ‹è¯•',
                    'execution_mode': 'sequential',
                    'depends_on': ['implementation'],
                    'quality_gate': {
                        'checklist': 'code_review,testing,security_scan'
                    }
                }
            ]
        }

    def _create_large_workflow_config(self, name: str) -> Dict[str, Any]:
        """åˆ›å»ºå¤§å‹å·¥ä½œæµé…ç½®ï¼ˆç”¨äºå†…å­˜æµ‹è¯•ï¼‰"""
        stages = []

        # åˆ›å»ºå¤šä¸ªä¾èµ–é˜¶æ®µ
        for i in range(10):
            stage = {
                'name': f'stage_{i}',
                'description': f'Large workflow stage {i}',
                'execution_mode': 'parallel' if i % 2 == 0 else 'sequential',
                'timeout': 3600
            }

            if i > 0:
                stage['depends_on'] = [f'stage_{i-1}']

            if i % 3 == 0:
                stage['sync_point'] = {
                    'type': 'validation',
                    'validation_criteria': {
                        'tasks_completed': '> 0',
                        'quality_score': '> 70'
                    }
                }

            stages.append(stage)

        return {
            'name': f'Large Workflow {name}',
            'global_context': {
                'large': True,
                'stages_count': len(stages),
                'complexity': 'high'
            },
            'stages': stages
        }

    def _analyze_load_test_results(self, results: List[LoadTestResult]) -> Dict[str, Any]:
        """åˆ†æè´Ÿè½½æµ‹è¯•ç»“æœ"""
        if not results:
            return {}

        success_rates = [r.success_rate for r in results]
        throughputs = [r.throughput for r in results]
        response_times = [r.avg_completion_time for r in results]

        return {
            'avg_success_rate': statistics.mean(success_rates),
            'min_success_rate': min(success_rates),
            'avg_throughput': statistics.mean(throughputs),
            'max_throughput': max(throughputs),
            'avg_response_time': statistics.mean(response_times),
            'scalability_trend': 'positive' if throughputs[-1] > throughputs[0] else 'negative'
        }

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    print("ğŸ¯ å¯åŠ¨Perfect21æ€§èƒ½æµ‹è¯•å¥—ä»¶")

    test_suite = Perfect21PerformanceTestSuite()

    try:
        # è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•
        final_report = test_suite.run_comprehensive_performance_test()

        print("\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“Š æ•´ä½“æ€§èƒ½è¯„åˆ†: {final_report.get('performance_scores', {}).get('overall', 'N/A')}/100")

        return final_report

    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()