#!/usr/bin/env python3
"""
Perfect21 ç»¼åˆæ€§èƒ½æµ‹è¯•æ‰§è¡Œå™¨
åè°ƒè¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•å¹¶ç”Ÿæˆç»Ÿä¸€æŠ¥å‘Š
"""

import os
import sys
import json
import time
import subprocess
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from perfect21_performance_test_suite import Perfect21PerformanceTestSuite
from database_performance_test import DatabasePerformanceTester

class ComprehensivePerformanceTestRunner:
    """ç»¼åˆæ€§èƒ½æµ‹è¯•æ‰§è¡Œå™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = Path("tests/performance/results")
        self.results_dir.mkdir(exist_ok=True)

        # æµ‹è¯•ç»„ä»¶
        self.system_tester = Perfect21PerformanceTestSuite()
        self.db_tester = DatabasePerformanceTester()

        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.test_results = {}
        self.test_start_time = None

    def run_all_performance_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸ¯ å¯åŠ¨Perfect21ç»¼åˆæ€§èƒ½æµ‹è¯•å¥—ä»¶")
        print("=" * 80)

        self.test_start_time = datetime.now()

        try:
            # 1. ç³»ç»Ÿæ€§èƒ½æµ‹è¯•
            print("\nğŸš€ ç¬¬ä¸€éƒ¨åˆ†ï¼šç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
            print("-" * 50)
            self.test_results['system_performance'] = self._run_system_performance_tests()

            # 2. æ•°æ®åº“æ€§èƒ½æµ‹è¯•
            print("\nğŸ—„ï¸  ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®åº“æ€§èƒ½æµ‹è¯•")
            print("-" * 50)
            self.test_results['database_performance'] = self._run_database_performance_tests()

            # 3. APIè´Ÿè½½æµ‹è¯• (K6)
            print("\nğŸ“¡ ç¬¬ä¸‰éƒ¨åˆ†ï¼šAPIè´Ÿè½½æµ‹è¯•")
            print("-" * 50)
            self.test_results['api_load_tests'] = self._run_k6_load_tests()

            # 4. é›†æˆæ€§èƒ½æµ‹è¯•
            print("\nğŸ”— ç¬¬å››éƒ¨åˆ†ï¼šé›†æˆæ€§èƒ½æµ‹è¯•")
            print("-" * 50)
            self.test_results['integration_performance'] = self._run_integration_performance_tests()

            # 5. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            print("\nğŸ“Š ç¬¬äº”éƒ¨åˆ†ï¼šç”Ÿæˆç»¼åˆæŠ¥å‘Š")
            print("-" * 50)
            comprehensive_report = self._generate_comprehensive_report()

            print("\nâœ… æ‰€æœ‰æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
            self._print_test_summary(comprehensive_report)

            return comprehensive_report

        except Exception as e:
            self.logger.error(f"æ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            raise

    def _run_system_performance_tests(self) -> Dict[str, Any]:
        """è¿è¡Œç³»ç»Ÿæ€§èƒ½æµ‹è¯•"""
        try:
            print("æ‰§è¡Œå·¥ä½œæµç¼–æ’å™¨å’Œå¹¶è¡Œç®¡ç†å™¨æ€§èƒ½æµ‹è¯•...")
            system_results = self.system_tester.run_comprehensive_performance_test()

            print("âœ… ç³»ç»Ÿæ€§èƒ½æµ‹è¯•å®Œæˆ")
            return {
                'status': 'completed',
                'results': system_results,
                'test_duration': (datetime.now() - self.test_start_time).total_seconds()
            }

        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'test_duration': (datetime.now() - self.test_start_time).total_seconds()
            }

    def _run_database_performance_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ•°æ®åº“æ€§èƒ½æµ‹è¯•"""
        try:
            print("æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢ã€æ’å…¥ã€æ›´æ–°å’Œå¹¶å‘æ€§èƒ½æµ‹è¯•...")
            db_results = self.db_tester.run_comprehensive_database_tests()

            print("âœ… æ•°æ®åº“æ€§èƒ½æµ‹è¯•å®Œæˆ")
            return {
                'status': 'completed',
                'results': db_results,
                'test_duration': (datetime.now() - self.test_start_time).total_seconds()
            }

        except Exception as e:
            self.logger.error(f"æ•°æ®åº“æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'test_duration': (datetime.now() - self.test_start_time).total_seconds()
            }

    def _run_k6_load_tests(self) -> Dict[str, Any]:
        """è¿è¡ŒK6è´Ÿè½½æµ‹è¯•"""
        try:
            # æ£€æŸ¥K6æ˜¯å¦å®‰è£…
            k6_available = self._check_k6_installation()
            if not k6_available:
                return {
                    'status': 'skipped',
                    'reason': 'K6 not installed',
                    'message': 'K6è´Ÿè½½æµ‹è¯•è¢«è·³è¿‡ï¼Œè¯·å®‰è£…K6ä»¥æ‰§è¡ŒAPIè´Ÿè½½æµ‹è¯•'
                }

            print("æ‰§è¡ŒK6 APIè´Ÿè½½æµ‹è¯•...")

            # æ£€æŸ¥APIæœåŠ¡æ˜¯å¦è¿è¡Œ
            api_available = self._check_api_service()
            if not api_available:
                return {
                    'status': 'skipped',
                    'reason': 'API service not available',
                    'message': 'APIæœåŠ¡æœªè¿è¡Œï¼Œè·³è¿‡è´Ÿè½½æµ‹è¯•'
                }

            # è¿è¡ŒK6æµ‹è¯•
            k6_script_path = Path(__file__).parent / "k6_load_test.js"
            k6_results = self._execute_k6_test(str(k6_script_path))

            print("âœ… K6è´Ÿè½½æµ‹è¯•å®Œæˆ")
            return {
                'status': 'completed',
                'results': k6_results,
                'test_duration': (datetime.now() - self.test_start_time).total_seconds()
            }

        except Exception as e:
            self.logger.error(f"K6è´Ÿè½½æµ‹è¯•å¤±è´¥: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'test_duration': (datetime.now() - self.test_start_time).total_seconds()
            }

    def _run_integration_performance_tests(self) -> Dict[str, Any]:
        """è¿è¡Œé›†æˆæ€§èƒ½æµ‹è¯•"""
        try:
            print("æ‰§è¡Œç«¯åˆ°ç«¯é›†æˆæ€§èƒ½æµ‹è¯•...")

            integration_results = {
                'workflow_to_database_integration': self._test_workflow_database_integration(),
                'api_to_workflow_integration': self._test_api_workflow_integration(),
                'end_to_end_workflow': self._test_end_to_end_workflow_performance()
            }

            print("âœ… é›†æˆæ€§èƒ½æµ‹è¯•å®Œæˆ")
            return {
                'status': 'completed',
                'results': integration_results,
                'test_duration': (datetime.now() - self.test_start_time).total_seconds()
            }

        except Exception as e:
            self.logger.error(f"é›†æˆæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'test_duration': (datetime.now() - self.test_start_time).total_seconds()
            }

    def _test_workflow_database_integration(self) -> Dict[str, Any]:
        """æµ‹è¯•å·¥ä½œæµä¸æ•°æ®åº“é›†æˆæ€§èƒ½"""
        print("  æµ‹è¯•å·¥ä½œæµçŠ¶æ€æŒä¹…åŒ–æ€§èƒ½...")

        start_time = time.time()
        test_workflows = 100
        successful_operations = 0

        try:
            for i in range(test_workflows):
                # åˆ›å»ºå·¥ä½œæµ
                workflow_config = self._create_integration_test_workflow(f"integration_test_{i}")

                # æµ‹è¯•å®Œæ•´çš„åˆ›å»ºã€æ‰§è¡Œã€çŠ¶æ€æ›´æ–°æµç¨‹
                load_result = self.system_tester.orchestrator.load_workflow(workflow_config)
                if load_result['success']:
                    # æ¨¡æ‹Ÿæ‰§è¡Œå’ŒçŠ¶æ€æ›´æ–°
                    self.system_tester.orchestrator._save_execution_state()
                    successful_operations += 1

                if i % 20 == 0:
                    print(f"    å·²æµ‹è¯• {i} ä¸ªå·¥ä½œæµ...")

            duration = time.time() - start_time
            throughput = successful_operations / duration

            return {
                'total_workflows': test_workflows,
                'successful_operations': successful_operations,
                'test_duration': duration,
                'throughput': throughput,
                'success_rate': successful_operations / test_workflows
            }

        except Exception as e:
            return {'error': str(e)}

    def _test_api_workflow_integration(self) -> Dict[str, Any]:
        """æµ‹è¯•APIä¸å·¥ä½œæµé›†æˆæ€§èƒ½"""
        print("  æµ‹è¯•APIè°ƒç”¨åˆ°å·¥ä½œæµæ‰§è¡Œçš„å®Œæ•´é“¾è·¯...")

        # æ¨¡æ‹ŸAPIè¯·æ±‚å¤„ç†æ€§èƒ½
        start_time = time.time()
        api_requests = 50
        successful_requests = 0

        try:
            for i in range(api_requests):
                # æ¨¡æ‹ŸAPIè¯·æ±‚å¤„ç†
                request_start = time.time()

                # åˆ›å»ºå·¥ä½œæµï¼ˆæ¨¡æ‹ŸAPIè°ƒç”¨ï¼‰
                workflow_config = self._create_integration_test_workflow(f"api_integration_{i}")
                load_result = self.system_tester.orchestrator.load_workflow(workflow_config)

                if load_result['success']:
                    # æ¨¡æ‹Ÿé˜¶æ®µæ‰§è¡Œ
                    stages = list(workflow_config['stages'])
                    if stages:
                        stage_result = self.system_tester.orchestrator.execute_stage(stages[0]['name'])
                        if stage_result['success']:
                            successful_requests += 1

                if i % 10 == 0:
                    print(f"    å·²å¤„ç† {i} ä¸ªAPIè¯·æ±‚...")

            duration = time.time() - start_time
            throughput = successful_requests / duration

            return {
                'total_requests': api_requests,
                'successful_requests': successful_requests,
                'test_duration': duration,
                'throughput': throughput,
                'avg_response_time': duration / api_requests
            }

        except Exception as e:
            return {'error': str(e)}

    def _test_end_to_end_workflow_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµæ€§èƒ½"""
        print("  æµ‹è¯•å®Œæ•´å·¥ä½œæµç”Ÿå‘½å‘¨æœŸæ€§èƒ½...")

        start_time = time.time()
        workflows = 20
        completed_workflows = 0

        workflow_metrics = []

        try:
            for i in range(workflows):
                workflow_start = time.time()

                # åˆ›å»ºå¤æ‚å·¥ä½œæµ
                workflow_config = self._create_complex_integration_workflow(f"e2e_test_{i}")

                # æ‰§è¡Œå®Œæ•´ç”Ÿå‘½å‘¨æœŸ
                load_result = self.system_tester.orchestrator.load_workflow(workflow_config)
                if load_result['success']:
                    # æ‰§è¡Œæ‰€æœ‰é˜¶æ®µ
                    stages = list(workflow_config['stages'])
                    stage_execution_times = []

                    for stage_config in stages:
                        stage_start = time.time()
                        stage_result = self.system_tester.orchestrator.execute_stage(stage_config['name'])
                        stage_duration = time.time() - stage_start
                        stage_execution_times.append(stage_duration)

                        if not stage_result['success']:
                            break

                    workflow_duration = time.time() - workflow_start
                    workflow_metrics.append({
                        'workflow_id': f"e2e_test_{i}",
                        'total_duration': workflow_duration,
                        'stage_count': len(stages),
                        'stage_execution_times': stage_execution_times,
                        'avg_stage_time': sum(stage_execution_times) / len(stage_execution_times) if stage_execution_times else 0
                    })

                    completed_workflows += 1

                if i % 5 == 0:
                    print(f"    å·²å®Œæˆ {i} ä¸ªç«¯åˆ°ç«¯å·¥ä½œæµ...")

            total_duration = time.time() - start_time

            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            if workflow_metrics:
                avg_workflow_duration = sum(w['total_duration'] for w in workflow_metrics) / len(workflow_metrics)
                max_workflow_duration = max(w['total_duration'] for w in workflow_metrics)
                min_workflow_duration = min(w['total_duration'] for w in workflow_metrics)
            else:
                avg_workflow_duration = max_workflow_duration = min_workflow_duration = 0

            return {
                'total_workflows': workflows,
                'completed_workflows': completed_workflows,
                'test_duration': total_duration,
                'success_rate': completed_workflows / workflows,
                'avg_workflow_duration': avg_workflow_duration,
                'max_workflow_duration': max_workflow_duration,
                'min_workflow_duration': min_workflow_duration,
                'workflow_metrics': workflow_metrics
            }

        except Exception as e:
            return {'error': str(e)}

    def _check_k6_installation(self) -> bool:
        """æ£€æŸ¥K6æ˜¯å¦å®‰è£…"""
        try:
            result = subprocess.run(['k6', 'version'], capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False

    def _check_api_service(self) -> bool:
        """æ£€æŸ¥APIæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            import requests
            response = requests.get('http://localhost:8000/health', timeout=5)
            return response.status_code == 200
        except:
            return False

    def _execute_k6_test(self, script_path: str) -> Dict[str, Any]:
        """æ‰§è¡ŒK6æµ‹è¯•"""
        try:
            # è¿è¡ŒK6è´Ÿè½½æµ‹è¯•
            cmd = [
                'k6', 'run',
                '--out', f'json={self.results_dir}/k6_results.json',
                '--env', 'BASE_URL=http://localhost:8000',
                '--env', 'TEST_TYPE=load',
                script_path
            ]

            print(f"æ‰§è¡ŒK6å‘½ä»¤: {' '.join(cmd)}")

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶
            )

            if result.returncode == 0:
                # è§£æK6ç»“æœ
                k6_output = result.stdout
                return {
                    'status': 'success',
                    'output': k6_output,
                    'stderr': result.stderr,
                    'results_file': f'{self.results_dir}/k6_results.json'
                }
            else:
                return {
                    'status': 'failed',
                    'error': result.stderr,
                    'output': result.stdout
                }

        except subprocess.TimeoutExpired:
            return {
                'status': 'timeout',
                'error': 'K6 test timed out after 30 minutes'
            }
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e)
            }

    def _create_integration_test_workflow(self, workflow_id: str) -> Dict[str, Any]:
        """åˆ›å»ºé›†æˆæµ‹è¯•å·¥ä½œæµé…ç½®"""
        return {
            'name': f'Integration Test Workflow {workflow_id}',
            'global_context': {'integration_test': True, 'workflow_id': workflow_id},
            'stages': [
                {
                    'name': 'setup',
                    'description': 'åˆå§‹åŒ–é˜¶æ®µ',
                    'execution_mode': 'sequential'
                },
                {
                    'name': 'execution',
                    'description': 'æ‰§è¡Œé˜¶æ®µ',
                    'execution_mode': 'parallel',
                    'depends_on': ['setup']
                }
            ]
        }

    def _create_complex_integration_workflow(self, workflow_id: str) -> Dict[str, Any]:
        """åˆ›å»ºå¤æ‚çš„é›†æˆæµ‹è¯•å·¥ä½œæµé…ç½®"""
        return {
            'name': f'Complex Integration Workflow {workflow_id}',
            'global_context': {
                'integration_test': True,
                'complexity': 'high',
                'workflow_id': workflow_id
            },
            'stages': [
                {
                    'name': 'requirements',
                    'description': 'éœ€æ±‚åˆ†æ',
                    'execution_mode': 'parallel'
                },
                {
                    'name': 'design',
                    'description': 'è®¾è®¡é˜¶æ®µ',
                    'execution_mode': 'sequential',
                    'depends_on': ['requirements'],
                    'sync_point': {
                        'type': 'validation',
                        'validation_criteria': {'tasks_completed': '> 0'}
                    }
                },
                {
                    'name': 'implementation',
                    'description': 'å®ç°é˜¶æ®µ',
                    'execution_mode': 'parallel',
                    'depends_on': ['design']
                },
                {
                    'name': 'testing',
                    'description': 'æµ‹è¯•é˜¶æ®µ',
                    'execution_mode': 'sequential',
                    'depends_on': ['implementation'],
                    'quality_gate': {
                        'checklist': 'unit_tests,integration_tests',
                        'must_pass': True
                    }
                }
            ]
        }

    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        test_end_time = datetime.now()
        total_test_duration = (test_end_time - self.test_start_time).total_seconds()

        report = {
            'test_metadata': {
                'test_start_time': self.test_start_time.isoformat(),
                'test_end_time': test_end_time.isoformat(),
                'total_test_duration': total_test_duration,
                'test_environment': self._get_test_environment_info()
            },
            'test_results': self.test_results,
            'performance_summary': self._calculate_overall_performance_summary(),
            'bottleneck_analysis': self._analyze_performance_bottlenecks(),
            'recommendations': self._generate_comprehensive_recommendations(),
            'executive_summary': self._generate_executive_summary()
        }

        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        self._save_comprehensive_report(report)

        return report

    def _get_test_environment_info(self) -> Dict[str, Any]:
        """è·å–æµ‹è¯•ç¯å¢ƒä¿¡æ¯"""
        import platform
        import psutil

        return {
            'platform': platform.platform(),
            'python_version': platform.python_version(),
            'cpu_count': psutil.cpu_count(),
            'memory_total_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
            'disk_space_gb': psutil.disk_usage('/').total / 1024 / 1024 / 1024
        }

    def _calculate_overall_performance_summary(self) -> Dict[str, Any]:
        """è®¡ç®—æ•´ä½“æ€§èƒ½æ‘˜è¦"""
        summary = {}

        # ç³»ç»Ÿæ€§èƒ½æ‘˜è¦
        if 'system_performance' in self.test_results and self.test_results['system_performance']['status'] == 'completed':
            system_results = self.test_results['system_performance']['results']
            if 'performance_scores' in system_results:
                summary['system_performance_score'] = system_results['performance_scores'].get('overall', 0)

        # æ•°æ®åº“æ€§èƒ½æ‘˜è¦
        if 'database_performance' in self.test_results and self.test_results['database_performance']['status'] == 'completed':
            db_results = self.test_results['database_performance']['results']
            if 'performance_summary' in db_results and 'overall' in db_results['performance_summary']:
                summary['database_performance_score'] = db_results['performance_summary']['overall'].get('performance_score', 0)

        # APIæ€§èƒ½æ‘˜è¦
        if 'api_load_tests' in self.test_results and self.test_results['api_load_tests']['status'] == 'completed':
            summary['api_load_test_status'] = 'completed'
        else:
            summary['api_load_test_status'] = 'failed_or_skipped'

        # é›†æˆæ€§èƒ½æ‘˜è¦
        if 'integration_performance' in self.test_results and self.test_results['integration_performance']['status'] == 'completed':
            integration_results = self.test_results['integration_performance']['results']
            e2e_results = integration_results.get('end_to_end_workflow', {})
            summary['integration_success_rate'] = e2e_results.get('success_rate', 0)

        # è®¡ç®—æ€»ä½“è¯„åˆ†
        scores = []
        if 'system_performance_score' in summary:
            scores.append(summary['system_performance_score'])
        if 'database_performance_score' in summary:
            scores.append(summary['database_performance_score'])
        if summary.get('integration_success_rate', 0) > 0:
            scores.append(summary['integration_success_rate'] * 100)

        if scores:
            summary['overall_performance_score'] = sum(scores) / len(scores)
        else:
            summary['overall_performance_score'] = 0

        return summary

    def _analyze_performance_bottlenecks(self) -> List[Dict[str, Any]]:
        """åˆ†ææ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []

        # åˆ†æç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆ
        if 'system_performance' in self.test_results:
            system_results = self.test_results['system_performance']
            if system_results['status'] == 'completed':
                # æ£€æŸ¥è´Ÿè½½æµ‹è¯•ç»“æœ
                load_tests = system_results['results'].get('load_tests', {})
                if 'results' in load_tests:
                    for result in load_tests['results']:
                        if hasattr(result, 'bottlenecks') and result.bottlenecks:
                            bottlenecks.extend([
                                {
                                    'category': 'system_load',
                                    'severity': 'medium',
                                    'description': bottleneck,
                                    'affected_component': 'workflow_orchestrator'
                                }
                                for bottleneck in result.bottlenecks
                            ])

        # åˆ†ææ•°æ®åº“æ€§èƒ½ç“¶é¢ˆ
        if 'database_performance' in self.test_results:
            db_results = self.test_results['database_performance']
            if db_results['status'] == 'completed':
                perf_summary = db_results['results'].get('performance_summary', {})
                if 'overall' in perf_summary:
                    overall = perf_summary['overall']
                    if overall.get('avg_response_time', 0) > 0.1:
                        bottlenecks.append({
                            'category': 'database_performance',
                            'severity': 'high' if overall['avg_response_time'] > 0.5 else 'medium',
                            'description': f"æ•°æ®åº“å¹³å‡å“åº”æ—¶é—´è¿‡é«˜: {overall['avg_response_time']:.3f}s",
                            'affected_component': 'database'
                        })

        return bottlenecks

    def _generate_comprehensive_recommendations(self) -> List[str]:
        """ç”Ÿæˆç»¼åˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºæ€§èƒ½æ‘˜è¦ç”Ÿæˆå»ºè®®
        summary = self._calculate_overall_performance_summary()

        if summary.get('overall_performance_score', 0) < 70:
            recommendations.append("æ•´ä½“æ€§èƒ½éœ€è¦æ”¹è¿›ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†å…³é”®æ€§èƒ½ç“¶é¢ˆ")

        if summary.get('system_performance_score', 0) < 60:
            recommendations.append("å·¥ä½œæµç¼–æ’å™¨æ€§èƒ½è¾ƒä½ï¼Œè€ƒè™‘ä¼˜åŒ–ä»»åŠ¡è°ƒåº¦å’Œèµ„æºç®¡ç†")

        if summary.get('database_performance_score', 0) < 60:
            recommendations.append("æ•°æ®åº“æ€§èƒ½éœ€è¦ä¼˜åŒ–ï¼Œå»ºè®®æ£€æŸ¥ç´¢å¼•ç­–ç•¥å’ŒæŸ¥è¯¢ä¼˜åŒ–")

        if summary.get('integration_success_rate', 0) < 0.9:
            recommendations.append("é›†æˆæµ‹è¯•æˆåŠŸç‡è¾ƒä½ï¼Œéœ€è¦æ”¹è¿›é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")

        # é€šç”¨å»ºè®®
        recommendations.extend([
            "å»ºè®®å®æ–½æŒç»­æ€§èƒ½ç›‘æ§ï¼ŒåŠæ—¶å‘ç°æ€§èƒ½å›å½’",
            "è€ƒè™‘å®ç°æ€§èƒ½é¢„ç®—ï¼Œç¡®ä¿æ–°åŠŸèƒ½ä¸å½±å“æ•´ä½“æ€§èƒ½",
            "å®šæœŸè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œè·Ÿè¸ªæ€§èƒ½è¶‹åŠ¿",
            "ä¼˜åŒ–å…³é”®è·¯å¾„ä¸Šçš„ç»„ä»¶ï¼Œæé«˜ç”¨æˆ·ä½“éªŒ"
        ])

        return recommendations

    def _generate_executive_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        summary = self._calculate_overall_performance_summary()
        bottlenecks = self._analyze_performance_bottlenecks()

        # ç¡®å®šæ•´ä½“çŠ¶æ€
        overall_score = summary.get('overall_performance_score', 0)
        if overall_score >= 80:
            status = 'excellent'
            status_description = 'æ€§èƒ½è¡¨ç°ä¼˜ç§€'
        elif overall_score >= 60:
            status = 'good'
            status_description = 'æ€§èƒ½è¡¨ç°è‰¯å¥½'
        elif overall_score >= 40:
            status = 'fair'
            status_description = 'æ€§èƒ½è¡¨ç°ä¸€èˆ¬ï¼Œéœ€è¦æ”¹è¿›'
        else:
            status = 'poor'
            status_description = 'æ€§èƒ½è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦ç«‹å³ä¼˜åŒ–'

        return {
            'overall_status': status,
            'status_description': status_description,
            'performance_score': overall_score,
            'total_bottlenecks': len(bottlenecks),
            'critical_bottlenecks': len([b for b in bottlenecks if b.get('severity') == 'high']),
            'key_metrics': {
                'system_performance': summary.get('system_performance_score', 0),
                'database_performance': summary.get('database_performance_score', 0),
                'integration_success_rate': summary.get('integration_success_rate', 0)
            },
            'priority_actions': self._get_priority_actions(bottlenecks)
        }

    def _get_priority_actions(self, bottlenecks: List[Dict]) -> List[str]:
        """è·å–ä¼˜å…ˆè¡ŒåŠ¨é¡¹"""
        actions = []

        high_severity = [b for b in bottlenecks if b.get('severity') == 'high']
        if high_severity:
            actions.append(f"ç«‹å³å¤„ç† {len(high_severity)} ä¸ªé«˜ä¸¥é‡æ€§æ€§èƒ½é—®é¢˜")

        medium_severity = [b for b in bottlenecks if b.get('severity') == 'medium']
        if medium_severity:
            actions.append(f"è®¡åˆ’å¤„ç† {len(medium_severity)} ä¸ªä¸­ç­‰ä¸¥é‡æ€§æ€§èƒ½é—®é¢˜")

        if not bottlenecks:
            actions.append("ç»§ç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡ï¼Œä¿æŒå½“å‰æ€§èƒ½æ°´å¹³")

        return actions

    def _save_comprehensive_report(self, report: Dict[str, Any]):
        """ä¿å­˜ç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # JSONè¯¦ç»†æŠ¥å‘Š
        json_filename = self.results_dir / f"comprehensive_performance_report_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)

        # HTMLæŠ¥å‘Š
        html_filename = self.results_dir / f"performance_dashboard_{timestamp}.html"
        self._generate_html_report(report, html_filename)

        # æ–‡æœ¬æ‘˜è¦
        txt_filename = self.results_dir / f"performance_executive_summary_{timestamp}.txt"
        self._generate_text_summary(report, txt_filename)

        print(f"\nğŸ“„ ç»¼åˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"  â€¢ JSONè¯¦ç»†æŠ¥å‘Š: {json_filename}")
        print(f"  â€¢ HTMLå¯è§†åŒ–æŠ¥å‘Š: {html_filename}")
        print(f"  â€¢ æ‰§è¡Œæ‘˜è¦: {txt_filename}")

    def _generate_html_report(self, report: Dict[str, Any], filename: Path):
        """ç”ŸæˆHTMLå¯è§†åŒ–æŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Perfect21 æ€§èƒ½æµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        .header {{
            text-align: center;
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 3px solid #007acc;
        }}
        .score-card {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            margin: 20px 0;
        }}
        .metrics-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }}
        .metric-card {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #007acc;
        }}
        .status-excellent {{ color: #28a745; }}
        .status-good {{ color: #17a2b8; }}
        .status-fair {{ color: #ffc107; }}
        .status-poor {{ color: #dc3545; }}
        .bottleneck {{
            background: #fff3cd;
            border: 1px solid #ffeaa7;
            padding: 10px;
            margin: 10px 0;
            border-radius: 5px;
        }}
        .recommendations {{
            background: #e7f3ff;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¯ Perfect21 æ€§èƒ½æµ‹è¯•æŠ¥å‘Š</h1>
            <p>æµ‹è¯•æ—¶é—´: {report['test_metadata']['test_start_time']} - {report['test_metadata']['test_end_time']}</p>
            <p>æ€»æµ‹è¯•æ—¶é•¿: {report['test_metadata']['total_test_duration']:.1f} ç§’</p>
        </div>

        <div class="score-card">
            <h2>æ•´ä½“æ€§èƒ½è¯„åˆ†</h2>
            <h1 class="status-{report['executive_summary']['overall_status']}">{report['executive_summary']['performance_score']:.1f}/100</h1>
            <p>{report['executive_summary']['status_description']}</p>
        </div>

        <div class="metrics-grid">
            <div class="metric-card">
                <h3>ğŸš€ ç³»ç»Ÿæ€§èƒ½</h3>
                <p><strong>{report['performance_summary'].get('system_performance_score', 'N/A')}</strong>/100</p>
                <p>å·¥ä½œæµç¼–æ’å™¨å’Œå¹¶è¡Œç®¡ç†å™¨æ€§èƒ½</p>
            </div>
            <div class="metric-card">
                <h3>ğŸ—„ï¸ æ•°æ®åº“æ€§èƒ½</h3>
                <p><strong>{report['performance_summary'].get('database_performance_score', 'N/A')}</strong>/100</p>
                <p>æ•°æ®æŸ¥è¯¢ã€æ’å…¥ã€æ›´æ–°æ€§èƒ½</p>
            </div>
            <div class="metric-card">
                <h3>ğŸ”— é›†æˆæµ‹è¯•</h3>
                <p><strong>{report['performance_summary'].get('integration_success_rate', 0)*100:.1f}%</strong></p>
                <p>ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•æˆåŠŸç‡</p>
            </div>
            <div class="metric-card">
                <h3>ğŸ“¡ APIè´Ÿè½½æµ‹è¯•</h3>
                <p><strong>{report['performance_summary'].get('api_load_test_status', 'N/A')}</strong></p>
                <p>APIè´Ÿè½½æµ‹è¯•æ‰§è¡ŒçŠ¶æ€</p>
            </div>
        </div>

        <h2>ğŸ” æ€§èƒ½ç“¶é¢ˆåˆ†æ</h2>
        <div>
            æ€»è®¡å‘ç° <strong>{report['executive_summary']['total_bottlenecks']}</strong> ä¸ªæ€§èƒ½é—®é¢˜ï¼Œ
            å…¶ä¸­ <strong>{report['executive_summary']['critical_bottlenecks']}</strong> ä¸ªä¸ºé«˜ä¸¥é‡æ€§ã€‚
        </div>
        {self._format_bottlenecks_html(report.get('bottleneck_analysis', []))}

        <div class="recommendations">
            <h2>ğŸ’¡ ä¼˜åŒ–å»ºè®®</h2>
            <h3>ä¼˜å…ˆè¡ŒåŠ¨é¡¹:</h3>
            <ul>
                {"".join(f"<li>{action}</li>" for action in report['executive_summary']['priority_actions'])}
            </ul>
            <h3>è¯¦ç»†å»ºè®®:</h3>
            <ul>
                {"".join(f"<li>{rec}</li>" for rec in report['recommendations'])}
            </ul>
        </div>
    </div>
</body>
</html>
        """

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(html_content)

    def _format_bottlenecks_html(self, bottlenecks: List[Dict]) -> str:
        """æ ¼å¼åŒ–ç“¶é¢ˆä¿¡æ¯ä¸ºHTML"""
        if not bottlenecks:
            return "<p>âœ… æœªå‘ç°æ˜æ˜¾çš„æ€§èƒ½ç“¶é¢ˆ</p>"

        html = ""
        for bottleneck in bottlenecks:
            severity_class = f"status-{bottleneck.get('severity', 'fair')}"
            html += f"""
            <div class="bottleneck">
                <strong class="{severity_class}">[{bottleneck.get('severity', 'unknown').upper()}]</strong>
                {bottleneck.get('description', 'No description')}
                <small>(ç»„ä»¶: {bottleneck.get('affected_component', 'unknown')})</small>
            </div>
            """
        return html

    def _generate_text_summary(self, report: Dict[str, Any], filename: Path):
        """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦æŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("Perfect21 æ€§èƒ½æµ‹è¯•æ‰§è¡Œæ‘˜è¦\n")
            f.write("=" * 50 + "\n\n")

            # æ‰§è¡Œæ‘˜è¦
            exec_summary = report['executive_summary']
            f.write(f"æ•´ä½“çŠ¶æ€: {exec_summary['status_description']}\n")
            f.write(f"æ€§èƒ½è¯„åˆ†: {exec_summary['performance_score']:.1f}/100\n")
            f.write(f"æµ‹è¯•æ—¶é•¿: {report['test_metadata']['total_test_duration']:.1f}ç§’\n\n")

            # å…³é”®æŒ‡æ ‡
            f.write("å…³é”®æ€§èƒ½æŒ‡æ ‡:\n")
            f.write("-" * 30 + "\n")
            key_metrics = exec_summary['key_metrics']
            f.write(f"ç³»ç»Ÿæ€§èƒ½è¯„åˆ†: {key_metrics['system_performance']:.1f}/100\n")
            f.write(f"æ•°æ®åº“æ€§èƒ½è¯„åˆ†: {key_metrics['database_performance']:.1f}/100\n")
            f.write(f"é›†æˆæµ‹è¯•æˆåŠŸç‡: {key_metrics['integration_success_rate']*100:.1f}%\n\n")

            # ä¼˜å…ˆè¡ŒåŠ¨é¡¹
            f.write("ä¼˜å…ˆè¡ŒåŠ¨é¡¹:\n")
            f.write("-" * 30 + "\n")
            for action in exec_summary['priority_actions']:
                f.write(f"â€¢ {action}\n")

    def _print_test_summary(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        exec_summary = report['executive_summary']

        print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•æ‘˜è¦")
        print("=" * 50)
        print(f"æ•´ä½“æ€§èƒ½è¯„åˆ†: {exec_summary['performance_score']:.1f}/100")
        print(f"çŠ¶æ€: {exec_summary['status_description']}")
        print(f"æ€»æµ‹è¯•æ—¶é•¿: {report['test_metadata']['total_test_duration']:.1f}ç§’")

        print(f"\nğŸ” å‘ç°é—®é¢˜:")
        print(f"â€¢ æ€»è®¡æ€§èƒ½é—®é¢˜: {exec_summary['total_bottlenecks']}ä¸ª")
        print(f"â€¢ é«˜ä¸¥é‡æ€§é—®é¢˜: {exec_summary['critical_bottlenecks']}ä¸ª")

        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
        for action in exec_summary['priority_actions']:
            print(f"â€¢ {action}")

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
    runner = ComprehensivePerformanceTestRunner()

    try:
        # è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•
        comprehensive_report = runner.run_all_performance_tests()

        return comprehensive_report

    except KeyboardInterrupt:
        print("\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return None
    except Exception as e:
        print(f"\nâŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()