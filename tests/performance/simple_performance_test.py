#!/usr/bin/env python3
"""
Perfect21 ç®€åŒ–æ€§èƒ½æµ‹è¯•
ä¸“æ³¨äºæ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡çš„å¿«é€Ÿè¯„ä¼°
"""

import os
import sys
import time
import json
import psutil
import statistics
import threading
import concurrent.futures
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

@dataclass
class PerformanceResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""
    test_name: str
    duration: float
    operations_count: int
    success_count: int
    failure_count: int
    throughput: float
    avg_response_time: float
    cpu_usage: float
    memory_usage: float

class SimplePerformanceMonitor:
    """ç®€å•æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.monitoring = False
        self.cpu_readings = []
        self.memory_readings = []

    def start(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.cpu_readings = []
        self.memory_readings = []

        def monitor():
            while self.monitoring:
                try:
                    self.cpu_readings.append(psutil.cpu_percent())
                    self.memory_readings.append(psutil.virtual_memory().percent)
                    time.sleep(0.5)
                except:
                    break

        self.thread = threading.Thread(target=monitor, daemon=True)
        self.thread.start()

    def stop(self) -> Dict[str, float]:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›ç»Ÿè®¡"""
        self.monitoring = False

        if self.cpu_readings and self.memory_readings:
            return {
                'avg_cpu': statistics.mean(self.cpu_readings),
                'max_cpu': max(self.cpu_readings),
                'avg_memory': statistics.mean(self.memory_readings),
                'max_memory': max(self.memory_readings)
            }
        return {'avg_cpu': 0, 'max_cpu': 0, 'avg_memory': 0, 'max_memory': 0}

class Perfect21SimplePerformanceTest:
    """Perfect21 ç®€åŒ–æ€§èƒ½æµ‹è¯•"""

    def __init__(self):
        self.results = []
        self.monitor = SimplePerformanceMonitor()

        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        self.results_dir = Path("tests/performance/results")
        self.results_dir.mkdir(exist_ok=True)

    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸ¯ Perfect21 æ€§èƒ½æµ‹è¯•å¼€å§‹")
        print("=" * 60)

        test_start = datetime.now()

        # 1. å·¥ä½œæµåˆ›å»ºæ€§èƒ½æµ‹è¯•
        self.results.append(self.test_workflow_creation())

        # 2. å¹¶å‘å·¥ä½œæµæµ‹è¯•
        self.results.append(self.test_concurrent_workflows())

        # 3. å†…å­˜ä½¿ç”¨æµ‹è¯•
        self.results.append(self.test_memory_usage())

        # 4. å“åº”æ—¶é—´æµ‹è¯•
        self.results.append(self.test_response_times())

        # 5. å‹åŠ›æµ‹è¯•
        self.results.append(self.test_stress_performance())

        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(test_start)
        self.save_report(report)

        return report

    def test_workflow_creation(self) -> PerformanceResult:
        """æµ‹è¯•å·¥ä½œæµåˆ›å»ºæ€§èƒ½"""
        print("\nğŸ“ 1. å·¥ä½œæµåˆ›å»ºæ€§èƒ½æµ‹è¯•")
        print("-" * 40)

        self.monitor.start()
        start_time = time.time()

        operations = 500
        success_count = 0
        response_times = []

        # æ¨¡æ‹Ÿå·¥ä½œæµåˆ›å»º
        for i in range(operations):
            op_start = time.time()

            try:
                # æ¨¡æ‹Ÿå·¥ä½œæµåˆ›å»ºæ“ä½œ
                workflow_config = self.create_mock_workflow(f"test_{i}")
                time.sleep(0.001)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

                response_time = time.time() - op_start
                response_times.append(response_time)
                success_count += 1

                if i % 100 == 0:
                    print(f"  å·²åˆ›å»º {i} ä¸ªå·¥ä½œæµ...")

            except Exception:
                pass

        duration = time.time() - start_time
        resource_stats = self.monitor.stop()

        result = PerformanceResult(
            test_name="workflow_creation",
            duration=duration,
            operations_count=operations,
            success_count=success_count,
            failure_count=operations - success_count,
            throughput=success_count / duration,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            cpu_usage=resource_stats['avg_cpu'],
            memory_usage=resource_stats['avg_memory']
        )

        print(f"âœ… å·¥ä½œæµåˆ›å»ºæµ‹è¯•å®Œæˆ: {success_count}/{operations} æˆåŠŸ")
        print(f"   ååé‡: {result.throughput:.1f} workflows/s")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {result.avg_response_time:.3f}s")

        return result

    def test_concurrent_workflows(self) -> PerformanceResult:
        """æµ‹è¯•å¹¶å‘å·¥ä½œæµæ€§èƒ½"""
        print("\nğŸš€ 2. å¹¶å‘å·¥ä½œæµæµ‹è¯•")
        print("-" * 40)

        self.monitor.start()
        start_time = time.time()

        concurrent_users = 10
        operations_per_user = 20
        total_operations = concurrent_users * operations_per_user

        success_count = 0
        failure_count = 0
        response_times = []
        lock = threading.Lock()

        def worker(worker_id):
            nonlocal success_count, failure_count
            worker_success = 0
            worker_failures = 0
            worker_times = []

            for i in range(operations_per_user):
                op_start = time.time()

                try:
                    # æ¨¡æ‹Ÿå¹¶å‘å·¥ä½œæµæ“ä½œ
                    workflow_config = self.create_mock_workflow(f"concurrent_{worker_id}_{i}")
                    time.sleep(0.005)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

                    response_time = time.time() - op_start
                    worker_times.append(response_time)
                    worker_success += 1

                except Exception:
                    worker_failures += 1

            with lock:
                success_count += worker_success
                failure_count += worker_failures
                response_times.extend(worker_times)

        # å¯åŠ¨å¹¶å‘å·¥ä½œçº¿ç¨‹
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(worker, i) for i in range(concurrent_users)]
            concurrent.futures.wait(futures)

        duration = time.time() - start_time
        resource_stats = self.monitor.stop()

        result = PerformanceResult(
            test_name="concurrent_workflows",
            duration=duration,
            operations_count=total_operations,
            success_count=success_count,
            failure_count=failure_count,
            throughput=success_count / duration,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            cpu_usage=resource_stats['avg_cpu'],
            memory_usage=resource_stats['avg_memory']
        )

        print(f"âœ… å¹¶å‘æµ‹è¯•å®Œæˆ: {success_count}/{total_operations} æˆåŠŸ")
        print(f"   å¹¶å‘ç”¨æˆ·: {concurrent_users}")
        print(f"   ååé‡: {result.throughput:.1f} ops/s")

        return result

    def test_memory_usage(self) -> PerformanceResult:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æ€§èƒ½"""
        print("\nğŸ§  3. å†…å­˜ä½¿ç”¨æµ‹è¯•")
        print("-" * 40)

        self.monitor.start()
        start_time = time.time()

        operations = 1000
        success_count = 0

        # åˆ›å»ºå¤§é‡å¯¹è±¡æµ‹è¯•å†…å­˜ä½¿ç”¨
        workflows = []

        initial_memory = psutil.virtual_memory().used / 1024 / 1024  # MB

        for i in range(operations):
            try:
                # åˆ›å»ºæ¨¡æ‹Ÿå·¥ä½œæµå¯¹è±¡
                workflow = self.create_large_mock_workflow(f"memory_test_{i}")
                workflows.append(workflow)
                success_count += 1

                if i % 200 == 0:
                    current_memory = psutil.virtual_memory().used / 1024 / 1024
                    print(f"  å·²åˆ›å»º {i} ä¸ªå¯¹è±¡ï¼Œå†…å­˜ä½¿ç”¨: {current_memory - initial_memory:.1f}MB")

            except Exception:
                pass

        duration = time.time() - start_time
        resource_stats = self.monitor.stop()

        final_memory = psutil.virtual_memory().used / 1024 / 1024
        memory_growth = final_memory - initial_memory

        result = PerformanceResult(
            test_name="memory_usage",
            duration=duration,
            operations_count=operations,
            success_count=success_count,
            failure_count=operations - success_count,
            throughput=success_count / duration,
            avg_response_time=duration / operations,
            cpu_usage=resource_stats['avg_cpu'],
            memory_usage=resource_stats['max_memory']
        )

        print(f"âœ… å†…å­˜æµ‹è¯•å®Œæˆ: {success_count} ä¸ªå¯¹è±¡åˆ›å»º")
        print(f"   å†…å­˜å¢é•¿: {memory_growth:.1f}MB")
        print(f"   å¹³å‡æ¯å¯¹è±¡: {memory_growth/success_count:.3f}MB" if success_count > 0 else "   å¹³å‡æ¯å¯¹è±¡: N/A")

        return result

    def test_response_times(self) -> PerformanceResult:
        """æµ‹è¯•å“åº”æ—¶é—´æ€§èƒ½"""
        print("\nâ±ï¸  4. å“åº”æ—¶é—´æµ‹è¯•")
        print("-" * 40)

        self.monitor.start()
        start_time = time.time()

        operations = 300
        success_count = 0
        response_times = []

        # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„æ“ä½œå“åº”æ—¶é—´
        complexities = [0.001, 0.005, 0.01]  # ç®€å•ã€ä¸­ç­‰ã€å¤æ‚

        for i in range(operations):
            complexity = complexities[i % len(complexities)]
            op_start = time.time()

            try:
                # æ¨¡æ‹Ÿä¸åŒå¤æ‚åº¦çš„æ“ä½œ
                workflow = self.create_mock_workflow(f"response_test_{i}")
                time.sleep(complexity)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

                response_time = time.time() - op_start
                response_times.append(response_time)
                success_count += 1

            except Exception:
                pass

        duration = time.time() - start_time
        resource_stats = self.monitor.stop()

        # è®¡ç®—å“åº”æ—¶é—´ç»Ÿè®¡
        if response_times:
            avg_response = statistics.mean(response_times)
            p95_response = sorted(response_times)[int(len(response_times) * 0.95)]
            max_response = max(response_times)
        else:
            avg_response = p95_response = max_response = 0

        result = PerformanceResult(
            test_name="response_times",
            duration=duration,
            operations_count=operations,
            success_count=success_count,
            failure_count=operations - success_count,
            throughput=success_count / duration,
            avg_response_time=avg_response,
            cpu_usage=resource_stats['avg_cpu'],
            memory_usage=resource_stats['avg_memory']
        )

        print(f"âœ… å“åº”æ—¶é—´æµ‹è¯•å®Œæˆ: {success_count} æ¬¡æ“ä½œ")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response:.3f}s")
        print(f"   P95å“åº”æ—¶é—´: {p95_response:.3f}s")
        print(f"   æœ€å¤§å“åº”æ—¶é—´: {max_response:.3f}s")

        return result

    def test_stress_performance(self) -> PerformanceResult:
        """æµ‹è¯•å‹åŠ›æ€§èƒ½"""
        print("\nğŸ”¥ 5. å‹åŠ›æµ‹è¯•")
        print("-" * 40)

        self.monitor.start()
        start_time = time.time()

        test_duration = 30  # 30ç§’å‹åŠ›æµ‹è¯•
        operations_count = 0
        success_count = 0
        response_times = []

        print(f"   è¿è¡Œ {test_duration} ç§’å‹åŠ›æµ‹è¯•...")

        while time.time() - start_time < test_duration:
            op_start = time.time()

            try:
                # é«˜é¢‘æ“ä½œ
                workflow = self.create_mock_workflow(f"stress_{operations_count}")
                time.sleep(0.002)  # æ¨¡æ‹Ÿå¿«é€Ÿæ“ä½œ

                response_time = time.time() - op_start
                response_times.append(response_time)
                success_count += 1

            except Exception:
                pass

            operations_count += 1

            # æ¯5ç§’æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
            if operations_count % 500 == 0:
                elapsed = time.time() - start_time
                current_throughput = operations_count / elapsed
                print(f"   {elapsed:.1f}s: {operations_count} æ“ä½œ, {current_throughput:.1f} ops/s")

        duration = time.time() - start_time
        resource_stats = self.monitor.stop()

        result = PerformanceResult(
            test_name="stress_test",
            duration=duration,
            operations_count=operations_count,
            success_count=success_count,
            failure_count=operations_count - success_count,
            throughput=success_count / duration,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            cpu_usage=resource_stats['avg_cpu'],
            memory_usage=resource_stats['avg_memory']
        )

        print(f"âœ… å‹åŠ›æµ‹è¯•å®Œæˆ: {success_count}/{operations_count} æˆåŠŸ")
        print(f"   æŒç»­æ—¶é—´: {duration:.1f}s")
        print(f"   å³°å€¼ååé‡: {result.throughput:.1f} ops/s")
        print(f"   CPUä½¿ç”¨: {resource_stats['max_cpu']:.1f}%")

        return result

    def create_mock_workflow(self, workflow_id: str) -> Dict[str, Any]:
        """åˆ›å»ºæ¨¡æ‹Ÿå·¥ä½œæµé…ç½®"""
        return {
            'workflow_id': workflow_id,
            'name': f'Test Workflow {workflow_id}',
            'stages': [
                {'name': 'analysis', 'tasks': ['task1', 'task2']},
                {'name': 'implementation', 'tasks': ['task3', 'task4', 'task5']}
            ],
            'agents': ['backend-architect', 'test-engineer'],
            'created_at': datetime.now().isoformat()
        }

    def create_large_mock_workflow(self, workflow_id: str) -> Dict[str, Any]:
        """åˆ›å»ºå¤§å‹æ¨¡æ‹Ÿå·¥ä½œæµï¼ˆç”¨äºå†…å­˜æµ‹è¯•ï¼‰"""
        return {
            'workflow_id': workflow_id,
            'name': f'Large Test Workflow {workflow_id}',
            'description': 'Large workflow for memory testing' * 10,
            'stages': [
                {
                    'name': f'stage_{i}',
                    'tasks': [f'task_{i}_{j}' for j in range(10)],
                    'config': {'data': 'x' * 100}  # æ·»åŠ ä¸€äº›æ•°æ®
                }
                for i in range(5)
            ],
            'agents': [f'agent_{i}' for i in range(10)],
            'metadata': {
                'created_at': datetime.now().isoformat(),
                'large_data': ['item' * 20 for _ in range(50)]
            }
        }

    def generate_report(self, test_start: datetime) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š...")

        test_end = datetime.now()
        total_duration = (test_end - test_start).total_seconds()

        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
        total_operations = sum(r.operations_count for r in self.results)
        total_success = sum(r.success_count for r in self.results)
        total_failures = sum(r.failure_count for r in self.results)

        avg_throughput = statistics.mean([r.throughput for r in self.results])
        avg_response_time = statistics.mean([r.avg_response_time for r in self.results])
        avg_cpu_usage = statistics.mean([r.cpu_usage for r in self.results])
        avg_memory_usage = statistics.mean([r.memory_usage for r in self.results])

        # è®¡ç®—æ€§èƒ½è¯„åˆ†
        performance_score = self.calculate_performance_score()

        report = {
            'test_metadata': {
                'test_start': test_start.isoformat(),
                'test_end': test_end.isoformat(),
                'total_duration': total_duration,
                'system_info': {
                    'cpu_count': psutil.cpu_count(),
                    'memory_total_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
                    'platform': sys.platform
                }
            },
            'overall_metrics': {
                'total_operations': total_operations,
                'total_success': total_success,
                'total_failures': total_failures,
                'success_rate': total_success / total_operations if total_operations > 0 else 0,
                'avg_throughput': avg_throughput,
                'avg_response_time': avg_response_time,
                'avg_cpu_usage': avg_cpu_usage,
                'avg_memory_usage': avg_memory_usage,
                'performance_score': performance_score
            },
            'test_results': [
                {
                    'test_name': r.test_name,
                    'duration': r.duration,
                    'operations_count': r.operations_count,
                    'success_count': r.success_count,
                    'failure_count': r.failure_count,
                    'throughput': r.throughput,
                    'avg_response_time': r.avg_response_time,
                    'cpu_usage': r.cpu_usage,
                    'memory_usage': r.memory_usage
                }
                for r in self.results
            ],
            'recommendations': self.generate_recommendations(),
            'performance_grade': self.get_performance_grade(performance_score)
        }

        return report

    def calculate_performance_score(self) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†ï¼ˆ0-100ï¼‰"""
        scores = []

        for result in self.results:
            # æˆåŠŸç‡è¯„åˆ† (40åˆ†)
            success_rate_score = (result.success_count / result.operations_count) * 40 if result.operations_count > 0 else 0

            # ååé‡è¯„åˆ† (30åˆ†) - åŸºäºåˆç†çš„æœŸæœ›å€¼
            expected_throughput = {
                'workflow_creation': 200,
                'concurrent_workflows': 50,
                'memory_usage': 100,
                'response_times': 150,
                'stress_test': 300
            }
            expected = expected_throughput.get(result.test_name, 100)
            throughput_score = min(30, (result.throughput / expected) * 30)

            # å“åº”æ—¶é—´è¯„åˆ† (20åˆ†) - å“åº”æ—¶é—´è¶Šä½è¶Šå¥½
            max_acceptable_response = 0.1  # 100ms
            response_score = max(0, 20 * (1 - result.avg_response_time / max_acceptable_response))

            # èµ„æºä½¿ç”¨è¯„åˆ† (10åˆ†) - CPUå’Œå†…å­˜ä½¿ç”¨è¶Šä½è¶Šå¥½
            resource_score = max(0, 10 * (1 - (result.cpu_usage + result.memory_usage) / 200))

            test_score = success_rate_score + throughput_score + response_score + resource_score
            scores.append(test_score)

        return statistics.mean(scores) if scores else 0

    def get_performance_grade(self, score: float) -> str:
        """è·å–æ€§èƒ½ç­‰çº§"""
        if score >= 90:
            return 'A+ (ä¼˜ç§€)'
        elif score >= 80:
            return 'A (è‰¯å¥½)'
        elif score >= 70:
            return 'B (ä¸­ç­‰)'
        elif score >= 60:
            return 'C (åŠæ ¼)'
        else:
            return 'D (éœ€è¦æ”¹è¿›)'

    def generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        for result in self.results:
            if result.success_count / result.operations_count < 0.95:
                recommendations.append(f"{result.test_name}: æˆåŠŸç‡è¾ƒä½ï¼Œéœ€è¦æ”¹è¿›é”™è¯¯å¤„ç†")

            if result.avg_response_time > 0.05:  # 50ms
                recommendations.append(f"{result.test_name}: å“åº”æ—¶é—´è¾ƒé«˜ï¼Œè€ƒè™‘æ€§èƒ½ä¼˜åŒ–")

            if result.cpu_usage > 70:
                recommendations.append(f"{result.test_name}: CPUä½¿ç”¨ç‡é«˜ï¼Œæ£€æŸ¥ç®—æ³•æ•ˆç‡")

            if result.memory_usage > 80:
                recommendations.append(f"{result.test_name}: å†…å­˜ä½¿ç”¨ç‡é«˜ï¼Œæ£€æŸ¥å†…å­˜æ³„æ¼")

        # é€šç”¨å»ºè®®
        if not recommendations:
            recommendations.append("æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰æ°´å¹³")

        recommendations.extend([
            "å»ºè®®å®æ–½æ€§èƒ½ç›‘æ§ä»¥è·Ÿè¸ªè¶‹åŠ¿",
            "å®šæœŸæ‰§è¡Œæ€§èƒ½æµ‹è¯•ä»¥ç¡®ä¿è´¨é‡",
            "è€ƒè™‘åœ¨CI/CDä¸­é›†æˆæ€§èƒ½æµ‹è¯•"
        ])

        return recommendations

    def save_report(self, report: Dict[str, Any]):
        """ä¿å­˜æ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # JSONæŠ¥å‘Š
        json_file = self.results_dir / f"simple_performance_report_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)

        # æ–‡æœ¬æ‘˜è¦
        txt_file = self.results_dir / f"performance_summary_{timestamp}.txt"
        self.generate_text_summary(report, txt_file)

        print(f"\nğŸ“„ æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜:")
        print(f"  â€¢ JSONæŠ¥å‘Š: {json_file}")
        print(f"  â€¢ æ–‡æœ¬æ‘˜è¦: {txt_file}")

        # æ‰“å°å…³é”®æŒ‡æ ‡
        metrics = report['overall_metrics']
        print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•æ‘˜è¦:")
        print(f"  â€¢ æ€»ä½“è¯„åˆ†: {metrics['performance_score']:.1f}/100 - {report['performance_grade']}")
        print(f"  â€¢ æˆåŠŸç‡: {metrics['success_rate']:.1%}")
        print(f"  â€¢ å¹³å‡ååé‡: {metrics['avg_throughput']:.1f} ops/s")
        print(f"  â€¢ å¹³å‡å“åº”æ—¶é—´: {metrics['avg_response_time']:.3f}s")
        print(f"  â€¢ å¹³å‡CPUä½¿ç”¨: {metrics['avg_cpu_usage']:.1f}%")
        print(f"  â€¢ å¹³å‡å†…å­˜ä½¿ç”¨: {metrics['avg_memory_usage']:.1f}%")

    def generate_text_summary(self, report: Dict[str, Any], filename: Path):
        """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦æŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("Perfect21 æ€§èƒ½æµ‹è¯•æ‘˜è¦æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")

            # æµ‹è¯•æ¦‚è§ˆ
            metadata = report['test_metadata']
            f.write(f"æµ‹è¯•æ—¶é—´: {metadata['test_start']} - {metadata['test_end']}\n")
            f.write(f"æµ‹è¯•æŒç»­æ—¶é—´: {metadata['total_duration']:.1f}ç§’\n")
            f.write(f"ç³»ç»Ÿé…ç½®: {metadata['system_info']['cpu_count']} CPUæ ¸å¿ƒ, "
                   f"{metadata['system_info']['memory_total_gb']:.1f}GB å†…å­˜\n\n")

            # æ•´ä½“æŒ‡æ ‡
            metrics = report['overall_metrics']
            f.write("æ•´ä½“æ€§èƒ½æŒ‡æ ‡:\n")
            f.write("-" * 30 + "\n")
            f.write(f"æ€§èƒ½è¯„åˆ†: {metrics['performance_score']:.1f}/100 ({report['performance_grade']})\n")
            f.write(f"æ€»æ“ä½œæ•°: {metrics['total_operations']}\n")
            f.write(f"æˆåŠŸç‡: {metrics['success_rate']:.1%}\n")
            f.write(f"å¹³å‡ååé‡: {metrics['avg_throughput']:.1f} ops/s\n")
            f.write(f"å¹³å‡å“åº”æ—¶é—´: {metrics['avg_response_time']:.3f}s\n")
            f.write(f"å¹³å‡CPUä½¿ç”¨: {metrics['avg_cpu_usage']:.1f}%\n")
            f.write(f"å¹³å‡å†…å­˜ä½¿ç”¨: {metrics['avg_memory_usage']:.1f}%\n\n")

            # è¯¦ç»†ç»“æœ
            f.write("å„é¡¹æµ‹è¯•è¯¦ç»†ç»“æœ:\n")
            f.write("-" * 30 + "\n")
            for result in report['test_results']:
                f.write(f"\n{result['test_name']}:\n")
                f.write(f"  æ“ä½œæ•°: {result['operations_count']}\n")
                f.write(f"  æˆåŠŸæ•°: {result['success_count']}\n")
                f.write(f"  ååé‡: {result['throughput']:.1f} ops/s\n")
                f.write(f"  å“åº”æ—¶é—´: {result['avg_response_time']:.3f}s\n")
                f.write(f"  CPUä½¿ç”¨: {result['cpu_usage']:.1f}%\n")
                f.write(f"  å†…å­˜ä½¿ç”¨: {result['memory_usage']:.1f}%\n")

            # ä¼˜åŒ–å»ºè®®
            f.write(f"\nä¼˜åŒ–å»ºè®®:\n")
            f.write("-" * 30 + "\n")
            for i, rec in enumerate(report['recommendations'], 1):
                f.write(f"{i}. {rec}\n")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Perfect21 ç®€åŒ–æ€§èƒ½æµ‹è¯•å¯åŠ¨")

    tester = Perfect21SimplePerformanceTest()

    try:
        report = tester.run_all_tests()

        print("\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆ!")

        return report

    except KeyboardInterrupt:
        print("\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return None
    except Exception as e:
        print(f"\nâŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()