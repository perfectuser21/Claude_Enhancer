#!/usr/bin/env python3
"""
Perfect21 æ•°æ®åº“æ€§èƒ½æµ‹è¯•
æµ‹è¯•å·¥ä½œæµçŠ¶æ€æŒä¹…åŒ–ã€æŸ¥è¯¢æ€§èƒ½å’Œå¹¶å‘è®¿é—®
"""

import os
import sys
import time
import sqlite3
import json
import statistics
import threading
import concurrent.futures
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
from pathlib import Path
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

@dataclass
class DatabaseMetrics:
    """æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡"""
    operation_type: str
    total_operations: int
    successful_operations: int
    failed_operations: int
    avg_response_time: float
    max_response_time: float
    min_response_time: float
    operations_per_second: float
    concurrent_connections: int
    test_duration: float

class DatabasePerformanceTester:
    """æ•°æ®åº“æ€§èƒ½æµ‹è¯•å™¨"""

    def __init__(self, db_path: str = None):
        self.db_path = db_path or ".perfect21/test_performance.db"
        Path(self.db_path).parent.mkdir(exist_ok=True)

        self.logger = logging.getLogger(__name__)
        logging.basicConfig(level=logging.INFO)

        self.setup_test_database()

    def setup_test_database(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®åº“ç»“æ„"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # å·¥ä½œæµè¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS workflows (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    workflow_id TEXT UNIQUE NOT NULL,
                    name TEXT NOT NULL,
                    state TEXT NOT NULL,
                    config TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    completed_at TIMESTAMP
                )
            ''')

            # é˜¶æ®µè¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS stages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    workflow_id TEXT NOT NULL,
                    stage_name TEXT NOT NULL,
                    status TEXT NOT NULL,
                    execution_mode TEXT NOT NULL,
                    started_at TIMESTAMP,
                    completed_at TIMESTAMP,
                    result TEXT,
                    FOREIGN KEY (workflow_id) REFERENCES workflows (workflow_id)
                )
            ''')

            # ä»»åŠ¡è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS tasks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    workflow_id TEXT NOT NULL,
                    stage_name TEXT NOT NULL,
                    task_id TEXT UNIQUE NOT NULL,
                    agent_name TEXT NOT NULL,
                    description TEXT NOT NULL,
                    status TEXT NOT NULL,
                    priority INTEGER DEFAULT 1,
                    started_at TIMESTAMP,
                    completed_at TIMESTAMP,
                    result TEXT,
                    error_message TEXT,
                    retry_count INTEGER DEFAULT 0,
                    FOREIGN KEY (workflow_id) REFERENCES workflows (workflow_id)
                )
            ''')

            # æ‰§è¡Œæ—¥å¿—è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS execution_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    workflow_id TEXT NOT NULL,
                    event_type TEXT NOT NULL,
                    event_data TEXT NOT NULL,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (workflow_id) REFERENCES workflows (workflow_id)
                )
            ''')

            # æ€§èƒ½æŒ‡æ ‡è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS performance_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    workflow_id TEXT NOT NULL,
                    metric_name TEXT NOT NULL,
                    metric_value REAL NOT NULL,
                    recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (workflow_id) REFERENCES workflows (workflow_id)
                )
            ''')

            # åˆ›å»ºç´¢å¼•
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_workflows_id ON workflows(workflow_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_workflows_state ON workflows(state)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_stages_workflow ON stages(workflow_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_tasks_workflow ON tasks(workflow_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_tasks_status ON tasks(status)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_logs_workflow ON execution_logs(workflow_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_logs_timestamp ON execution_logs(timestamp)')

            conn.commit()

    def run_comprehensive_database_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„æ•°æ®åº“æ€§èƒ½æµ‹è¯•"""
        print("ğŸ—„ï¸  å¼€å§‹æ•°æ®åº“æ€§èƒ½æµ‹è¯•")
        print("=" * 60)

        test_results = {}

        # 1. æ’å…¥æ€§èƒ½æµ‹è¯•
        test_results['insert_performance'] = self.test_insert_performance()

        # 2. æŸ¥è¯¢æ€§èƒ½æµ‹è¯•
        test_results['query_performance'] = self.test_query_performance()

        # 3. æ›´æ–°æ€§èƒ½æµ‹è¯•
        test_results['update_performance'] = self.test_update_performance()

        # 4. å¹¶å‘è®¿é—®æµ‹è¯•
        test_results['concurrent_access'] = self.test_concurrent_access()

        # 5. å¤§æ•°æ®é‡æ€§èƒ½æµ‹è¯•
        test_results['large_dataset_performance'] = self.test_large_dataset_performance()

        # 6. äº‹åŠ¡æ€§èƒ½æµ‹è¯•
        test_results['transaction_performance'] = self.test_transaction_performance()

        # 7. ç´¢å¼•æ•ˆç‡æµ‹è¯•
        test_results['index_efficiency'] = self.test_index_efficiency()

        # ç¼–è¯‘æµ‹è¯•æŠ¥å‘Š
        final_report = self.compile_database_performance_report(test_results)

        return final_report

    def test_insert_performance(self) -> DatabaseMetrics:
        """æµ‹è¯•æ’å…¥æ€§èƒ½"""
        print("\nğŸ“ 1. æ’å…¥æ€§èƒ½æµ‹è¯•")
        print("-" * 30)

        operation_count = 1000
        response_times = []
        successful_ops = 0
        failed_ops = 0

        start_time = time.time()

        for i in range(operation_count):
            op_start = time.time()

            try:
                workflow_data = self._generate_test_workflow_data(f"insert_test_{i}")
                self._insert_workflow_with_stages(workflow_data)

                op_time = time.time() - op_start
                response_times.append(op_time)
                successful_ops += 1

                if i % 100 == 0:
                    print(f"å·²æ’å…¥ {i} ä¸ªå·¥ä½œæµ...")

            except Exception as e:
                failed_ops += 1
                self.logger.error(f"æ’å…¥æ“ä½œå¤±è´¥: {e}")

        total_time = time.time() - start_time

        metrics = DatabaseMetrics(
            operation_type="INSERT",
            total_operations=operation_count,
            successful_operations=successful_ops,
            failed_operations=failed_ops,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            operations_per_second=successful_ops / total_time,
            concurrent_connections=1,
            test_duration=total_time
        )

        print(f"âœ… æ’å…¥æµ‹è¯•å®Œæˆ: {successful_ops}/{operation_count} æˆåŠŸ, "
              f"{metrics.operations_per_second:.1f} ops/s, "
              f"å¹³å‡å“åº”æ—¶é—´: {metrics.avg_response_time:.3f}s")

        return metrics

    def test_query_performance(self) -> DatabaseMetrics:
        """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½"""
        print("\nğŸ” 2. æŸ¥è¯¢æ€§èƒ½æµ‹è¯•")
        print("-" * 30)

        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æµ‹è¯•æ•°æ®
        self._ensure_test_data(500)

        query_count = 1000
        response_times = []
        successful_ops = 0
        failed_ops = 0

        queries = [
            "SELECT * FROM workflows WHERE state = 'running'",
            "SELECT * FROM workflows WHERE created_at > datetime('now', '-1 day')",
            "SELECT w.*, COUNT(s.id) as stage_count FROM workflows w LEFT JOIN stages s ON w.workflow_id = s.workflow_id GROUP BY w.id",
            "SELECT * FROM tasks WHERE status = 'completed' ORDER BY completed_at DESC LIMIT 10",
            "SELECT workflow_id, COUNT(*) as task_count FROM tasks GROUP BY workflow_id HAVING task_count > 5"
        ]

        start_time = time.time()

        for i in range(query_count):
            query = queries[i % len(queries)]
            op_start = time.time()

            try:
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute(query)
                    results = cursor.fetchall()

                op_time = time.time() - op_start
                response_times.append(op_time)
                successful_ops += 1

                if i % 200 == 0:
                    print(f"å·²æ‰§è¡Œ {i} ä¸ªæŸ¥è¯¢...")

            except Exception as e:
                failed_ops += 1
                self.logger.error(f"æŸ¥è¯¢æ“ä½œå¤±è´¥: {e}")

        total_time = time.time() - start_time

        metrics = DatabaseMetrics(
            operation_type="SELECT",
            total_operations=query_count,
            successful_operations=successful_ops,
            failed_operations=failed_ops,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            operations_per_second=successful_ops / total_time,
            concurrent_connections=1,
            test_duration=total_time
        )

        print(f"âœ… æŸ¥è¯¢æµ‹è¯•å®Œæˆ: {successful_ops}/{query_count} æˆåŠŸ, "
              f"{metrics.operations_per_second:.1f} ops/s, "
              f"å¹³å‡å“åº”æ—¶é—´: {metrics.avg_response_time:.3f}s")

        return metrics

    def test_update_performance(self) -> DatabaseMetrics:
        """æµ‹è¯•æ›´æ–°æ€§èƒ½"""
        print("\nâœï¸  3. æ›´æ–°æ€§èƒ½æµ‹è¯•")
        print("-" * 30)

        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æµ‹è¯•æ•°æ®
        workflow_ids = self._ensure_test_data(200)

        update_count = 500
        response_times = []
        successful_ops = 0
        failed_ops = 0

        start_time = time.time()

        for i in range(update_count):
            workflow_id = workflow_ids[i % len(workflow_ids)]
            op_start = time.time()

            try:
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()

                    # æ›´æ–°å·¥ä½œæµçŠ¶æ€
                    new_state = ['running', 'completed', 'failed'][i % 3]
                    cursor.execute(
                        "UPDATE workflows SET state = ?, updated_at = CURRENT_TIMESTAMP WHERE workflow_id = ?",
                        (new_state, workflow_id)
                    )

                    # æ›´æ–°ç›¸å…³ä»»åŠ¡çŠ¶æ€
                    cursor.execute(
                        "UPDATE tasks SET status = ? WHERE workflow_id = ?",
                        (new_state, workflow_id)
                    )

                    conn.commit()

                op_time = time.time() - op_start
                response_times.append(op_time)
                successful_ops += 1

                if i % 100 == 0:
                    print(f"å·²æ›´æ–° {i} ä¸ªå·¥ä½œæµ...")

            except Exception as e:
                failed_ops += 1
                self.logger.error(f"æ›´æ–°æ“ä½œå¤±è´¥: {e}")

        total_time = time.time() - start_time

        metrics = DatabaseMetrics(
            operation_type="UPDATE",
            total_operations=update_count,
            successful_operations=successful_ops,
            failed_operations=failed_ops,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            operations_per_second=successful_ops / total_time,
            concurrent_connections=1,
            test_duration=total_time
        )

        print(f"âœ… æ›´æ–°æµ‹è¯•å®Œæˆ: {successful_ops}/{update_count} æˆåŠŸ, "
              f"{metrics.operations_per_second:.1f} ops/s, "
              f"å¹³å‡å“åº”æ—¶é—´: {metrics.avg_response_time:.3f}s")

        return metrics

    def test_concurrent_access(self) -> DatabaseMetrics:
        """æµ‹è¯•å¹¶å‘è®¿é—®æ€§èƒ½"""
        print("\nğŸš€ 4. å¹¶å‘è®¿é—®æµ‹è¯•")
        print("-" * 30)

        concurrent_users = 10
        operations_per_user = 50
        total_operations = concurrent_users * operations_per_user

        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æµ‹è¯•æ•°æ®
        workflow_ids = self._ensure_test_data(100)

        response_times = []
        successful_ops = 0
        failed_ops = 0
        lock = threading.Lock()

        def concurrent_worker(worker_id: int):
            """å¹¶å‘å·¥ä½œçº¿ç¨‹"""
            nonlocal successful_ops, failed_ops

            worker_successful = 0
            worker_failed = 0
            worker_times = []

            for i in range(operations_per_user):
                operation_type = i % 4  # 4ç§æ“ä½œç±»å‹
                op_start = time.time()

                try:
                    if operation_type == 0:  # æ’å…¥
                        workflow_data = self._generate_test_workflow_data(f"concurrent_{worker_id}_{i}")
                        self._insert_workflow_with_stages(workflow_data)

                    elif operation_type == 1:  # æŸ¥è¯¢
                        with sqlite3.connect(self.db_path) as conn:
                            cursor = conn.cursor()
                            cursor.execute("SELECT * FROM workflows WHERE state = 'running' LIMIT 10")
                            results = cursor.fetchall()

                    elif operation_type == 2:  # æ›´æ–°
                        workflow_id = workflow_ids[i % len(workflow_ids)]
                        with sqlite3.connect(self.db_path) as conn:
                            cursor = conn.cursor()
                            cursor.execute(
                                "UPDATE workflows SET updated_at = CURRENT_TIMESTAMP WHERE workflow_id = ?",
                                (workflow_id,)
                            )
                            conn.commit()

                    else:  # åˆ é™¤å’Œé‡æ–°æ’å…¥
                        workflow_id = workflow_ids[i % len(workflow_ids)]
                        with sqlite3.connect(self.db_path) as conn:
                            cursor = conn.cursor()
                            cursor.execute("DELETE FROM tasks WHERE workflow_id = ?", (workflow_id,))
                            cursor.execute("DELETE FROM stages WHERE workflow_id = ?", (workflow_id,))
                            cursor.execute("DELETE FROM workflows WHERE workflow_id = ?", (workflow_id,))
                            conn.commit()

                    op_time = time.time() - op_start
                    worker_times.append(op_time)
                    worker_successful += 1

                except Exception as e:
                    worker_failed += 1
                    self.logger.error(f"Worker {worker_id} æ“ä½œå¤±è´¥: {e}")

            with lock:
                successful_ops += worker_successful
                failed_ops += worker_failed
                response_times.extend(worker_times)

        print(f"å¯åŠ¨ {concurrent_users} ä¸ªå¹¶å‘ç”¨æˆ·ï¼Œæ¯ç”¨æˆ· {operations_per_user} æ“ä½œ...")

        start_time = time.time()

        # å¯åŠ¨å¹¶å‘çº¿ç¨‹
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(concurrent_worker, i) for i in range(concurrent_users)]
            concurrent.futures.wait(futures)

        total_time = time.time() - start_time

        metrics = DatabaseMetrics(
            operation_type="CONCURRENT_MIXED",
            total_operations=total_operations,
            successful_operations=successful_ops,
            failed_operations=failed_ops,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            operations_per_second=successful_ops / total_time,
            concurrent_connections=concurrent_users,
            test_duration=total_time
        )

        print(f"âœ… å¹¶å‘æµ‹è¯•å®Œæˆ: {successful_ops}/{total_operations} æˆåŠŸ, "
              f"{metrics.operations_per_second:.1f} ops/s, "
              f"å¹³å‡å“åº”æ—¶é—´: {metrics.avg_response_time:.3f}s")

        return metrics

    def test_large_dataset_performance(self) -> DatabaseMetrics:
        """æµ‹è¯•å¤§æ•°æ®é‡æ€§èƒ½"""
        print("\nğŸ“Š 5. å¤§æ•°æ®é‡æ€§èƒ½æµ‹è¯•")
        print("-" * 30)

        # åˆ›å»ºå¤§é‡æµ‹è¯•æ•°æ®
        large_dataset_size = 5000
        print(f"åˆ›å»º {large_dataset_size} ä¸ªå·¥ä½œæµçš„å¤§æ•°æ®é›†...")

        workflow_ids = self._ensure_test_data(large_dataset_size)

        # æµ‹è¯•å¤æ‚æŸ¥è¯¢æ€§èƒ½
        complex_queries = [
            """
            SELECT w.workflow_id, w.name, w.state,
                   COUNT(DISTINCT s.stage_name) as stage_count,
                   COUNT(t.id) as task_count,
                   AVG(CASE WHEN t.completed_at IS NOT NULL AND t.started_at IS NOT NULL
                       THEN (julianday(t.completed_at) - julianday(t.started_at)) * 24 * 3600
                       ELSE NULL END) as avg_task_duration
            FROM workflows w
            LEFT JOIN stages s ON w.workflow_id = s.workflow_id
            LEFT JOIN tasks t ON w.workflow_id = t.workflow_id
            GROUP BY w.workflow_id, w.name, w.state
            HAVING task_count > 0
            ORDER BY avg_task_duration DESC
            LIMIT 50
            """,

            """
            SELECT DATE(created_at) as date,
                   COUNT(*) as workflows_created,
                   SUM(CASE WHEN state = 'completed' THEN 1 ELSE 0 END) as completed_count,
                   AVG(CASE WHEN completed_at IS NOT NULL
                       THEN (julianday(completed_at) - julianday(created_at)) * 24 * 3600
                       ELSE NULL END) as avg_completion_time
            FROM workflows
            WHERE created_at > datetime('now', '-30 days')
            GROUP BY DATE(created_at)
            ORDER BY date DESC
            """,

            """
            SELECT agent_name,
                   COUNT(*) as total_tasks,
                   SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed_tasks,
                   SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed_tasks,
                   AVG(retry_count) as avg_retries
            FROM tasks
            GROUP BY agent_name
            HAVING total_tasks > 10
            ORDER BY completed_tasks DESC
            """
        ]

        response_times = []
        successful_ops = 0
        failed_ops = 0

        start_time = time.time()

        for i, query in enumerate(complex_queries):
            for run in range(10):  # æ¯ä¸ªæŸ¥è¯¢è¿è¡Œ10æ¬¡
                op_start = time.time()

                try:
                    with sqlite3.connect(self.db_path) as conn:
                        cursor = conn.cursor()
                        cursor.execute(query)
                        results = cursor.fetchall()

                    op_time = time.time() - op_start
                    response_times.append(op_time)
                    successful_ops += 1

                    print(f"æŸ¥è¯¢ {i+1}, è¿è¡Œ {run+1}: {op_time:.3f}s, {len(results)} è¡Œç»“æœ")

                except Exception as e:
                    failed_ops += 1
                    self.logger.error(f"å¤§æ•°æ®æŸ¥è¯¢å¤±è´¥: {e}")

        total_time = time.time() - start_time

        metrics = DatabaseMetrics(
            operation_type="LARGE_DATASET_QUERY",
            total_operations=len(complex_queries) * 10,
            successful_operations=successful_ops,
            failed_operations=failed_ops,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            operations_per_second=successful_ops / total_time,
            concurrent_connections=1,
            test_duration=total_time
        )

        print(f"âœ… å¤§æ•°æ®é‡æµ‹è¯•å®Œæˆ: {successful_ops}/{len(complex_queries) * 10} æˆåŠŸ, "
              f"å¹³å‡å“åº”æ—¶é—´: {metrics.avg_response_time:.3f}s")

        return metrics

    def test_transaction_performance(self) -> DatabaseMetrics:
        """æµ‹è¯•äº‹åŠ¡æ€§èƒ½"""
        print("\nğŸ’¾ 6. äº‹åŠ¡æ€§èƒ½æµ‹è¯•")
        print("-" * 30)

        transaction_count = 200
        response_times = []
        successful_ops = 0
        failed_ops = 0

        start_time = time.time()

        for i in range(transaction_count):
            op_start = time.time()

            try:
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()

                    # å¼€å§‹äº‹åŠ¡
                    cursor.execute("BEGIN TRANSACTION")

                    # åˆ›å»ºå®Œæ•´çš„å·¥ä½œæµæ•°æ®ï¼ˆå¤šè¡¨æ“ä½œï¼‰
                    workflow_data = self._generate_test_workflow_data(f"transaction_test_{i}")

                    # æ’å…¥å·¥ä½œæµ
                    cursor.execute(
                        "INSERT INTO workflows (workflow_id, name, state, config) VALUES (?, ?, ?, ?)",
                        (workflow_data['workflow_id'], workflow_data['name'],
                         workflow_data['state'], json.dumps(workflow_data['config']))
                    )

                    # æ’å…¥é˜¶æ®µ
                    for stage in workflow_data['stages']:
                        cursor.execute(
                            "INSERT INTO stages (workflow_id, stage_name, status, execution_mode) VALUES (?, ?, ?, ?)",
                            (workflow_data['workflow_id'], stage['name'], stage['status'], stage['execution_mode'])
                        )

                    # æ’å…¥ä»»åŠ¡
                    for task in workflow_data['tasks']:
                        cursor.execute(
                            "INSERT INTO tasks (workflow_id, stage_name, task_id, agent_name, description, status, priority) VALUES (?, ?, ?, ?, ?, ?, ?)",
                            (workflow_data['workflow_id'], task['stage_name'], task['task_id'],
                             task['agent_name'], task['description'], task['status'], task['priority'])
                        )

                    # æ’å…¥æ‰§è¡Œæ—¥å¿—
                    cursor.execute(
                        "INSERT INTO execution_logs (workflow_id, event_type, event_data) VALUES (?, ?, ?)",
                        (workflow_data['workflow_id'], 'workflow_created', json.dumps({'test': True}))
                    )

                    # æäº¤äº‹åŠ¡
                    cursor.execute("COMMIT")

                op_time = time.time() - op_start
                response_times.append(op_time)
                successful_ops += 1

                if i % 50 == 0:
                    print(f"å·²å®Œæˆ {i} ä¸ªäº‹åŠ¡...")

            except Exception as e:
                failed_ops += 1
                self.logger.error(f"äº‹åŠ¡æ“ä½œå¤±è´¥: {e}")
                # å›æ»šäº‹åŠ¡
                try:
                    with sqlite3.connect(self.db_path) as conn:
                        conn.rollback()
                except:
                    pass

        total_time = time.time() - start_time

        metrics = DatabaseMetrics(
            operation_type="TRANSACTION",
            total_operations=transaction_count,
            successful_operations=successful_ops,
            failed_operations=failed_ops,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            operations_per_second=successful_ops / total_time,
            concurrent_connections=1,
            test_duration=total_time
        )

        print(f"âœ… äº‹åŠ¡æµ‹è¯•å®Œæˆ: {successful_ops}/{transaction_count} æˆåŠŸ, "
              f"{metrics.operations_per_second:.1f} ops/s, "
              f"å¹³å‡å“åº”æ—¶é—´: {metrics.avg_response_time:.3f}s")

        return metrics

    def test_index_efficiency(self) -> Dict[str, Any]:
        """æµ‹è¯•ç´¢å¼•æ•ˆç‡"""
        print("\nğŸ—‚ï¸  7. ç´¢å¼•æ•ˆç‡æµ‹è¯•")
        print("-" * 30)

        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æµ‹è¯•æ•°æ®
        self._ensure_test_data(1000)

        test_queries = [
            ("workflow_idç´¢å¼•", "SELECT * FROM workflows WHERE workflow_id = 'test_workflow_500'"),
            ("stateç´¢å¼•", "SELECT * FROM workflows WHERE state = 'running'"),
            ("æ— ç´¢å¼•æŸ¥è¯¢", "SELECT * FROM workflows WHERE name LIKE '%test%'"),
            ("JOINæŸ¥è¯¢", """
                SELECT w.*, COUNT(t.id) as task_count
                FROM workflows w
                LEFT JOIN tasks t ON w.workflow_id = t.workflow_id
                GROUP BY w.id
            """),
            ("å¤åˆæ¡ä»¶", "SELECT * FROM tasks WHERE workflow_id LIKE 'test_%' AND status = 'completed'")
        ]

        results = {}

        for test_name, query in test_queries:
            print(f"æµ‹è¯•: {test_name}")

            # æ‰§è¡ŒæŸ¥è¯¢å¤šæ¬¡å–å¹³å‡å€¼
            execution_times = []

            for _ in range(20):
                start_time = time.time()

                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute(query)
                    results_data = cursor.fetchall()

                execution_time = time.time() - start_time
                execution_times.append(execution_time)

            avg_time = statistics.mean(execution_times)
            max_time = max(execution_times)
            min_time = min(execution_times)

            results[test_name] = {
                'avg_execution_time': avg_time,
                'max_execution_time': max_time,
                'min_execution_time': min_time,
                'result_count': len(results_data),
                'query': query
            }

            print(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time:.3f}s, ç»“æœæ•°é‡: {len(results_data)}")

        return results

    def _generate_test_workflow_data(self, workflow_id: str) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•å·¥ä½œæµæ•°æ®"""
        agents = ['backend-architect', 'frontend-specialist', 'database-specialist',
                 'test-engineer', 'security-auditor', 'devops-engineer']

        stages = []
        tasks = []

        # ç”Ÿæˆé˜¶æ®µå’Œä»»åŠ¡
        for i, stage_name in enumerate(['analysis', 'design', 'implementation', 'testing']):
            stages.append({
                'name': stage_name,
                'status': ['created', 'running', 'completed'][i % 3],
                'execution_mode': 'parallel' if i % 2 == 0 else 'sequential'
            })

            # ä¸ºæ¯ä¸ªé˜¶æ®µç”Ÿæˆä»»åŠ¡
            for j, agent in enumerate(agents[:3]):  # æ¯é˜¶æ®µ3ä¸ªä»»åŠ¡
                task_id = f"{workflow_id}_{stage_name}_{agent}"
                tasks.append({
                    'task_id': task_id,
                    'stage_name': stage_name,
                    'agent_name': agent,
                    'description': f'{agent} task for {stage_name}',
                    'status': ['created', 'running', 'completed', 'failed'][j % 4],
                    'priority': j + 1
                })

        return {
            'workflow_id': workflow_id,
            'name': f'Test Workflow {workflow_id}',
            'state': ['created', 'running', 'completed', 'failed'][len(workflow_id) % 4],
            'config': {
                'test': True,
                'complexity': 'medium',
                'agents_count': len(agents)
            },
            'stages': stages,
            'tasks': tasks
        }

    def _insert_workflow_with_stages(self, workflow_data: Dict[str, Any]):
        """æ’å…¥å®Œæ•´çš„å·¥ä½œæµæ•°æ®"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # æ’å…¥å·¥ä½œæµ
            cursor.execute(
                "INSERT INTO workflows (workflow_id, name, state, config) VALUES (?, ?, ?, ?)",
                (workflow_data['workflow_id'], workflow_data['name'],
                 workflow_data['state'], json.dumps(workflow_data['config']))
            )

            # æ’å…¥é˜¶æ®µ
            for stage in workflow_data['stages']:
                cursor.execute(
                    "INSERT INTO stages (workflow_id, stage_name, status, execution_mode) VALUES (?, ?, ?, ?)",
                    (workflow_data['workflow_id'], stage['name'], stage['status'], stage['execution_mode'])
                )

            # æ’å…¥ä»»åŠ¡
            for task in workflow_data['tasks']:
                cursor.execute(
                    "INSERT INTO tasks (workflow_id, stage_name, task_id, agent_name, description, status, priority) VALUES (?, ?, ?, ?, ?, ?, ?)",
                    (workflow_data['workflow_id'], task['stage_name'], task['task_id'],
                     task['agent_name'], task['description'], task['status'], task['priority'])
                )

            conn.commit()

    def _ensure_test_data(self, min_count: int) -> List[str]:
        """ç¡®ä¿æœ‰è¶³å¤Ÿçš„æµ‹è¯•æ•°æ®"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM workflows")
            current_count = cursor.fetchone()[0]

        workflow_ids = []

        if current_count < min_count:
            needed = min_count - current_count
            print(f"åˆ›å»º {needed} ä¸ªæµ‹è¯•å·¥ä½œæµ...")

            for i in range(needed):
                workflow_id = f"test_workflow_{current_count + i}"
                workflow_data = self._generate_test_workflow_data(workflow_id)
                self._insert_workflow_with_stages(workflow_data)
                workflow_ids.append(workflow_id)

        # è·å–æ‰€æœ‰å·¥ä½œæµID
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT workflow_id FROM workflows LIMIT ?", (min_count,))
            workflow_ids.extend([row[0] for row in cursor.fetchall()])

        return workflow_ids

    def compile_database_performance_report(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç¼–è¯‘æ•°æ®åº“æ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“‹ æ­£åœ¨ç”Ÿæˆæ•°æ®åº“æ€§èƒ½æµ‹è¯•æŠ¥å‘Š...")

        # è®¡ç®—æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
        db_stats = self._get_database_statistics()

        report = {
            'test_summary': {
                'test_date': datetime.now().isoformat(),
                'database_path': self.db_path,
                'database_size_mb': os.path.getsize(self.db_path) / 1024 / 1024 if os.path.exists(self.db_path) else 0,
                'total_records': db_stats['total_records']
            },
            'performance_metrics': test_results,
            'database_statistics': db_stats,
            'performance_summary': self._calculate_performance_summary(test_results),
            'recommendations': self._generate_database_recommendations(test_results)
        }

        # ä¿å­˜æŠ¥å‘Š
        self._save_database_report(report)

        return report

    def _get_database_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # è¡¨è®°å½•æ•°ç»Ÿè®¡
            tables = ['workflows', 'stages', 'tasks', 'execution_logs', 'performance_metrics']
            total_records = 0

            for table in tables:
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                count = cursor.fetchone()[0]
                stats[f'{table}_count'] = count
                total_records += count

            stats['total_records'] = total_records

            # æ•°æ®åº“æ–‡ä»¶å¤§å°
            if os.path.exists(self.db_path):
                stats['database_size_bytes'] = os.path.getsize(self.db_path)
                stats['database_size_mb'] = stats['database_size_bytes'] / 1024 / 1024

        return stats

    def _calculate_performance_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½æ‘˜è¦"""
        summary = {}

        # æå–æ•°å­—æŒ‡æ ‡
        for test_name, result in results.items():
            if isinstance(result, DatabaseMetrics):
                summary[test_name] = {
                    'operations_per_second': result.operations_per_second,
                    'avg_response_time': result.avg_response_time,
                    'success_rate': result.successful_operations / result.total_operations if result.total_operations > 0 else 0
                }

        # è®¡ç®—æ•´ä½“è¯„åˆ†
        if summary:
            avg_ops_per_sec = statistics.mean([s['operations_per_second'] for s in summary.values()])
            avg_response_time = statistics.mean([s['avg_response_time'] for s in summary.values()])
            avg_success_rate = statistics.mean([s['success_rate'] for s in summary.values()])

            summary['overall'] = {
                'avg_operations_per_second': avg_ops_per_sec,
                'avg_response_time': avg_response_time,
                'avg_success_rate': avg_success_rate,
                'performance_score': min(100, int(avg_ops_per_sec * 10 + (1 - avg_response_time) * 50 + avg_success_rate * 40))
            }

        return summary

    def _generate_database_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ•°æ®åº“ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        for test_name, result in results.items():
            if isinstance(result, DatabaseMetrics):
                if result.avg_response_time > 0.1:  # 100ms
                    recommendations.append(f"{test_name}: å“åº”æ—¶é—´è¾ƒé«˜ï¼Œè€ƒè™‘ä¼˜åŒ–æŸ¥è¯¢æˆ–æ·»åŠ ç´¢å¼•")

                if result.operations_per_second < 100:
                    recommendations.append(f"{test_name}: ååé‡è¾ƒä½ï¼Œæ£€æŸ¥æ•°æ®åº“é…ç½®å’Œç¡¬ä»¶æ€§èƒ½")

                if result.failed_operations > 0:
                    recommendations.append(f"{test_name}: å­˜åœ¨å¤±è´¥æ“ä½œï¼Œæ£€æŸ¥é”™è¯¯å¤„ç†å’Œæ•°æ®ä¸€è‡´æ€§")

        # é€šç”¨å»ºè®®
        recommendations.extend([
            "å®šæœŸæ‰§è¡ŒVACUUMæ“ä½œä»¥ä¼˜åŒ–æ•°æ®åº“æ–‡ä»¶",
            "è€ƒè™‘å®ç°è¿æ¥æ± ä»¥æé«˜å¹¶å‘æ€§èƒ½",
            "ç›‘æ§æ•°æ®åº“å¤§å°å’ŒæŸ¥è¯¢æ€§èƒ½è¶‹åŠ¿",
            "å®ç°æ•°æ®å½’æ¡£ç­–ç•¥ä»¥æ§åˆ¶æ•°æ®åº“å¢é•¿"
        ])

        return recommendations

    def _save_database_report(self, report: Dict[str, Any]):
        """ä¿å­˜æ•°æ®åº“æ€§èƒ½æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        results_dir = Path("tests/performance/results")
        results_dir.mkdir(exist_ok=True)

        # ä¿å­˜JSONæŠ¥å‘Š
        json_filename = results_dir / f"database_performance_report_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)

        # ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
        text_filename = results_dir / f"database_performance_summary_{timestamp}.txt"
        with open(text_filename, 'w', encoding='utf-8') as f:
            f.write("Perfect21 æ•°æ®åº“æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")

            # æµ‹è¯•æ‘˜è¦
            summary = report['test_summary']
            f.write(f"æµ‹è¯•æ—¥æœŸ: {summary['test_date']}\n")
            f.write(f"æ•°æ®åº“å¤§å°: {summary['database_size_mb']:.2f}MB\n")
            f.write(f"æ€»è®°å½•æ•°: {summary['total_records']}\n\n")

            # æ€§èƒ½æŒ‡æ ‡
            perf_summary = report['performance_summary']
            if 'overall' in perf_summary:
                overall = perf_summary['overall']
                f.write("æ•´ä½“æ€§èƒ½æŒ‡æ ‡:\n")
                f.write("-" * 20 + "\n")
                f.write(f"å¹³å‡æ“ä½œ/ç§’: {overall['avg_operations_per_second']:.1f}\n")
                f.write(f"å¹³å‡å“åº”æ—¶é—´: {overall['avg_response_time']:.3f}s\n")
                f.write(f"å¹³å‡æˆåŠŸç‡: {overall['avg_success_rate']:.1%}\n")
                f.write(f"æ€§èƒ½è¯„åˆ†: {overall['performance_score']}/100\n\n")

            # ä¼˜åŒ–å»ºè®®
            f.write("ä¼˜åŒ–å»ºè®®:\n")
            f.write("-" * 20 + "\n")
            for i, rec in enumerate(report['recommendations'], 1):
                f.write(f"{i}. {rec}\n")

        print(f"\nğŸ“„ æ•°æ®åº“æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜:")
        print(f"  â€¢ è¯¦ç»†æŠ¥å‘Š: {json_filename}")
        print(f"  â€¢ æ‘˜è¦æŠ¥å‘Š: {text_filename}")

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ•°æ®åº“æ€§èƒ½æµ‹è¯•"""
    print("ğŸ—„ï¸  å¯åŠ¨Perfect21æ•°æ®åº“æ€§èƒ½æµ‹è¯•")

    tester = DatabasePerformanceTester()

    try:
        # è¿è¡Œå®Œæ•´çš„æ•°æ®åº“æ€§èƒ½æµ‹è¯•
        final_report = tester.run_comprehensive_database_tests()

        print("\nâœ… æ•°æ®åº“æ€§èƒ½æµ‹è¯•å®Œæˆï¼")

        # æ˜¾ç¤ºæ‘˜è¦
        if 'performance_summary' in final_report and 'overall' in final_report['performance_summary']:
            overall = final_report['performance_summary']['overall']
            print(f"ğŸ“Š æ•´ä½“æ€§èƒ½è¯„åˆ†: {overall['performance_score']}/100")
            print(f"ğŸ’« å¹³å‡æ“ä½œ/ç§’: {overall['avg_operations_per_second']:.1f}")
            print(f"â±ï¸  å¹³å‡å“åº”æ—¶é—´: {overall['avg_response_time']:.3f}s")

        return final_report

    except Exception as e:
        print(f"âŒ æ•°æ®åº“æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()