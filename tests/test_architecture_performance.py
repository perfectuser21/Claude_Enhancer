#!/usr/bin/env python3
"""
Perfect21 æ¶æ„æ€§èƒ½åŸºå‡†æµ‹è¯•
åˆ†æManagerç±»çš„åŠ è½½æ€§èƒ½ã€å†…å­˜ä½¿ç”¨å’Œè€¦åˆåº¦
"""

import time
import sys
import os
import gc
import psutil
import importlib
from typing import Dict, List, Any
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

@dataclass
class ManagerBenchmark:
    """Manageræ€§èƒ½åŸºå‡†"""
    name: str
    module_path: str
    load_time_ms: float
    memory_usage_mb: float
    dependencies: List[str]
    class_count: int
    lines_of_code: int
    success: bool = True
    error: str = None

class ArchitectureAnalyzer:
    """æ¶æ„åˆ†æå™¨"""

    def __init__(self):
        self.manager_modules = [
            ('architecture_manager', 'modules.architecture_manager'),
            ('parallel_manager', 'features.parallel_manager'),
            ('workflow_orchestrator', 'features.workflow_orchestrator.orchestrator'),
            ('sync_point_manager', 'features.sync_point_manager.sync_manager'),
            ('auth_manager', 'features.auth_system.auth_manager'),
            ('token_manager', 'features.auth_system.token_manager'),
            ('task_manager', 'features.workflow_orchestrator.task_manager'),
            ('workspace_manager', 'features.multi_workspace.workspace_manager'),
            ('decision_recorder', 'features.decision_recorder.adr_manager'),
            ('branch_manager', 'features.git_workflow.branch_manager'),
            ('hooks_manager', 'features.git_workflow.hooks_manager'),
            ('plugin_manager', 'features.git_workflow.plugins.plugin_manager'),
            ('lifecycle_manager', 'features.claude_md_manager.lifecycle_manager'),
            ('template_manager', 'features.claude_md_manager.template_manager'),
            ('version_manager', 'features.version_manager.version_manager'),
        ]

    def analyze_manager_performance(self, name: str, module_path: str) -> ManagerBenchmark:
        """åˆ†æå•ä¸ªManagerçš„æ€§èƒ½"""
        print(f"åˆ†æ {name}...")

        # è®°å½•åˆå§‹å†…å­˜
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024

        try:
            # æµ‹é‡åŠ è½½æ—¶é—´
            start_time = time.time()

            # åŠ¨æ€å¯¼å…¥æ¨¡å—
            module = importlib.import_module(module_path)

            load_time = (time.time() - start_time) * 1000

            # æµ‹é‡å†…å­˜ä½¿ç”¨
            final_memory = process.memory_info().rss / 1024 / 1024
            memory_usage = final_memory - initial_memory

            # åˆ†ææ¨¡å—ä¿¡æ¯
            dependencies = self._analyze_dependencies(module)
            class_count = self._count_manager_classes(module)
            lines_of_code = self._count_lines_of_code(module_path)

            return ManagerBenchmark(
                name=name,
                module_path=module_path,
                load_time_ms=load_time,
                memory_usage_mb=memory_usage,
                dependencies=dependencies,
                class_count=class_count,
                lines_of_code=lines_of_code
            )

        except Exception as e:
            return ManagerBenchmark(
                name=name,
                module_path=module_path,
                load_time_ms=0,
                memory_usage_mb=0,
                dependencies=[],
                class_count=0,
                lines_of_code=0,
                success=False,
                error=str(e)
            )

    def _analyze_dependencies(self, module) -> List[str]:
        """åˆ†ææ¨¡å—ä¾èµ–"""
        dependencies = []

        # æ£€æŸ¥æ¨¡å—çš„__dict__ä¸­çš„å¯¼å…¥
        for name, obj in module.__dict__.items():
            if hasattr(obj, '__module__') and obj.__module__:
                if 'perfect21' in obj.__module__.lower() or 'features' in obj.__module__:
                    dependencies.append(obj.__module__)

        return list(set(dependencies))

    def _count_manager_classes(self, module) -> int:
        """ç»Ÿè®¡Managerç±»æ•°é‡"""
        count = 0
        for name, obj in module.__dict__.items():
            if isinstance(obj, type) and 'manager' in name.lower():
                count += 1
        return count

    def _count_lines_of_code(self, module_path: str) -> int:
        """ç»Ÿè®¡ä»£ç è¡Œæ•°"""
        try:
            # å°è¯•æ‰¾åˆ°æ–‡ä»¶è·¯å¾„
            file_path = module_path.replace('.', '/') + '.py'
            full_path = os.path.join(os.path.dirname(__file__), file_path)

            if os.path.exists(full_path):
                with open(full_path, 'r', encoding='utf-8') as f:
                    return len([line for line in f if line.strip() and not line.strip().startswith('#')])
        except:
            pass
        return 0

    def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæ¶æ„åˆ†æ"""
        print("ğŸ” Perfect21 æ¶æ„æ€§èƒ½åˆ†æå¼€å§‹...")
        print("=" * 80)

        benchmarks = []

        # åˆ†ææ¯ä¸ªManager
        for name, module_path in self.manager_modules:
            benchmark = self.analyze_manager_performance(name, module_path)
            benchmarks.append(benchmark)

            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()

        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        return self._generate_analysis_report(benchmarks)

    def _generate_analysis_report(self, benchmarks: List[ManagerBenchmark]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        successful_benchmarks = [b for b in benchmarks if b.success]
        failed_benchmarks = [b for b in benchmarks if not b.success]

        # æ€§èƒ½ç»Ÿè®¡
        total_load_time = sum(b.load_time_ms for b in successful_benchmarks)
        total_memory = sum(b.memory_usage_mb for b in successful_benchmarks)
        total_lines = sum(b.lines_of_code for b in successful_benchmarks)

        # è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
        slow_managers = sorted(successful_benchmarks, key=lambda x: x.load_time_ms, reverse=True)[:5]
        memory_heavy = sorted(successful_benchmarks, key=lambda x: x.memory_usage_mb, reverse=True)[:5]

        # ä¾èµ–åˆ†æ
        dependency_graph = self._analyze_dependency_coupling(successful_benchmarks)

        report = {
            'summary': {
                'total_managers': len(benchmarks),
                'successful_loads': len(successful_benchmarks),
                'failed_loads': len(failed_benchmarks),
                'total_load_time_ms': total_load_time,
                'total_memory_mb': total_memory,
                'total_lines_of_code': total_lines,
                'average_load_time_ms': total_load_time / len(successful_benchmarks) if successful_benchmarks else 0,
                'average_memory_mb': total_memory / len(successful_benchmarks) if successful_benchmarks else 0
            },
            'performance_bottlenecks': {
                'slowest_managers': [(b.name, b.load_time_ms) for b in slow_managers],
                'memory_heavy_managers': [(b.name, b.memory_usage_mb) for b in memory_heavy],
            },
            'coupling_analysis': dependency_graph,
            'failed_modules': [(b.name, b.error) for b in failed_benchmarks],
            'detailed_benchmarks': [
                {
                    'name': b.name,
                    'load_time_ms': b.load_time_ms,
                    'memory_mb': b.memory_usage_mb,
                    'dependencies_count': len(b.dependencies),
                    'lines_of_code': b.lines_of_code,
                    'success': b.success
                } for b in benchmarks
            ]
        }

        return report

    def _analyze_dependency_coupling(self, benchmarks: List[ManagerBenchmark]) -> Dict[str, Any]:
        """åˆ†æä¾èµ–è€¦åˆåº¦"""
        # æ„å»ºä¾èµ–å›¾
        dependency_count = {}
        for benchmark in benchmarks:
            dependency_count[benchmark.name] = len(benchmark.dependencies)

        # è¯†åˆ«é«˜è€¦åˆæ¨¡å—
        high_coupling = {name: count for name, count in dependency_count.items() if count > 3}

        return {
            'dependency_counts': dependency_count,
            'high_coupling_modules': high_coupling,
            'average_dependencies': sum(dependency_count.values()) / len(dependency_count) if dependency_count else 0
        }

    def generate_refactoring_recommendations(self, report: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆé‡æ„å»ºè®®"""
        recommendations = []

        # æ€§èƒ½ä¼˜åŒ–å»ºè®®
        if report['summary']['total_load_time_ms'] > 1000:
            recommendations.append("âš ï¸ æ€»åŠ è½½æ—¶é—´è¶…è¿‡1ç§’ï¼Œå»ºè®®ä¼˜åŒ–æ¨¡å—åŠ è½½ç­–ç•¥")

        if report['summary']['total_memory_mb'] > 50:
            recommendations.append("âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå»ºè®®å®ç°æ‡’åŠ è½½æœºåˆ¶")

        # æ¨¡å—åˆå¹¶å»ºè®®
        if report['summary']['total_managers'] > 20:
            recommendations.append("ğŸ”„ Managerç±»è¿‡å¤šï¼Œå»ºè®®åˆå¹¶åŠŸèƒ½ç›¸è¿‘çš„æ¨¡å—")

        # è€¦åˆåº¦å»ºè®®
        high_coupling = report['coupling_analysis']['high_coupling_modules']
        if high_coupling:
            recommendations.append(f"ğŸ”— é«˜è€¦åˆæ¨¡å—éœ€è¦è§£è€¦: {list(high_coupling.keys())}")

        # å¤±è´¥æ¨¡å—å»ºè®®
        if report['failed_modules']:
            recommendations.append(f"âŒ ä¿®å¤å¤±è´¥çš„æ¨¡å—: {[name for name, _ in report['failed_modules']]}")

        return recommendations

def run_architecture_benchmark():
    """è¿è¡Œæ¶æ„åŸºå‡†æµ‹è¯•"""
    analyzer = ArchitectureAnalyzer()

    print("ğŸš€ Perfect21 æ¶æ„åˆ†æå·¥å…·")
    print("=" * 80)

    # è¿è¡Œåˆ†æ
    report = analyzer.run_comprehensive_analysis()

    # ç”Ÿæˆå»ºè®®
    recommendations = analyzer.generate_refactoring_recommendations(report)

    # æ‰“å°ç»“æœ
    print("\nğŸ“Š åˆ†æç»“æœ:")
    print("-" * 40)
    summary = report['summary']
    print(f"æ€»Manageræ•°é‡: {summary['total_managers']}")
    print(f"æˆåŠŸåŠ è½½: {summary['successful_loads']}")
    print(f"åŠ è½½å¤±è´¥: {summary['failed_loads']}")
    print(f"æ€»åŠ è½½æ—¶é—´: {summary['total_load_time_ms']:.1f}ms")
    print(f"æ€»å†…å­˜ä½¿ç”¨: {summary['total_memory_mb']:.1f}MB")
    print(f"æ€»ä»£ç è¡Œæ•°: {summary['total_lines_of_code']}")
    print(f"å¹³å‡åŠ è½½æ—¶é—´: {summary['average_load_time_ms']:.1f}ms")

    print(f"\nğŸŒ æœ€æ…¢çš„5ä¸ªManager:")
    for name, time_ms in report['performance_bottlenecks']['slowest_managers']:
        print(f"  â€¢ {name}: {time_ms:.1f}ms")

    print(f"\nğŸ§  å†…å­˜ä½¿ç”¨æœ€å¤šçš„5ä¸ªManager:")
    for name, memory_mb in report['performance_bottlenecks']['memory_heavy_managers']:
        print(f"  â€¢ {name}: {memory_mb:.1f}MB")

    print(f"\nğŸ”— é«˜è€¦åˆæ¨¡å—:")
    for name, dep_count in report['coupling_analysis']['high_coupling_modules'].items():
        print(f"  â€¢ {name}: {dep_count}ä¸ªä¾èµ–")

    if report['failed_modules']:
        print(f"\nâŒ å¤±è´¥çš„æ¨¡å—:")
        for name, error in report['failed_modules']:
            print(f"  â€¢ {name}: {error}")

    print(f"\nğŸ’¡ é‡æ„å»ºè®®:")
    for i, rec in enumerate(recommendations, 1):
        print(f"  {i}. {rec}")

    return report

if __name__ == "__main__":
    report = run_architecture_benchmark()

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    import json
    with open('architecture_performance_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: architecture_performance_report.json")