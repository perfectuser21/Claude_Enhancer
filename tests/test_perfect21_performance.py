#!/usr/bin/env python3
"""
Perfect21 æ€§èƒ½æµ‹è¯•åˆ†æå™¨
æ·±å…¥åˆ†æç³»ç»Ÿçš„å„ä¸ªæ€§èƒ½ç»´åº¦
"""

import os
import sys
import time
import psutil
import json
import asyncio
import threading
import tracemalloc
import gc
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from pathlib import Path
import subprocess
import concurrent.futures
import resource

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__)))

# Performance measurement utilities
@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    timestamp: str
    startup_time: float
    memory_usage: Dict[str, float]
    cpu_usage: float
    io_operations: Dict[str, int]
    execution_times: Dict[str, float]
    parallel_efficiency: Dict[str, Any]
    database_metrics: Dict[str, Any]
    bottlenecks: List[str]
    recommendations: List[str]

class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.start_time = time.time()
        self.measurements = []
        self.process = psutil.Process()
        self.baseline_memory = self.process.memory_info().rss / 1024 / 1024  # MB

        # å¯ç”¨å†…å­˜è·Ÿè¸ª
        tracemalloc.start()

        # æ€§èƒ½é˜ˆå€¼é…ç½®
        self.thresholds = {
            'startup_time': 3.0,  # ç§’
            'memory_growth': 100,  # MB
            'cpu_usage': 80,  # %
            'response_time': 1.0,  # ç§’
            'parallel_efficiency': 0.7  # 70%
        }

    def measure_startup_performance(self) -> Dict[str, Any]:
        """æµ‹é‡å¯åŠ¨æ€§èƒ½"""
        print("ğŸš€ æµ‹é‡Perfect21å¯åŠ¨æ€§èƒ½...")

        startup_metrics = {}

        # æµ‹é‡æ ¸å¿ƒæ¨¡å—åŠ è½½æ—¶é—´
        module_load_times = {}

        modules_to_test = [
            'main.perfect21',
            'main.cli',
            'features.capability_discovery',
            'features.git_workflow',
            'features.workflow_orchestrator',
            'features.sync_point_manager',
            'features.parallel_executor'
        ]

        for module in modules_to_test:
            start = time.time()
            try:
                __import__(module)
                load_time = time.time() - start
                module_load_times[module] = load_time
                print(f"  âœ… {module}: {load_time:.3f}s")
            except ImportError as e:
                module_load_times[module] = -1
                print(f"  âŒ {module}: å¯¼å…¥å¤±è´¥ - {e}")

        # æµ‹é‡Perfect21å®ä¾‹åŒ–æ—¶é—´
        start = time.time()
        try:
            from main.perfect21 import Perfect21
            p21 = Perfect21()
            instance_time = time.time() - start
            startup_metrics['instance_creation'] = instance_time
            print(f"  âœ… Perfect21å®ä¾‹åŒ–: {instance_time:.3f}s")

            # æµ‹é‡ç¬¬ä¸€æ¬¡çŠ¶æ€æŸ¥è¯¢æ—¶é—´
            start = time.time()
            status = p21.status()
            first_call_time = time.time() - start
            startup_metrics['first_status_call'] = first_call_time
            print(f"  âœ… é¦–æ¬¡çŠ¶æ€æŸ¥è¯¢: {first_call_time:.3f}s")

        except Exception as e:
            print(f"  âŒ Perfect21å®ä¾‹åŒ–å¤±è´¥: {e}")
            startup_metrics['instance_creation'] = -1
            startup_metrics['first_status_call'] = -1

        total_startup = time.time() - self.start_time
        startup_metrics['total_startup_time'] = total_startup
        startup_metrics['module_load_times'] = module_load_times

        print(f"ğŸ“Š æ€»å¯åŠ¨æ—¶é—´: {total_startup:.3f}s")

        return startup_metrics

    def measure_memory_usage(self) -> Dict[str, Any]:
        """æµ‹é‡å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("ğŸ’¾ åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼...")

        memory_metrics = {}

        # å½“å‰å†…å­˜ä½¿ç”¨
        memory_info = self.process.memory_info()
        memory_metrics['rss'] = memory_info.rss / 1024 / 1024  # MB
        memory_metrics['vms'] = memory_info.vms / 1024 / 1024  # MB
        memory_metrics['shared'] = getattr(memory_info, 'shared', 0) / 1024 / 1024  # MB

        # å†…å­˜å¢é•¿
        memory_growth = memory_metrics['rss'] - self.baseline_memory
        memory_metrics['growth_since_startup'] = memory_growth

        # Pythonå¯¹è±¡å†…å­˜ç»Ÿè®¡
        current, peak = tracemalloc.get_traced_memory()
        memory_metrics['python_current'] = current / 1024 / 1024  # MB
        memory_metrics['python_peak'] = peak / 1024 / 1024  # MB

        # åƒåœ¾å›æ”¶ç»Ÿè®¡
        gc_stats = gc.get_stats()
        memory_metrics['gc_stats'] = gc_stats
        memory_metrics['gc_objects'] = len(gc.get_objects())

        # ç³»ç»Ÿå†…å­˜
        system_memory = psutil.virtual_memory()
        memory_metrics['system_total'] = system_memory.total / 1024 / 1024 / 1024  # GB
        memory_metrics['system_available'] = system_memory.available / 1024 / 1024 / 1024  # GB
        memory_metrics['system_usage_percent'] = system_memory.percent

        print(f"  RSSå†…å­˜: {memory_metrics['rss']:.1f}MB")
        print(f"  Pythonå†…å­˜: {memory_metrics['python_current']:.1f}MB (å³°å€¼: {memory_metrics['python_peak']:.1f}MB)")
        print(f"  å†…å­˜å¢é•¿: {memory_growth:.1f}MB")
        print(f"  ç³»ç»Ÿå†…å­˜ä½¿ç”¨: {memory_metrics['system_usage_percent']:.1f}%")

        return memory_metrics

    def measure_execution_performance(self) -> Dict[str, Any]:
        """æµ‹é‡æ‰§è¡Œæ€§èƒ½"""
        print("âš¡ æµ‹é‡æ‰§è¡Œæ€§èƒ½...")

        execution_metrics = {}

        # æµ‹è¯•å„ç§æ“ä½œçš„æ‰§è¡Œæ—¶é—´
        operations = [
            ('status_query', self._test_status_query),
            ('git_hooks_status', self._test_git_hooks_status),
            ('capability_discovery', self._test_capability_discovery),
            ('workflow_status', self._test_workflow_status),
            ('parallel_preparation', self._test_parallel_preparation)
        ]

        for op_name, op_func in operations:
            times = []
            for i in range(5):  # è¿è¡Œ5æ¬¡å–å¹³å‡å€¼
                start = time.time()
                try:
                    result = op_func()
                    execution_time = time.time() - start
                    times.append(execution_time)
                    if i == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡æ˜¾ç¤ºç»“æœ
                        success = "âœ…" if result.get('success', True) else "âŒ"
                        print(f"  {success} {op_name}: {execution_time:.3f}s")
                except Exception as e:
                    times.append(-1)
                    print(f"  âŒ {op_name}: å¤±è´¥ - {e}")

            valid_times = [t for t in times if t > 0]
            if valid_times:
                execution_metrics[op_name] = {
                    'avg_time': sum(valid_times) / len(valid_times),
                    'min_time': min(valid_times),
                    'max_time': max(valid_times),
                    'success_rate': len(valid_times) / len(times)
                }
            else:
                execution_metrics[op_name] = {
                    'avg_time': -1,
                    'success_rate': 0
                }

        return execution_metrics

    def measure_io_performance(self) -> Dict[str, Any]:
        """æµ‹é‡I/Oæ€§èƒ½"""
        print("ğŸ’¿ åˆ†æI/Oæ€§èƒ½...")

        io_metrics = {}

        # è¿›ç¨‹I/Oç»Ÿè®¡
        try:
            io_counters = self.process.io_counters()
            io_metrics['read_count'] = io_counters.read_count
            io_metrics['write_count'] = io_counters.write_count
            io_metrics['read_bytes'] = io_counters.read_bytes / 1024 / 1024  # MB
            io_metrics['write_bytes'] = io_counters.write_bytes / 1024 / 1024  # MB
        except (psutil.AccessDenied, AttributeError):
            io_metrics['error'] = 'æ— æ³•è·å–I/Oç»Ÿè®¡'

        # æµ‹è¯•æ–‡ä»¶æ“ä½œæ€§èƒ½
        test_file = '/tmp/perfect21_io_test.txt'
        file_ops = {}

        # å†™å…¥æµ‹è¯•
        start = time.time()
        try:
            with open(test_file, 'w') as f:
                f.write('x' * 1024 * 1024)  # 1MB
            file_ops['write_1mb'] = time.time() - start
        except Exception as e:
            file_ops['write_1mb'] = -1

        # è¯»å–æµ‹è¯•
        start = time.time()
        try:
            with open(test_file, 'r') as f:
                content = f.read()
            file_ops['read_1mb'] = time.time() - start
        except Exception as e:
            file_ops['read_1mb'] = -1

        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        try:
            os.remove(test_file)
        except:
            pass

        io_metrics['file_operations'] = file_ops

        # ç£ç›˜ä½¿ç”¨æƒ…å†µ
        try:
            disk_usage = psutil.disk_usage('.')
            io_metrics['disk_total'] = disk_usage.total / 1024 / 1024 / 1024  # GB
            io_metrics['disk_used'] = disk_usage.used / 1024 / 1024 / 1024  # GB
            io_metrics['disk_free'] = disk_usage.free / 1024 / 1024 / 1024  # GB
            io_metrics['disk_usage_percent'] = (disk_usage.used / disk_usage.total) * 100
        except Exception as e:
            io_metrics['disk_error'] = str(e)

        if 'read_bytes' in io_metrics:
            print(f"  I/Oè¯»å–: {io_metrics['read_bytes']:.2f}MB ({io_metrics['read_count']}æ¬¡)")
            print(f"  I/Oå†™å…¥: {io_metrics['write_bytes']:.2f}MB ({io_metrics['write_count']}æ¬¡)")

        if 'write_1mb' in file_ops and file_ops['write_1mb'] > 0:
            print(f"  æ–‡ä»¶å†™å…¥(1MB): {file_ops['write_1mb']:.3f}s")
        if 'read_1mb' in file_ops and file_ops['read_1mb'] > 0:
            print(f"  æ–‡ä»¶è¯»å–(1MB): {file_ops['read_1mb']:.3f}s")

        return io_metrics

    def measure_parallel_efficiency(self) -> Dict[str, Any]:
        """æµ‹é‡å¹¶è¡Œæ‰§è¡Œæ•ˆç‡"""
        print("ğŸ”„ åˆ†æå¹¶è¡Œæ‰§è¡Œæ•ˆç‡...")

        parallel_metrics = {}

        try:
            # æµ‹è¯•æ™ºèƒ½åˆ†è§£å™¨
            from features.smart_decomposer import get_smart_decomposer

            decomposer = get_smart_decomposer()

            # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡åˆ†è§£
            test_tasks = [
                "å®ç°ä¸€ä¸ªç®€å•çš„ç”¨æˆ·æ³¨å†ŒåŠŸèƒ½",
                "è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„ç”µå•†ç³»ç»Ÿï¼ŒåŒ…æ‹¬ç”¨æˆ·ç®¡ç†ã€å•†å“ç®¡ç†ã€è®¢å•å¤„ç†ã€æ”¯ä»˜é›†æˆã€åº“å­˜ç®¡ç†",
                "ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½"
            ]

            decomposition_times = []
            complexities = []

            for task in test_tasks:
                start = time.time()
                try:
                    analysis = decomposer.decompose_task(task)
                    decomp_time = time.time() - start
                    decomposition_times.append(decomp_time)

                    if analysis:
                        complexities.append(analysis.complexity.value)
                        print(f"  âœ… ä»»åŠ¡åˆ†è§£: {decomp_time:.3f}s (å¤æ‚åº¦: {analysis.complexity.value})")
                    else:
                        print(f"  âŒ ä»»åŠ¡åˆ†è§£å¤±è´¥: {task[:30]}...")
                except Exception as e:
                    print(f"  âŒ åˆ†è§£é”™è¯¯: {e}")
                    decomposition_times.append(-1)

            valid_times = [t for t in decomposition_times if t > 0]
            if valid_times:
                parallel_metrics['decomposition_avg_time'] = sum(valid_times) / len(valid_times)
                parallel_metrics['decomposition_success_rate'] = len(valid_times) / len(test_tasks)

            parallel_metrics['tested_complexities'] = complexities

        except ImportError:
            parallel_metrics['error'] = 'å¹¶è¡Œæ‰§è¡Œæ¨¡å—æœªå¯ç”¨'
            print("  âš ï¸ å¹¶è¡Œæ‰§è¡Œæ¨¡å—æœªå®‰è£…æˆ–ä¸å¯ç”¨")

        # CPUæ ¸å¿ƒæ•°å’Œå¹¶è¡Œæ½œåŠ›
        parallel_metrics['cpu_cores'] = psutil.cpu_count(logical=False)
        parallel_metrics['logical_cores'] = psutil.cpu_count(logical=True)
        parallel_metrics['cpu_freq'] = psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None

        print(f"  CPUæ ¸å¿ƒ: {parallel_metrics['cpu_cores']}ç‰©ç† / {parallel_metrics['logical_cores']}é€»è¾‘")

        return parallel_metrics

    def measure_database_performance(self) -> Dict[str, Any]:
        """æµ‹é‡æ•°æ®åº“æ€§èƒ½"""
        print("ğŸ—„ï¸ åˆ†ææ•°æ®åº“æ€§èƒ½...")

        db_metrics = {}

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®åº“è¿æ¥
        db_files = []
        for ext in ['.db', '.sqlite', '.sqlite3']:
            db_files.extend(Path('.').rglob(f'*{ext}'))

        db_metrics['database_files'] = [str(f) for f in db_files]
        db_metrics['database_count'] = len(db_files)

        # åˆ†ææ•°æ®åº“æ–‡ä»¶å¤§å°
        total_db_size = 0
        for db_file in db_files:
            try:
                size = db_file.stat().st_size / 1024 / 1024  # MB
                total_db_size += size
                print(f"  ğŸ“ {db_file.name}: {size:.2f}MB")
            except Exception as e:
                print(f"  âŒ æ— æ³•è¯»å– {db_file}: {e}")

        db_metrics['total_database_size'] = total_db_size

        # æ£€æŸ¥æ•°æ®åº“é…ç½®æ–‡ä»¶
        config_files = []
        for pattern in ['*database*', '*db*', '*config*']:
            config_files.extend(Path('.').rglob(f'{pattern}.yaml'))
            config_files.extend(Path('.').rglob(f'{pattern}.yml'))
            config_files.extend(Path('.').rglob(f'{pattern}.json'))

        db_metrics['config_files'] = [str(f) for f in config_files]

        # æ¨¡æ‹Ÿæ•°æ®åº“æ“ä½œæ€§èƒ½æµ‹è¯•
        try:
            import sqlite3
            test_db = '/tmp/perfect21_db_test.sqlite'

            # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
            start = time.time()
            conn = sqlite3.connect(test_db)
            cursor = conn.cursor()

            # åˆ›å»ºè¡¨
            cursor.execute('''
                CREATE TABLE test_table (
                    id INTEGER PRIMARY KEY,
                    name TEXT,
                    value INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # æ’å…¥æµ‹è¯•æ•°æ®
            insert_start = time.time()
            for i in range(1000):
                cursor.execute('INSERT INTO test_table (name, value) VALUES (?, ?)',
                             (f'item_{i}', i))
            conn.commit()
            insert_time = time.time() - insert_start

            # æŸ¥è¯¢æµ‹è¯•
            query_start = time.time()
            cursor.execute('SELECT COUNT(*) FROM test_table WHERE value > 500')
            result = cursor.fetchone()
            query_time = time.time() - query_start

            conn.close()

            # æ¸…ç†
            os.remove(test_db)

            db_metrics['mock_performance'] = {
                'insert_1000_records': insert_time,
                'count_query': query_time,
                'total_test_time': time.time() - start
            }

            print(f"  âœ… æ¨¡æ‹ŸDBæ’å…¥(1000æ¡): {insert_time:.3f}s")
            print(f"  âœ… æ¨¡æ‹ŸDBæŸ¥è¯¢: {query_time:.3f}s")

        except Exception as e:
            db_metrics['mock_performance'] = {'error': str(e)}
            print(f"  âŒ æ•°æ®åº“æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")

        return db_metrics

    def identify_bottlenecks(self, metrics: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []

        # æ£€æŸ¥å¯åŠ¨æ—¶é—´
        startup = metrics.get('startup_performance', {})
        if startup.get('total_startup_time', 0) > self.thresholds['startup_time']:
            bottlenecks.append(f"å¯åŠ¨æ—¶é—´è¿‡é•¿: {startup.get('total_startup_time', 0):.2f}s > {self.thresholds['startup_time']}s")

        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        memory = metrics.get('memory_usage', {})
        if memory.get('growth_since_startup', 0) > self.thresholds['memory_growth']:
            bottlenecks.append(f"å†…å­˜å¢é•¿è¿‡å¤š: {memory.get('growth_since_startup', 0):.1f}MB > {self.thresholds['memory_growth']}MB")

        # æ£€æŸ¥CPUä½¿ç”¨
        cpu_percent = metrics.get('cpu_usage', {}).get('current', 0)
        if cpu_percent > self.thresholds['cpu_usage']:
            bottlenecks.append(f"CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_percent:.1f}% > {self.thresholds['cpu_usage']}%")

        # æ£€æŸ¥æ‰§è¡Œæ—¶é—´
        execution = metrics.get('execution_performance', {})
        for op_name, op_metrics in execution.items():
            if isinstance(op_metrics, dict) and op_metrics.get('avg_time', 0) > self.thresholds['response_time']:
                bottlenecks.append(f"{op_name}å“åº”æ—¶é—´è¿‡é•¿: {op_metrics['avg_time']:.3f}s > {self.thresholds['response_time']}s")

        # æ£€æŸ¥I/Oæ€§èƒ½
        io_metrics = metrics.get('io_performance', {})
        file_ops = io_metrics.get('file_operations', {})
        if file_ops.get('write_1mb', 0) > 1.0:  # 1MBå†™å…¥è¶…è¿‡1ç§’
            bottlenecks.append(f"æ–‡ä»¶å†™å…¥æ€§èƒ½è¾ƒå·®: {file_ops['write_1mb']:.3f}s/MB")

        return bottlenecks

    def generate_recommendations(self, metrics: Dict[str, Any], bottlenecks: List[str]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºç“¶é¢ˆçš„å»ºè®®
        for bottleneck in bottlenecks:
            if "å¯åŠ¨æ—¶é—´" in bottleneck:
                recommendations.append("ä¼˜åŒ–æ¨¡å—å¯¼å…¥ï¼šä½¿ç”¨å»¶è¿Ÿå¯¼å…¥(lazy import)å‡å°‘å¯åŠ¨æ—¶é—´")
                recommendations.append("ç¼“å­˜é…ç½®ï¼šå°†é…ç½®æ–‡ä»¶è§£æç»“æœç¼“å­˜åˆ°å†…å­˜")

            if "å†…å­˜å¢é•¿" in bottleneck:
                recommendations.append("å†…å­˜ç®¡ç†ï¼šå®ç°å¯¹è±¡æ± å’Œç¼“å­˜æ¸…ç†æœºåˆ¶")
                recommendations.append("åƒåœ¾å›æ”¶ï¼šè°ƒæ•´GCé˜ˆå€¼ï¼Œå®šæœŸæ‰§è¡Œå¼ºåˆ¶åƒåœ¾å›æ”¶")

            if "CPUä½¿ç”¨ç‡" in bottleneck:
                recommendations.append("å¼‚æ­¥å¤„ç†ï¼šå°†CPUå¯†é›†å‹æ“ä½œç§»è‡³åå°çº¿ç¨‹")
                recommendations.append("ç®—æ³•ä¼˜åŒ–ï¼šä¼˜åŒ–å…³é”®è·¯å¾„çš„ç®—æ³•å¤æ‚åº¦")

            if "å“åº”æ—¶é—´" in bottleneck:
                recommendations.append("ç¼“å­˜ç­–ç•¥ï¼šå®ç°ç»“æœç¼“å­˜å‡å°‘é‡å¤è®¡ç®—")
                recommendations.append("æ•°æ®åº“ä¼˜åŒ–ï¼šæ·»åŠ ç´¢å¼•å’ŒæŸ¥è¯¢ä¼˜åŒ–")

        # åŸºäºç³»ç»Ÿèµ„æºçš„å»ºè®®
        memory = metrics.get('memory_usage', {})
        if memory.get('system_usage_percent', 0) > 80:
            recommendations.append("ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼šè€ƒè™‘å¢åŠ ç³»ç»Ÿå†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")

        parallel = metrics.get('parallel_efficiency', {})
        if parallel.get('cpu_cores', 1) > 2:
            recommendations.append("å¹¶è¡Œä¼˜åŒ–ï¼šå……åˆ†åˆ©ç”¨å¤šæ ¸CPUï¼Œå¢åŠ å¹¶è¡Œæ‰§è¡Œçš„æœºä¼š")

        # I/Oä¼˜åŒ–å»ºè®®
        io_metrics = metrics.get('io_performance', {})
        if io_metrics.get('disk_usage_percent', 0) > 90:
            recommendations.append("ç£ç›˜ç©ºé—´ï¼šæ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œè€ƒè™‘å¢åŠ å­˜å‚¨ç©ºé—´")

        # æ•°æ®åº“ä¼˜åŒ–å»ºè®®
        db_metrics = metrics.get('database_performance', {})
        if db_metrics.get('total_database_size', 0) > 100:  # 100MB
            recommendations.append("æ•°æ®åº“ä¼˜åŒ–ï¼šè€ƒè™‘æ•°æ®åº“æ¸…ç†ã€ç´¢å¼•ä¼˜åŒ–æˆ–åˆ†åº“åˆ†è¡¨")

        return recommendations

    # æµ‹è¯•è¾…åŠ©æ–¹æ³•
    def _test_status_query(self) -> Dict[str, Any]:
        """æµ‹è¯•çŠ¶æ€æŸ¥è¯¢"""
        try:
            from main.perfect21 import Perfect21
            p21 = Perfect21()
            return p21.status()
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _test_git_hooks_status(self) -> Dict[str, Any]:
        """æµ‹è¯•Gité’©å­çŠ¶æ€"""
        try:
            from features.git_workflow import GitHooks
            hooks = GitHooks('.')
            return hooks.get_hook_status()
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _test_capability_discovery(self) -> Dict[str, Any]:
        """æµ‹è¯•èƒ½åŠ›å‘ç°"""
        try:
            from features.capability_discovery import get_perfect21_capabilities
            return get_perfect21_capabilities()
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _test_workflow_status(self) -> Dict[str, Any]:
        """æµ‹è¯•å·¥ä½œæµçŠ¶æ€"""
        try:
            from features.git_workflow import WorkflowManager
            wm = WorkflowManager('.')
            return wm.get_workflow_status()
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _test_parallel_preparation(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶è¡Œå‡†å¤‡"""
        try:
            from features.smart_decomposer import get_smart_decomposer
            decomposer = get_smart_decomposer()
            return {'success': True, 'decomposer_ready': True}
        except Exception as e:
            return {'success': False, 'error': str(e)}

def run_comprehensive_performance_analysis():
    """è¿è¡Œå®Œæ•´çš„æ€§èƒ½åˆ†æ"""
    print("ğŸ¯ Perfect21 æ€§èƒ½åˆ†æå¼€å§‹")
    print("=" * 80)

    profiler = PerformanceProfiler()

    # 1. å¯åŠ¨æ€§èƒ½åˆ†æ
    startup_metrics = profiler.measure_startup_performance()
    print()

    # 2. å†…å­˜ä½¿ç”¨åˆ†æ
    memory_metrics = profiler.measure_memory_usage()
    print()

    # 3. æ‰§è¡Œæ€§èƒ½åˆ†æ
    execution_metrics = profiler.measure_execution_performance()
    print()

    # 4. I/Oæ€§èƒ½åˆ†æ
    io_metrics = profiler.measure_io_performance()
    print()

    # 5. å¹¶è¡Œæ•ˆç‡åˆ†æ
    parallel_metrics = profiler.measure_parallel_efficiency()
    print()

    # 6. æ•°æ®åº“æ€§èƒ½åˆ†æ
    database_metrics = profiler.measure_database_performance()
    print()

    # CPUä½¿ç”¨ç‡å¿«ç…§
    cpu_metrics = {
        'current': psutil.cpu_percent(interval=1),
        'per_cpu': psutil.cpu_percent(percpu=True),
        'load_average': os.getloadavg() if hasattr(os, 'getloadavg') else None
    }

    # ç»¼åˆæ‰€æœ‰æŒ‡æ ‡
    all_metrics = {
        'timestamp': datetime.now().isoformat(),
        'startup_performance': startup_metrics,
        'memory_usage': memory_metrics,
        'cpu_usage': cpu_metrics,
        'execution_performance': execution_metrics,
        'io_performance': io_metrics,
        'parallel_efficiency': parallel_metrics,
        'database_performance': database_metrics
    }

    # è¯†åˆ«ç“¶é¢ˆ
    print("ğŸ” åˆ†ææ€§èƒ½ç“¶é¢ˆ...")
    bottlenecks = profiler.identify_bottlenecks(all_metrics)
    all_metrics['bottlenecks'] = bottlenecks

    # ç”Ÿæˆä¼˜åŒ–å»ºè®®
    print("ğŸ’¡ ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
    recommendations = profiler.generate_recommendations(all_metrics, bottlenecks)
    all_metrics['recommendations'] = recommendations

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_file = f"performance_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(all_metrics, f, indent=2, ensure_ascii=False, default=str)

    # æ˜¾ç¤ºæ‘˜è¦
    print("\n" + "=" * 80)
    print("ğŸ“Š Perfect21 æ€§èƒ½åˆ†ææ‘˜è¦")
    print("=" * 80)

    # å…³é”®æŒ‡æ ‡
    print("ğŸš€ å…³é”®æ€§èƒ½æŒ‡æ ‡:")
    print(f"  å¯åŠ¨æ—¶é—´: {startup_metrics.get('total_startup_time', 0):.3f}s")
    print(f"  å†…å­˜ä½¿ç”¨: {memory_metrics.get('rss', 0):.1f}MB")
    print(f"  CPUä½¿ç”¨ç‡: {cpu_metrics.get('current', 0):.1f}%")

    if execution_metrics:
        avg_response = sum(m.get('avg_time', 0) for m in execution_metrics.values() if isinstance(m, dict)) / len(execution_metrics)
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response:.3f}s")

    # ç“¶é¢ˆåˆ†æ
    print(f"\nğŸš¨ å‘ç°ç“¶é¢ˆ: {len(bottlenecks)}ä¸ª")
    for bottleneck in bottlenecks:
        print(f"  âš ï¸ {bottleneck}")

    # ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®: {len(recommendations)}ä¸ª")
    for i, rec in enumerate(recommendations[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ªå»ºè®®
        print(f"  {i}. {rec}")

    if len(recommendations) > 5:
        print(f"  ... è¿˜æœ‰{len(recommendations) - 5}ä¸ªå»ºè®®")

    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    # æ€§èƒ½è¯„çº§
    score = calculate_performance_score(all_metrics)
    grade = get_performance_grade(score)
    print(f"\nâ­ Perfect21 æ€§èƒ½è¯„çº§: {grade} ({score:.1f}/100)")

    return all_metrics

def calculate_performance_score(metrics: Dict[str, Any]) -> float:
    """è®¡ç®—æ€§èƒ½å¾—åˆ†"""
    score = 100.0

    # å¯åŠ¨æ—¶é—´ (20åˆ†)
    startup_time = metrics.get('startup_performance', {}).get('total_startup_time', 0)
    if startup_time > 5:
        score -= 20
    elif startup_time > 3:
        score -= 10
    elif startup_time > 1:
        score -= 5

    # å†…å­˜ä½¿ç”¨ (25åˆ†)
    memory_growth = metrics.get('memory_usage', {}).get('growth_since_startup', 0)
    if memory_growth > 200:
        score -= 25
    elif memory_growth > 100:
        score -= 15
    elif memory_growth > 50:
        score -= 8

    # CPUä½¿ç”¨ (20åˆ†)
    cpu_usage = metrics.get('cpu_usage', {}).get('current', 0)
    if cpu_usage > 80:
        score -= 20
    elif cpu_usage > 60:
        score -= 10
    elif cpu_usage > 40:
        score -= 5

    # å“åº”æ—¶é—´ (20åˆ†)
    execution = metrics.get('execution_performance', {})
    if execution:
        avg_times = [m.get('avg_time', 0) for m in execution.values() if isinstance(m, dict) and m.get('avg_time', 0) > 0]
        if avg_times:
            avg_response = sum(avg_times) / len(avg_times)
            if avg_response > 2:
                score -= 20
            elif avg_response > 1:
                score -= 10
            elif avg_response > 0.5:
                score -= 5

    # ç“¶é¢ˆæ•°é‡ (15åˆ†)
    bottleneck_count = len(metrics.get('bottlenecks', []))
    if bottleneck_count > 5:
        score -= 15
    elif bottleneck_count > 3:
        score -= 10
    elif bottleneck_count > 1:
        score -= 5

    return max(0, score)

def get_performance_grade(score: float) -> str:
    """è·å–æ€§èƒ½ç­‰çº§"""
    if score >= 90:
        return "A+ (ä¼˜ç§€)"
    elif score >= 80:
        return "A (è‰¯å¥½)"
    elif score >= 70:
        return "B (ä¸­ç­‰)"
    elif score >= 60:
        return "C (åŠæ ¼)"
    else:
        return "D (éœ€è¦æ”¹è¿›)"

if __name__ == '__main__':
    try:
        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        print("ğŸ”§ ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥:")
        print(f"  Pythonç‰ˆæœ¬: {sys.version}")
        print(f"  CPU: {psutil.cpu_count(logical=False)}æ ¸å¿ƒ / {psutil.cpu_count(logical=True)}çº¿ç¨‹")
        print(f"  å†…å­˜: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f}GB")
        print(f"  ç£ç›˜: {psutil.disk_usage('.').free / 1024 / 1024 / 1024:.1f}GB å¯ç”¨")
        print()

        # è¿è¡Œæ€§èƒ½åˆ†æ
        metrics = run_comprehensive_performance_analysis()

        print("\nâœ… æ€§èƒ½åˆ†æå®Œæˆ!")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ æ€§èƒ½åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ€§èƒ½åˆ†æå¤±è´¥: {e}")
        traceback.print_exc()