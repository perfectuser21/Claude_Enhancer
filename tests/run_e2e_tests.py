#!/usr/bin/env python3
"""
Claude Enhancer 5.1 ç«¯åˆ°ç«¯æµ‹è¯•æ‰§è¡Œå™¨
æ”¯æŒä¸åŒæµ‹è¯•æ¨¡å¼å’Œé…ç½®é€‰é¡¹
"""

import os
import sys
import argparse
import json
from pathlib import Path
from e2e_test_framework import E2ETestFramework

def main():
    parser = argparse.ArgumentParser(
        description="Claude Enhancer 5.1 ç«¯åˆ°ç«¯æµ‹è¯•æ‰§è¡Œå™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æµ‹è¯•æ¨¡å¼è¯´æ˜:
  full         - å®Œæ•´æµ‹è¯•å¥—ä»¶ï¼ˆé»˜è®¤ï¼‰
  workflow     - ä»…å·¥ä½œæµæµ‹è¯•ï¼ˆPhase 0-7ï¼‰
  agents       - ä»…Agentåä½œæµ‹è¯•
  git          - ä»…Gité›†æˆæµ‹è¯•
  hooks        - ä»…Hookè§¦å‘æµ‹è¯•
  recovery     - ä»…é”™è¯¯æ¢å¤æµ‹è¯•
  scenarios    - ä»…ç”¨æˆ·åœºæ™¯æµ‹è¯•
  performance  - ä»…æ€§èƒ½å‹åŠ›æµ‹è¯•
  concurrent   - ä»…å¹¶å‘æ‰§è¡Œæµ‹è¯•

ç¤ºä¾‹:
  python run_e2e_tests.py                    # è¿è¡Œå®Œæ•´æµ‹è¯•
  python run_e2e_tests.py --mode workflow    # ä»…æµ‹è¯•å·¥ä½œæµ
  python run_e2e_tests.py --mode agents --verbose    # è¯¦ç»†æ¨¡å¼æµ‹è¯•Agent
  python run_e2e_tests.py --quick            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
  python run_e2e_tests.py --stress           # å‹åŠ›æµ‹è¯•æ¨¡å¼
        """
    )

    parser.add_argument(
        "--mode", "-m",
        choices=["full", "workflow", "agents", "git", "hooks", "recovery", "scenarios", "performance", "concurrent"],
        default="full",
        help="æµ‹è¯•æ¨¡å¼ï¼ˆé»˜è®¤: fullï¼‰"
    )

    parser.add_argument(
        "--project-path", "-p",
        default="/home/xx/dev/Claude Enhancer 5.0",
        help="é¡¹ç›®è·¯å¾„ï¼ˆé»˜è®¤: /home/xx/dev/Claude Enhancer 5.0ï¼‰"
    )

    parser.add_argument(
        "--output", "-o",
        help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„"
    )

    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="è¯¦ç»†è¾“å‡ºæ¨¡å¼"
    )

    parser.add_argument(
        "--quick",
        action="store_true",
        help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆè·³è¿‡è€—æ—¶æµ‹è¯•ï¼‰"
    )

    parser.add_argument(
        "--stress",
        action="store_true",
        help="å‹åŠ›æµ‹è¯•æ¨¡å¼ï¼ˆå¢åŠ æµ‹è¯•å¼ºåº¦ï¼‰"
    )

    parser.add_argument(
        "--parallel",
        type=int,
        default=4,
        help="å¹¶å‘æµ‹è¯•çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 4ï¼‰"
    )

    parser.add_argument(
        "--timeout",
        type=int,
        default=30,
        help="å•ä¸ªæµ‹è¯•è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤: 30ï¼‰"
    )

    parser.add_argument(
        "--config",
        help="è‡ªå®šä¹‰é…ç½®æ–‡ä»¶è·¯å¾„"
    )

    args = parser.parse_args()

    # åˆ›å»ºæµ‹è¯•æ¡†æ¶å®ä¾‹
    framework = E2ETestFramework(args.project_path)

    # åº”ç”¨é…ç½®
    if args.config and Path(args.config).exists():
        with open(args.config, 'r', encoding='utf-8') as f:
            custom_config = json.load(f)
            framework.test_config.update(custom_config)

    # æ›´æ–°é…ç½®
    framework.test_config.update({
        "timeout": args.timeout,
        "parallel_tests": args.parallel,
        "verbose": args.verbose,
        "quick_mode": args.quick,
        "stress_mode": args.stress
    })

    print("ğŸš€ å¯åŠ¨Claude Enhancer 5.1ç«¯åˆ°ç«¯æµ‹è¯•")
    print("="*60)
    print(f"æµ‹è¯•æ¨¡å¼: {args.mode}")
    print(f"é¡¹ç›®è·¯å¾„: {args.project_path}")
    print(f"å¹¶å‘æ•°: {args.parallel}")
    print(f"è¶…æ—¶: {args.timeout}ç§’")
    if args.quick:
        print("ğŸƒ å¿«é€Ÿæ¨¡å¼: å¯ç”¨")
    if args.stress:
        print("ğŸ’ª å‹åŠ›æ¨¡å¼: å¯ç”¨")
    print("="*60)

    # æ‰§è¡Œæµ‹è¯•
    if args.mode == "full":
        report = framework.run_all_tests()
    elif args.mode == "workflow":
        framework.test_results = framework.test_complete_workflow()
        report = framework.generate_test_report(sum(r.duration for r in framework.test_results))
    elif args.mode == "agents":
        framework.test_results = framework.test_agent_collaboration()
        report = framework.generate_test_report(sum(r.duration for r in framework.test_results))
    elif args.mode == "git":
        framework.test_results = framework.test_git_integration()
        report = framework.generate_test_report(sum(r.duration for r in framework.test_results))
    elif args.mode == "hooks":
        framework.test_results = framework.test_hook_triggering()
        report = framework.generate_test_report(sum(r.duration for r in framework.test_results))
    elif args.mode == "recovery":
        framework.test_results = framework.test_error_recovery()
        report = framework.generate_test_report(sum(r.duration for r in framework.test_results))
    elif args.mode == "scenarios":
        framework.test_results = framework.test_user_scenarios()
        report = framework.generate_test_report(sum(r.duration for r in framework.test_results))
    elif args.mode == "performance":
        framework.test_results = framework.test_performance_stress()
        report = framework.generate_test_report(sum(r.duration for r in framework.test_results))
    elif args.mode == "concurrent":
        framework.test_results = framework.test_concurrent_execution()
        report = framework.generate_test_report(sum(r.duration for r in framework.test_results))

    # è¾“å‡ºç»“æœ
    print_report(report, args.verbose)

    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        output_path = Path(args.output)
    else:
        timestamp = __import__('datetime').datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = Path(f"/tmp/claude/e2e_test_report_{args.mode}_{timestamp}.json")

    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {output_path}")

    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    if report['summary']['success_rate'] >= 80:
        print("âœ… æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼")
        return 1

def print_report(report, verbose=False):
    """æ‰“å°æµ‹è¯•æŠ¥å‘Š"""
    summary = report['summary']

    print("\n" + "="*80)
    print("ğŸ¯ æµ‹è¯•ç»“æœæ‘˜è¦")
    print("="*80)

    # åŸºæœ¬ç»Ÿè®¡
    print(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
    print(f"âœ… é€šè¿‡: {summary['passed']}")
    print(f"âŒ å¤±è´¥: {summary['failed']}")
    print(f"ğŸ’¥ é”™è¯¯: {summary['errors']}")
    print(f"â­ï¸  è·³è¿‡: {summary['skipped']}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.1f}%")
    print(f"â±ï¸  æ€»è€—æ—¶: {summary['total_duration']:.2f}ç§’")
    print(f"â±ï¸  å¹³å‡è€—æ—¶: {summary['avg_test_duration']:.3f}ç§’")

    # æˆåŠŸç‡æŒ‡ç¤ºå™¨
    success_rate = summary['success_rate']
    if success_rate >= 95:
        status = "ğŸŸ¢ ä¼˜ç§€"
    elif success_rate >= 80:
        status = "ğŸŸ¡ è‰¯å¥½"
    elif success_rate >= 60:
        status = "ğŸŸ  ä¸€èˆ¬"
    else:
        status = "ğŸ”´ éœ€è¦æ”¹è¿›"

    print(f"ğŸ¯ æ•´ä½“è¯„ä»·: {status}")

    # å„é˜¶æ®µæƒ…å†µ
    if report.get('phase_breakdown'):
        print(f"\nğŸ“‹ å„é˜¶æ®µæµ‹è¯•æƒ…å†µ:")
        for phase, stats in report['phase_breakdown'].items():
            phase_success_rate = (stats['passed'] / stats['total']) * 100 if stats['total'] > 0 else 0
            print(f"   ğŸ“Œ {phase}: {stats['passed']}/{stats['total']} ({phase_success_rate:.1f}%)")

    # å¤±è´¥çš„æµ‹è¯•
    if report.get('failed_tests'):
        print(f"\nâŒ å¤±è´¥çš„æµ‹è¯• ({len(report['failed_tests'])}):")
        for i, test in enumerate(report['failed_tests'][:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"   {i}. {test['name']} ({test.get('phase', 'N/A')})")
            if verbose:
                print(f"      åŸå› : {test.get('message', 'N/A')}")
                print(f"      è€—æ—¶: {test.get('duration', 0):.3f}ç§’")

        if len(report['failed_tests']) > 10:
            print(f"   ... è¿˜æœ‰ {len(report['failed_tests']) - 10} ä¸ªå¤±è´¥æµ‹è¯•")

    # æ€§èƒ½æŒ‡æ ‡
    if report.get('performance_metrics'):
        print(f"\nğŸ”§ æ€§èƒ½æŒ‡æ ‡:")
        for metric, value in report['performance_metrics'].items():
            if value > 0:
                print(f"   ğŸ“Š {metric.replace('_', ' ').title()}: {value}")

    # æ”¹è¿›å»ºè®®
    if report.get('recommendations'):
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for i, rec in enumerate(report['recommendations'][:5], 1):
            print(f"   {i}. {rec}")

    # è¯¦ç»†ç»“æœï¼ˆä»…åœ¨verboseæ¨¡å¼ä¸‹æ˜¾ç¤ºï¼‰
    if verbose and report.get('detailed_results'):
        print(f"\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for result in report['detailed_results'][:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
            status_icon = {"PASS": "âœ…", "FAIL": "âŒ", "ERROR": "ğŸ’¥", "SKIP": "â­ï¸"}.get(result['status'], "â“")
            print(f"   {status_icon} {result['name']} ({result.get('duration', 0):.3f}s)")
            if result['status'] in ['FAIL', 'ERROR'] and result.get('message'):
                print(f"      ğŸ’¬ {result['message']}")

    print("="*80)

if __name__ == "__main__":
    sys.exit(main())