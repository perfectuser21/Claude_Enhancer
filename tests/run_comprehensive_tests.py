#!/usr/bin/env python3
"""
Perfect21 ç»¼åˆæµ‹è¯•æ‰§è¡Œå™¨
ç»Ÿä¸€æ‰§è¡Œæ‰€æœ‰æµ‹è¯•å¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import os
import sys
import json
import time
import subprocess
import argparse
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime
import xml.etree.ElementTree as ET

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class TestRunner:
    """æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, test_dir: Path):
        self.test_dir = test_dir
        self.project_root = test_dir.parent
        self.results = {
            'start_time': datetime.now().isoformat(),
            'test_suites': {},
            'coverage': {},
            'performance': {},
            'security': {},
            'summary': {}
        }
    
    def run_all_tests(self, test_types: List[str] = None) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ Perfect21 ç»¼åˆæµ‹è¯•å¼€å§‹")
        print(f"ğŸ“… å¼€å§‹æ—¶é—´: {self.results['start_time']}")
        print(f"ğŸ“ æµ‹è¯•ç›®å½•: {self.test_dir}")
        
        available_test_types = ['unit', 'integration', 'security', 'performance', 'e2e']
        
        if test_types is None:
            test_types = available_test_types
        
        # æ£€æŸ¥pytestå¯ç”¨æ€§
        if not self._check_pytest_available():
            print("âŒ pytestä¸å¯ç”¨ï¼Œè¯·å…ˆå®‰è£…: pip install -r tests/requirements.txt")
            return self.results
        
        for test_type in test_types:
            print(f"\nğŸ“‹ æ­£åœ¨è¿è¡Œ {test_type} æµ‹è¯•...")
            
            if test_type == 'unit':
                self.run_unit_tests()
            elif test_type == 'integration':
                self.run_integration_tests()
            elif test_type == 'security':
                self.run_security_tests()
            elif test_type == 'performance':
                self.run_performance_tests()
            elif test_type == 'e2e':
                self.run_e2e_tests()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_final_report()
        
        return self.results
    
    def _check_pytest_available(self) -> bool:
        """æ£€æŸ¥pytestæ˜¯å¦å¯ç”¨"""
        try:
            subprocess.run(['pytest', '--version'], capture_output=True, check=True)
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            return False
    
    def run_unit_tests(self):
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        cmd = [
            'python3', '-m', 'pytest',
            str(self.test_dir),
            '-m', 'unit',
            '--cov=api', '--cov=config', '--cov=features', '--cov=main',
            '--cov-report=xml:coverage-unit.xml',
            '--cov-report=term-missing',
            '--junitxml=junit-unit.xml',
            '-v', '--tb=short'
        ]
        
        result = self._run_test_command(cmd, 'unit')
        self.results['test_suites']['unit'] = result
    
    def run_integration_tests(self):
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        cmd = [
            'python3', '-m', 'pytest',
            str(self.test_dir),
            '-m', 'integration',
            '--junitxml=junit-integration.xml',
            '-v', '--tb=short'
        ]
        
        result = self._run_test_command(cmd, 'integration')
        self.results['test_suites']['integration'] = result
    
    def run_security_tests(self):
        """è¿è¡Œå®‰å…¨æµ‹è¯•"""
        cmd = [
            'python3', '-m', 'pytest',
            str(self.test_dir),
            '-m', 'security',
            '--junitxml=junit-security.xml',
            '-v', '--tb=short'
        ]
        
        result = self._run_test_command(cmd, 'security')
        self.results['test_suites']['security'] = result
        
        # é¢å¤–è¿è¡Œbanditå®‰å…¨æ‰«æ
        self._run_bandit_scan()
    
    def run_performance_tests(self):
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        cmd = [
            'python3', '-m', 'pytest',
            str(self.test_dir),
            '-m', 'performance',
            '--benchmark-json=benchmark-results.json',
            '--junitxml=junit-performance.xml',
            '-v', '--tb=short'
        ]
        
        result = self._run_test_command(cmd, 'performance')
        self.results['test_suites']['performance'] = result
        
        # è¿è¡Œè´Ÿè½½æµ‹è¯•
        self._run_load_tests()
    
    def run_e2e_tests(self):
        """è¿è¡ŒE2Eæµ‹è¯•"""
        cmd = [
            'python3', '-m', 'pytest',
            str(self.test_dir),
            '-m', 'e2e',
            '--junitxml=junit-e2e.xml',
            '-v', '--tb=short'
        ]
        
        result = self._run_test_command(cmd, 'e2e')
        self.results['test_suites']['e2e'] = result
    
    def _run_test_command(self, cmd: List[str], test_type: str) -> Dict[str, Any]:
        """è¿è¡Œæµ‹è¯•å‘½ä»¤"""
        start_time = time.time()
        
        try:
            # è®¾ç½®å·¥ä½œç›®å½•
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )
            
            duration = time.time() - start_time
            
            test_result = {
                'command': ' '.join(cmd),
                'return_code': result.returncode,
                'duration': duration,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'success': result.returncode == 0
            }
            
            # è§£æJUnit XMLç»“æœ
            junit_file = f"junit-{test_type}.xml"
            if os.path.exists(junit_file):
                test_result['junit_stats'] = self._parse_junit_xml(junit_file)
            
            print(f"âœ… {test_type} æµ‹è¯•å®Œæˆ - {duration:.2f}s" if test_result['success'] else f"âŒ {test_type} æµ‹è¯•å¤±è´¥")
            
            return test_result
            
        except subprocess.TimeoutExpired:
            return {
                'command': ' '.join(cmd),
                'return_code': -1,
                'duration': time.time() - start_time,
                'error': 'Timeout',
                'success': False
            }
        except Exception as e:
            return {
                'command': ' '.join(cmd),
                'return_code': -1,
                'duration': time.time() - start_time,
                'error': str(e),
                'success': False
            }
    
    def _parse_junit_xml(self, junit_file: str) -> Dict[str, Any]:
        """è§£æJUnit XMLæ–‡ä»¶"""
        try:
            tree = ET.parse(junit_file)
            root = tree.getroot()
            
            return {
                'tests': int(root.get('tests', 0)),
                'failures': int(root.get('failures', 0)),
                'errors': int(root.get('errors', 0)),
                'skipped': int(root.get('skipped', 0)),
                'time': float(root.get('time', 0.0))
            }
        except Exception as e:
            print(f"âš ï¸ è§£æJUnitæ–‡ä»¶å¤±è´¥ {junit_file}: {e}")
            return {}
    
    def _run_bandit_scan(self):
        """è¿è¡ŒBanditå®‰å…¨æ‰«æ"""
        try:
            cmd = [
                'bandit', '-r', 'api', 'config', 'features', 'main',
                '-f', 'json', '-o', 'bandit-report.json'
            ]
            
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True
            )
            
            self.results['security']['bandit'] = {
                'return_code': result.returncode,
                'success': result.returncode == 0,
                'output_file': 'bandit-report.json'
            }
            
            print("âœ… Banditå®‰å…¨æ‰«æå®Œæˆ")
            
        except FileNotFoundError:
            print("âš ï¸ Banditæœªå®‰è£…ï¼Œè·³è¿‡å®‰å…¨æ‰«æ")
    
    def _run_load_tests(self):
        """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
        load_test_file = self.test_dir / 'load_test_auth_api.py'
        
        if not load_test_file.exists():
            print("âš ï¸ è´Ÿè½½æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡è´Ÿè½½æµ‹è¯•")
            return
        
        try:
            cmd = ['python', str(load_test_file)]
            
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            self.results['performance']['load_test'] = {
                'return_code': result.returncode,
                'success': result.returncode == 0,
                'stdout': result.stdout,
                'stderr': result.stderr
            }
            
            print("âœ… è´Ÿè½½æµ‹è¯•å®Œæˆ" if result.returncode == 0 else "âŒ è´Ÿè½½æµ‹è¯•å¤±è´¥")
            
        except subprocess.TimeoutExpired:
            print("âš ï¸ è´Ÿè½½æµ‹è¯•è¶…æ—¶")
        except Exception as e:
            print(f"âš ï¸ è´Ÿè½½æµ‹è¯•é”™è¯¯: {e}")
    
    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        self.results['end_time'] = datetime.now().isoformat()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        summary = {
            'total_tests': 0,
            'total_passed': 0,
            'total_failed': 0,
            'total_skipped': 0,
            'total_duration': 0,
            'success_rate': 0,
            'test_suites_run': 0
        }
        
        for suite_name, suite_result in self.results['test_suites'].items():
            if suite_result.get('success') and 'junit_stats' in suite_result:
                stats = suite_result['junit_stats']
                summary['total_tests'] += stats.get('tests', 0)
                summary['total_failed'] += stats.get('failures', 0) + stats.get('errors', 0)
                summary['total_skipped'] += stats.get('skipped', 0)
                summary['total_duration'] += suite_result.get('duration', 0)
                summary['test_suites_run'] += 1
        
        summary['total_passed'] = summary['total_tests'] - summary['total_failed'] - summary['total_skipped']
        
        if summary['total_tests'] > 0:
            summary['success_rate'] = (summary['total_passed'] / summary['total_tests']) * 100
        
        self.results['summary'] = summary
        
        # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        self._save_results()
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report()
        
        # æ‰“å°ç®€è¦ç»Ÿè®¡
        self._print_summary()
    
    def _save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"comprehensive_test_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    def _generate_markdown_report(self):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"comprehensive_test_report_{timestamp}.md"
        
        summary = self.results['summary']
        
        report = f"""
# Perfect21 ç»¼åˆæµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ‘˜è¦

- **å¼€å§‹æ—¶é—´**: {self.results['start_time']}
- **ç»“æŸæ—¶é—´**: {self.results['end_time']}
- **æ€»è€—æ—¶**: {summary['total_duration']:.2f}ç§’
- **æµ‹è¯•å¥—ä»¶æ•°**: {summary['test_suites_run']}

## æµ‹è¯•ç»Ÿè®¡

- **æ€»æµ‹è¯•æ•°**: {summary['total_tests']}
- **é€šè¿‡**: {summary['total_passed']} âœ…
- **å¤±è´¥**: {summary['total_failed']} âŒ
- **è·³è¿‡**: {summary['total_skipped']} â­ï¸
- **æˆåŠŸç‡**: {summary['success_rate']:.2f}%

## æµ‹è¯•å¥—ä»¶ç»“æœ

"""
        
        # æ·»åŠ å„ä¸ªæµ‹è¯•å¥—ä»¶çš„ç»“æœ
        for suite_name, suite_result in self.results['test_suites'].items():
            status = "âœ… é€šè¿‡" if suite_result.get('success') else "âŒ å¤±è´¥"
            duration = suite_result.get('duration', 0)
            
            report += f"### {suite_name.title()} æµ‹è¯• {status}\n\n"
            report += f"- **æ‰§è¡Œæ—¶é—´**: {duration:.2f}ç§’\n"
            
            if 'junit_stats' in suite_result:
                stats = suite_result['junit_stats']
                report += f"- **æµ‹è¯•æ•°é‡**: {stats.get('tests', 0)}\n"
                report += f"- **é€šè¿‡**: {stats.get('tests', 0) - stats.get('failures', 0) - stats.get('errors', 0) - stats.get('skipped', 0)}\n"
                report += f"- **å¤±è´¥**: {stats.get('failures', 0)}\n"
                report += f"- **é”™è¯¯**: {stats.get('errors', 0)}\n"
                report += f"- **è·³è¿‡**: {stats.get('skipped', 0)}\n"
            
            if not suite_result.get('success') and suite_result.get('stderr'):
                report += f"\n**é”™è¯¯è¾“å‡º**:\n```\n{suite_result['stderr'][:1000]}\n```\n"
            
            report += "\n"
        
        # æ·»åŠ å®‰å…¨æ‰«æç»“æœ
        if self.results.get('security'):
            report += "## å®‰å…¨æ‰«æ\n\n"
            
            if 'bandit' in self.results['security']:
                bandit_result = self.results['security']['bandit']
                status = "âœ… é€šè¿‡" if bandit_result.get('success') else "âŒ å¤±è´¥"
                report += f"### Bandit å®‰å…¨æ‰«æ {status}\n\n"
                
                if bandit_result.get('output_file'):
                    report += f"- **æŠ¥å‘Šæ–‡ä»¶**: {bandit_result['output_file']}\n"
        
        # æ·»åŠ æ€§èƒ½æµ‹è¯•ç»“æœ
        if self.results.get('performance'):
            report += "## æ€§èƒ½æµ‹è¯•\n\n"
            
            if 'load_test' in self.results['performance']:
                load_result = self.results['performance']['load_test']
                status = "âœ… é€šè¿‡" if load_result.get('success') else "âŒ å¤±è´¥"
                report += f"### è´Ÿè½½æµ‹è¯• {status}\n\n"
        
        # æ·»åŠ å»ºè®®
        report += "\n## å»ºè®®\n\n"
        
        if summary['success_rate'] < 90:
            report += "- â— æµ‹è¯•æˆåŠŸç‡ä½äº90%ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹\n"
        
        if summary['total_failed'] > 0:
            report += "- â— å­˜åœ¨å¤±è´¥çš„æµ‹è¯•ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯\n"
        
        if summary['total_tests'] == 0:
            report += "- âš ï¸ æœªå‘ç°ä»»ä½•æµ‹è¯•ï¼Œè¯·æ£€æŸ¥æµ‹è¯•æ–‡ä»¶\n"
        
        if summary['success_rate'] >= 95 and summary['total_failed'] == 0:
            report += "- âœ… æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼Œä»£ç è´¨é‡è‰¯å¥½\n"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
    
    def _print_summary(self):
        """æ‰“å°ç®€è¦ç»Ÿè®¡"""
        summary = self.results['summary']
        
        print("\n" + "="*60)
        print("ğŸ Perfect21 æµ‹è¯•ç»¼åˆæŠ¥å‘Š")
        print("="*60)
        print(f"ğŸ“ˆ æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"âœ… é€šè¿‡: {summary['total_passed']}")
        print(f"âŒ å¤±è´¥: {summary['total_failed']}")
        print(f"â­ï¸ è·³è¿‡: {summary['total_skipped']}")
        print(f"ğŸ“Š æˆåŠŸç‡: {summary['success_rate']:.2f}%")
        print(f"â±ï¸ æ€»è€—æ—¶: {summary['total_duration']:.2f}ç§’")
        
        # æ ¹æ®ç»“æœç»™å‡ºç­‰çº§
        if summary['success_rate'] >= 95 and summary['total_failed'] == 0:
            print("ğŸ† ç­‰çº§: ä¼˜ç§€ - æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        elif summary['success_rate'] >= 85:
            print("ğŸŸ¡ ç­‰çº§: è‰¯å¥½ - å¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡")
        elif summary['success_rate'] >= 70:
            print("ğŸŸ  ç­‰çº§: ä¸€èˆ¬ - éœ€è¦æ”¹è¿›")
        else:
            print("ğŸ”´ ç­‰çº§: å·® - éœ€è¦é‡ç‚¹å…³æ³¨")
        
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Perfect21 ç»¼åˆæµ‹è¯•æ‰§è¡Œå™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python run_comprehensive_tests.py                    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
  python run_comprehensive_tests.py --types unit      # åªè¿è¡Œå•å…ƒæµ‹è¯•
  python run_comprehensive_tests.py --types unit integration security  # è¿è¡Œå¤šç§æµ‹è¯•
        """
    )
    
    parser.add_argument(
        '--types', 
        nargs='*',
        choices=['unit', 'integration', 'security', 'performance', 'e2e'],
        help='æŒ‡å®šè¦è¿è¡Œçš„æµ‹è¯•ç±»å‹'
    )
    
    parser.add_argument(
        '--test-dir',
        type=Path,
        default=Path(__file__).parent,
        help='æµ‹è¯•ç›®å½•è·¯å¾„'
    )
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨
    runner = TestRunner(args.test_dir)
    
    # è¿è¡Œæµ‹è¯•
    results = runner.run_all_tests(args.types)
    
    # è¿”å›é€€å‡ºç 
    summary = results.get('summary', {})
    if summary.get('total_failed', 0) > 0:
        sys.exit(1)  # æœ‰å¤±è´¥çš„æµ‹è¯•
    else:
        sys.exit(0)  # æ‰€æœ‰æµ‹è¯•é€šè¿‡


if __name__ == '__main__':
    main()
