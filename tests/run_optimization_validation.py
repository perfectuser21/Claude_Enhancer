#!/usr/bin/env python3
"""
Perfect21ä¼˜åŒ–éªŒè¯æµ‹è¯•è¿è¡Œå™¨
è‡ªåŠ¨åŒ–æ‰§è¡Œå®Œæ•´çš„æ€§èƒ½éªŒè¯æµ‹è¯•å¥—ä»¶å¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import os
import sys
import time
import json
import argparse
import subprocess
import yaml
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

class OptimizationTestRunner:
    """ä¼˜åŒ–æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path or "tests/performance_benchmark_config.yaml"
        self.config = self._load_config()
        self.results = {}
        self.start_time = None
        self.end_time = None

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"Warning: Config file {self.config_path} not found, using defaults")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'performance_thresholds': {
                'workflow_generation_ms': 500,
                'min_parallel_agents': 3,
                'parallel_efficiency_threshold': 0.8,
                'max_memory_usage_mb': 100,
                'max_cpu_usage_percent': 80
            }
        }

    def run_all_tests(self, test_filter: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰ä¼˜åŒ–éªŒè¯æµ‹è¯•"""
        print("ğŸš€ Starting Perfect21 Optimization Validation Tests")
        print("=" * 60)

        self.start_time = time.time()

        test_suites = [
            ('parallel_execution', 'TestParallelExecution'),
            ('performance_benchmarks', 'TestPerformanceBenchmarks'),
            ('agent_selection', 'TestAgentSelectionAccuracy'),
            ('resource_monitoring', 'TestResourceMonitoring'),
            ('integration_scenarios', 'TestIntegrationScenarios')
        ]

        for suite_name, test_class in test_suites:
            if test_filter and test_filter not in suite_name:
                continue

            print(f"\nğŸ“‹ Running {suite_name} tests...")
            try:
                result = self._run_test_suite(test_class)
                self.results[suite_name] = result
                self._print_suite_summary(suite_name, result)
            except Exception as e:
                print(f"âŒ Error running {suite_name}: {str(e)}")
                self.results[suite_name] = {
                    'status': 'error',
                    'error': str(e),
                    'tests_run': 0,
                    'tests_passed': 0,
                    'tests_failed': 1
                }

        self.end_time = time.time()

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_comprehensive_report()

        return self.results

    def _run_test_suite(self, test_class: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¥—ä»¶"""
        cmd = [
            sys.executable, "-m", "pytest",
            f"tests/test_perfect21_optimization_validation.py::{test_class}",
            "-v",
            "--tb=short"
        ]

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
                cwd=Path(__file__).parent.parent
            )

            # è§£æpytestçš„æ ‡å‡†è¾“å‡º
            return self._parse_stdout_results(result)

        except subprocess.TimeoutExpired:
            return {
                'status': 'timeout',
                'tests_run': 0,
                'tests_passed': 0,
                'tests_failed': 1,
                'duration': 300
            }
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e),
                'tests_run': 0,
                'tests_passed': 0,
                'tests_failed': 1
            }

    def _parse_pytest_results(self, json_file: str, result: subprocess.CompletedProcess) -> Dict[str, Any]:
        """è§£æpytestç»“æœ"""
        try:
            if os.path.exists(json_file):
                with open(json_file, 'r') as f:
                    json_data = json.load(f)

                return {
                    'status': 'passed' if result.returncode == 0 else 'failed',
                    'tests_run': json_data.get('summary', {}).get('total', 0),
                    'tests_passed': json_data.get('summary', {}).get('passed', 0),
                    'tests_failed': json_data.get('summary', {}).get('failed', 0),
                    'tests_skipped': json_data.get('summary', {}).get('skipped', 0),
                    'duration': json_data.get('duration', 0),
                    'stdout': result.stdout,
                    'stderr': result.stderr
                }
            else:
                # å¦‚æœæ²¡æœ‰JSONæ–‡ä»¶ï¼Œä»æ ‡å‡†è¾“å‡ºè§£æ
                return self._parse_stdout_results(result)

        except Exception as e:
            return {
                'status': 'error',
                'error': f"Failed to parse results: {str(e)}",
                'tests_run': 0,
                'tests_passed': 0,
                'tests_failed': 1,
                'stdout': result.stdout,
                'stderr': result.stderr
            }
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(json_file):
                os.remove(json_file)

    def _parse_stdout_results(self, result: subprocess.CompletedProcess) -> Dict[str, Any]:
        """ä»æ ‡å‡†è¾“å‡ºè§£ææµ‹è¯•ç»“æœ"""
        stdout = result.stdout

        # ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼è§£æ
        import re

        # æŸ¥æ‰¾æµ‹è¯•ç»“æœæ‘˜è¦
        summary_pattern = r'(\d+) passed.*?in ([\d.]+)s'
        match = re.search(summary_pattern, stdout)

        if match:
            passed = int(match.group(1))
            duration = float(match.group(2))

            return {
                'status': 'passed' if result.returncode == 0 else 'failed',
                'tests_run': passed,
                'tests_passed': passed if result.returncode == 0 else 0,
                'tests_failed': 0 if result.returncode == 0 else passed,
                'duration': duration,
                'stdout': stdout,
                'stderr': result.stderr
            }

        return {
            'status': 'unknown',
            'tests_run': 0,
            'tests_passed': 0,
            'tests_failed': 1,
            'stdout': stdout,
            'stderr': result.stderr
        }

    def _print_suite_summary(self, suite_name: str, result: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•å¥—ä»¶æ‘˜è¦"""
        status = result.get('status', 'unknown')
        passed = result.get('tests_passed', 0)
        failed = result.get('tests_failed', 0)
        duration = result.get('duration', 0)

        status_emoji = {
            'passed': 'âœ…',
            'failed': 'âŒ',
            'error': 'ğŸ’¥',
            'timeout': 'â°',
            'unknown': 'â“'
        }.get(status, 'â“')

        print(f"{status_emoji} {suite_name}: {passed} passed, {failed} failed ({duration:.2f}s)")

        # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        if result.get('error'):
            print(f"   Error: {result['error']}")

        # æ˜¾ç¤ºå…³é”®è¾“å‡º
        if result.get('stdout') and 'âœ…' in result.get('stdout', ''):
            for line in result['stdout'].split('\n'):
                if 'âœ…' in line:
                    print(f"   {line.strip()}")

    def _generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        report_data = {
            'test_run_info': {
                'timestamp': datetime.now().isoformat(),
                'duration': self.end_time - self.start_time if self.end_time and self.start_time else 0,
                'python_version': sys.version,
                'config_file': self.config_path
            },
            'summary': self._calculate_summary(),
            'detailed_results': self.results,
            'performance_analysis': self._analyze_performance(),
            'recommendations': self._generate_recommendations()
        }

        # ä¿å­˜JSONæŠ¥å‘Š
        json_report_path = f"tests/optimization_validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_report_path, 'w') as f:
            json.dump(report_data, f, indent=2)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        markdown_report = self._generate_markdown_report(report_data)
        md_report_path = json_report_path.replace('.json', '.md')
        with open(md_report_path, 'w') as f:
            f.write(markdown_report)

        print(f"\nğŸ“Š Reports generated:")
        print(f"   - JSON: {json_report_path}")
        print(f"   - Markdown: {md_report_path}")

        # æ‰“å°æ€»ç»“
        self._print_final_summary(report_data)

    def _calculate_summary(self) -> Dict[str, Any]:
        """è®¡ç®—æµ‹è¯•æ‘˜è¦"""
        total_tests = sum(r.get('tests_run', 0) for r in self.results.values())
        total_passed = sum(r.get('tests_passed', 0) for r in self.results.values())
        total_failed = sum(r.get('tests_failed', 0) for r in self.results.values())
        total_duration = sum(r.get('duration', 0) for r in self.results.values())

        return {
            'total_test_suites': len(self.results),
            'total_tests': total_tests,
            'total_passed': total_passed,
            'total_failed': total_failed,
            'total_skipped': sum(r.get('tests_skipped', 0) for r in self.results.values()),
            'success_rate': (total_passed / total_tests * 100) if total_tests > 0 else 0,
            'total_duration': total_duration,
            'overall_status': 'PASSED' if total_failed == 0 else 'FAILED'
        }

    def _analyze_performance(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æµ‹è¯•ç»“æœ"""
        analysis = {
            'parallel_execution': 'Not tested',
            'performance_benchmarks': 'Not tested',
            'resource_efficiency': 'Not tested',
            'agent_selection_accuracy': 'Not tested'
        }

        # åˆ†æå¹¶è¡Œæ‰§è¡Œç»“æœ
        if 'parallel_execution' in self.results:
            pe_result = self.results['parallel_execution']
            if pe_result.get('status') == 'passed':
                analysis['parallel_execution'] = 'PASSED - Parallel execution capabilities verified'
            else:
                analysis['parallel_execution'] = f"FAILED - {pe_result.get('error', 'Unknown error')}"

        # åˆ†ææ€§èƒ½åŸºå‡†ç»“æœ
        if 'performance_benchmarks' in self.results:
            pb_result = self.results['performance_benchmarks']
            if pb_result.get('status') == 'passed':
                analysis['performance_benchmarks'] = 'PASSED - Performance thresholds met'
            else:
                analysis['performance_benchmarks'] = f"FAILED - Performance issues detected"

        # åˆ†æèµ„æºç›‘æ§ç»“æœ
        if 'resource_monitoring' in self.results:
            rm_result = self.results['resource_monitoring']
            if rm_result.get('status') == 'passed':
                analysis['resource_efficiency'] = 'PASSED - Resource usage within limits'
            else:
                analysis['resource_efficiency'] = f"FAILED - Resource usage exceeded limits"

        # åˆ†æagenté€‰æ‹©å‡†ç¡®æ€§
        if 'agent_selection' in self.results:
            as_result = self.results['agent_selection']
            if as_result.get('status') == 'passed':
                analysis['agent_selection_accuracy'] = 'PASSED - Agent selection accuracy acceptable'
            else:
                analysis['agent_selection_accuracy'] = f"FAILED - Agent selection needs improvement"

        return analysis

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        for suite_name, result in self.results.items():
            if result.get('status') != 'passed':
                if suite_name == 'parallel_execution':
                    recommendations.append("ä¼˜åŒ–å¹¶è¡Œæ‰§è¡Œæœºåˆ¶ï¼Œæ£€æŸ¥çº¿ç¨‹æ± é…ç½®å’Œä»»åŠ¡è°ƒåº¦ç®—æ³•")
                elif suite_name == 'performance_benchmarks':
                    recommendations.append("ä¼˜åŒ–æ€§èƒ½ç“¶é¢ˆï¼Œè€ƒè™‘ç¼“å­˜ç­–ç•¥å’Œç®—æ³•ä¼˜åŒ–")
                elif suite_name == 'resource_monitoring':
                    recommendations.append("ä¼˜åŒ–èµ„æºä½¿ç”¨ï¼Œæ£€æŸ¥å†…å­˜æ³„æ¼å’ŒCPUå¯†é›†å‹æ“ä½œ")
                elif suite_name == 'agent_selection':
                    recommendations.append("æ”¹è¿›agenté€‰æ‹©ç®—æ³•ï¼Œä¼˜åŒ–å…³é”®è¯åŒ¹é…å’Œä¸Šä¸‹æ–‡åˆ†æ")

        if not recommendations:
            recommendations.append("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½")

        return recommendations

    def _generate_markdown_report(self, report_data: Dict[str, Any]) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        md = f"""# Perfect21 ä¼˜åŒ–éªŒè¯æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ‘˜è¦

- **æµ‹è¯•æ—¶é—´**: {report_data['test_run_info']['timestamp']}
- **æ€»æ‰§è¡Œæ—¶é—´**: {report_data['test_run_info']['duration']:.2f}ç§’
- **æµ‹è¯•å¥—ä»¶æ•°**: {report_data['summary']['total_test_suites']}
- **æ€»æµ‹è¯•æ•°**: {report_data['summary']['total_tests']}
- **é€šè¿‡æ•°**: {report_data['summary']['total_passed']}
- **å¤±è´¥æ•°**: {report_data['summary']['total_failed']}
- **æˆåŠŸç‡**: {report_data['summary']['success_rate']:.2f}%
- **æ•´ä½“çŠ¶æ€**: {report_data['summary']['overall_status']}

## è¯¦ç»†ç»“æœ

"""

        # æ·»åŠ æ¯ä¸ªæµ‹è¯•å¥—ä»¶çš„è¯¦ç»†ç»“æœ
        for suite_name, result in report_data['detailed_results'].items():
            status_emoji = 'âœ…' if result.get('status') == 'passed' else 'âŒ'
            md += f"### {status_emoji} {suite_name.replace('_', ' ').title()}\n\n"
            md += f"- **çŠ¶æ€**: {result.get('status', 'unknown')}\n"
            md += f"- **æµ‹è¯•æ•°**: {result.get('tests_run', 0)}\n"
            md += f"- **é€šè¿‡æ•°**: {result.get('tests_passed', 0)}\n"
            md += f"- **å¤±è´¥æ•°**: {result.get('tests_failed', 0)}\n"
            md += f"- **æ‰§è¡Œæ—¶é—´**: {result.get('duration', 0):.2f}ç§’\n\n"

            if result.get('error'):
                md += f"**é”™è¯¯ä¿¡æ¯**: {result['error']}\n\n"

        # æ·»åŠ æ€§èƒ½åˆ†æ
        md += "## æ€§èƒ½åˆ†æ\n\n"
        for metric, status in report_data['performance_analysis'].items():
            status_emoji = 'âœ…' if 'PASSED' in status else 'âŒ' if 'FAILED' in status else 'â¸ï¸'
            md += f"- {status_emoji} **{metric.replace('_', ' ').title()}**: {status}\n"

        # æ·»åŠ æ”¹è¿›å»ºè®®
        md += "\n## æ”¹è¿›å»ºè®®\n\n"
        for i, recommendation in enumerate(report_data['recommendations'], 1):
            md += f"{i}. {recommendation}\n"

        md += f"\n---\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"

        return md

    def _print_final_summary(self, report_data: Dict[str, Any]):
        """æ‰“å°æœ€ç»ˆæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“‹ PERFECT21 OPTIMIZATION VALIDATION SUMMARY")
        print("="*60)

        summary = report_data['summary']
        status_emoji = 'âœ…' if summary['overall_status'] == 'PASSED' else 'âŒ'

        print(f"{status_emoji} Overall Status: {summary['overall_status']}")
        print(f"ğŸ“Š Tests: {summary['total_passed']}/{summary['total_tests']} passed ({summary['success_rate']:.1f}%)")
        print(f"â±ï¸  Duration: {summary['total_duration']:.2f}s")

        print("\nğŸ¯ Key Performance Indicators:")
        for metric, status in report_data['performance_analysis'].items():
            status_emoji = 'âœ…' if 'PASSED' in status else 'âŒ' if 'FAILED' in status else 'â¸ï¸'
            metric_name = metric.replace('_', ' ').title()
            print(f"   {status_emoji} {metric_name}")

        if report_data['recommendations']:
            print("\nğŸ’¡ Recommendations:")
            for rec in report_data['recommendations'][:3]:  # æ˜¾ç¤ºå‰3ä¸ªå»ºè®®
                print(f"   â€¢ {rec}")

        print("\n" + "="*60)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Run Perfect21 optimization validation tests')
    parser.add_argument('--config', '-c', help='Configuration file path')
    parser.add_argument('--filter', '-f', help='Filter tests by name')
    parser.add_argument('--quiet', '-q', action='store_true', help='Reduce output verbosity')

    args = parser.parse_args()

    runner = OptimizationTestRunner(config_path=args.config)

    try:
        results = runner.run_all_tests(test_filter=args.filter)

        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        overall_status = runner._calculate_summary()['overall_status']
        sys.exit(0 if overall_status == 'PASSED' else 1)

    except KeyboardInterrupt:
        print("\nâš ï¸ Tests interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {str(e)}")
        sys.exit(1)

if __name__ == '__main__':
    main()