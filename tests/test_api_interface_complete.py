#!/usr/bin/env python3
"""
Perfect21 API Interface Complete Test
å®Œæ•´æµ‹è¯•Perfect21çš„æ‰€æœ‰APIæ¥å£å’Œå¤šAgentåä½œèƒ½åŠ›

æµ‹è¯•ç›®æ ‡ï¼š
- éªŒè¯CLIæ¥å£å®Œæ•´æ€§
- æµ‹è¯•orchestrator_gatewayåŠŸèƒ½
- éªŒè¯auto_capability_injectionæœºåˆ¶
- æµ‹è¯•å¤šAgentå¹¶è¡Œåè°ƒ
- ç¡®è®¤ç³»ç»Ÿç”Ÿäº§å°±ç»ªçŠ¶æ€
"""

import os
import sys
import json
import time
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(__file__))

class Perfect21APITestSuite:
    """Perfect21 APIæ¥å£å®Œæ•´æµ‹è¯•å¥—ä»¶"""

    def __init__(self):
        self.project_root = Path(__file__).parent
        self.test_results = {
            'test_session': {
                'timestamp': datetime.now().isoformat(),
                'test_type': 'API_Complete_Test',
                'environment': {
                    'python_version': sys.version,
                    'project_path': str(self.project_root)
                }
            },
            'api_tests': {},
            'integration_tests': {},
            'performance_metrics': {},
            'agent_coordination': {}
        }
        print("ğŸš€ Perfect21 API Complete Test Suite - åˆå§‹åŒ–")

    def test_cli_status_command(self):
        """æµ‹è¯•CLI statuså‘½ä»¤"""
        print("ğŸ“‹ æµ‹è¯•CLI statuså‘½ä»¤...")

        try:
            # æµ‹è¯•CLI statuså‘½ä»¤
            start_time = time.time()
            result = subprocess.run(['python3', 'main/cli.py', 'status'],
                                  cwd=self.project_root,
                                  capture_output=True,
                                  text=True,
                                  timeout=30)

            execution_time = time.time() - start_time

            cli_test = {
                'command': 'main/cli.py status',
                'return_code': result.returncode,
                'execution_time': execution_time,
                'stdout_length': len(result.stdout) if result.stdout else 0,
                'stderr_length': len(result.stderr) if result.stderr else 0,
                'success': result.returncode == 0,
                'output_contains_perfect21': 'Perfect21' in result.stdout if result.stdout else False
            }

            if result.stdout:
                cli_test['output_preview'] = result.stdout[:300] + '...' if len(result.stdout) > 300 else result.stdout

            if result.stderr:
                cli_test['error_preview'] = result.stderr[:200] + '...' if len(result.stderr) > 200 else result.stderr

            self.test_results['api_tests']['cli_status'] = cli_test

            print(f"âœ… CLI statuså‘½ä»¤æµ‹è¯•å®Œæˆ - è¿”å›ç : {result.returncode}, æ—¶é—´: {execution_time:.3f}ç§’")
            return result.returncode == 0

        except subprocess.TimeoutExpired:
            self.test_results['api_tests']['cli_status'] = {
                'success': False,
                'error': 'Command timeout after 30 seconds',
                'timeout': True
            }
            print("âŒ CLI statuså‘½ä»¤è¶…æ—¶")
            return False
        except Exception as e:
            self.test_results['api_tests']['cli_status'] = {
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__
            }
            print(f"âŒ CLI statuså‘½ä»¤æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_orchestrator_gateway_api(self):
        """æµ‹è¯•orchestrator_gateway API"""
        print("ğŸ¯ æµ‹è¯•orchestrator_gateway API...")

        try:
            from main.orchestrator_gateway import OrchestratorGateway

            # æµ‹è¯•ç½‘å…³åˆå§‹åŒ–
            start_time = time.time()
            gateway = OrchestratorGateway()
            init_time = time.time() - start_time

            # æµ‹è¯•å¯¹è¯åŠŸèƒ½
            test_request = "æµ‹è¯•Perfect21ç³»ç»Ÿå®Œæ•´åŠŸèƒ½ï¼Œå±•ç¤ºå¤šAgentåä½œèƒ½åŠ›"
            start_time = time.time()
            orchestrator_call = gateway.talk_to_orchestrator(test_request)
            call_time = time.time() - start_time

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = gateway.get_stats()

            gateway_test = {
                'init_time': init_time,
                'call_time': call_time,
                'orchestrator_call_generated': isinstance(orchestrator_call, str) and len(orchestrator_call) > 0,
                'call_content_length': len(orchestrator_call) if isinstance(orchestrator_call, str) else 0,
                'contains_perfect21_context': 'Perfect21' in orchestrator_call if isinstance(orchestrator_call, str) else False,
                'contains_capability_info': 'capability' in orchestrator_call.lower() if isinstance(orchestrator_call, str) else False,
                'gateway_stats': {
                    'conversations': stats.get('gateway_conversations', 0),
                    'capabilities_available': stats.get('perfect21_capabilities', {})
                },
                'success': True
            }

            # éªŒè¯è°ƒç”¨å†…å®¹è´¨é‡
            if isinstance(orchestrator_call, str):
                gateway_test['content_quality'] = {
                    'has_user_request': test_request in orchestrator_call,
                    'has_instructions': 'ä½ ç°åœ¨æ˜¯@orchestrator' in orchestrator_call,
                    'has_capabilities': 'Perfect21å¹³å°' in orchestrator_call,
                    'length_sufficient': len(orchestrator_call) > 1000
                }

            self.test_results['api_tests']['orchestrator_gateway'] = gateway_test

            print(f"âœ… orchestrator_gateway APIæµ‹è¯•å®Œæˆ - ç”Ÿæˆå†…å®¹: {len(orchestrator_call) if isinstance(orchestrator_call, str) else 0}å­—ç¬¦")
            return True

        except Exception as e:
            self.test_results['api_tests']['orchestrator_gateway'] = {
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__
            }
            print(f"âŒ orchestrator_gateway APIæµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_capability_injection_api(self):
        """æµ‹è¯•capability_injection API"""
        print("ğŸ’‰ æµ‹è¯•capability_injection API...")

        try:
            from features.auto_capability_injection import (
                get_global_injector,
                auto_inject_and_call,
                get_direct_orchestrator_call
            )

            # æµ‹è¯•å…¨å±€æ³¨å…¥å™¨
            start_time = time.time()
            injector = get_global_injector()
            init_time = time.time() - start_time

            # æµ‹è¯•è‡ªåŠ¨æ³¨å…¥å’Œè°ƒç”¨
            test_request = "åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„Webåº”ç”¨ï¼ŒåŒ…å«å‰ç«¯ã€åç«¯å’Œæ•°æ®åº“"
            start_time = time.time()
            injection_result = auto_inject_and_call(test_request)
            injection_time = time.time() - start_time

            # æµ‹è¯•ç›´æ¥è°ƒç”¨ç”Ÿæˆ
            start_time = time.time()
            direct_call = get_direct_orchestrator_call(test_request)
            direct_call_time = time.time() - start_time

            # è·å–æ³¨å…¥ç»Ÿè®¡
            stats = injector.get_injection_stats()

            injection_test = {
                'injector_init_time': init_time,
                'injection_time': injection_time,
                'direct_call_time': direct_call_time,
                'injection_success': injection_result.get('success', False),
                'injection_stats': injection_result.get('injection_stats', {}),
                'direct_call_length': len(direct_call) if isinstance(direct_call, str) else 0,
                'capability_stats': {
                    'total_injections': stats.get('total_injections', 0),
                    'capabilities_available': stats.get('capabilities_available', {}),
                    'templates_count': stats.get('capabilities_available', {}).get('templates', 0),
                    'agents_count': stats.get('capabilities_available', {}).get('agents', 0)
                },
                'success': injection_result.get('success', False)
            }

            # éªŒè¯æ³¨å…¥å†…å®¹è´¨é‡
            if injection_result.get('orchestrator_context'):
                context = injection_result['orchestrator_context']
                injection_test['context_quality'] = {
                    'has_user_request': test_request in context,
                    'has_capabilities_briefing': 'Perfect21å¹³å°' in context,
                    'has_agent_info': 'Agent' in context,
                    'has_execution_plan': 'æ‰§è¡Œè®¡åˆ’' in context
                }

            self.test_results['api_tests']['capability_injection'] = injection_test

            print(f"âœ… capability_injection APIæµ‹è¯•å®Œæˆ - æ¨¡æ¿æ•°: {injection_test['capability_stats']['templates_count']}")
            return injection_result.get('success', False)

        except Exception as e:
            self.test_results['api_tests']['capability_injection'] = {
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__
            }
            print(f"âŒ capability_injection APIæµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_git_hooks_api(self):
        """æµ‹è¯•Git hooks API"""
        print("ğŸ”„ æµ‹è¯•Git hooks API...")

        try:
            from features.git_workflow.hooks_manager import GitHooksManager
            from main.perfect21 import Perfect21

            # æµ‹è¯•hooksç®¡ç†å™¨
            start_time = time.time()
            hooks_manager = GitHooksManager()
            manager_init_time = time.time() - start_time

            # æµ‹è¯•Perfect21 Gité’©å­å¤„ç†
            start_time = time.time()
            p21 = Perfect21()
            p21_init_time = time.time() - start_time

            # æµ‹è¯•é’©å­æ‰§è¡Œ
            hook_tests = {}
            test_hooks = ['pre-commit', 'pre-push', 'post-checkout']

            for hook_name in test_hooks:
                start_time = time.time()
                result = p21.git_hook_handler(hook_name)
                execution_time = time.time() - start_time

                hook_tests[hook_name] = {
                    'execution_time': execution_time,
                    'success': result.get('success', False),
                    'agent_called': 'call_info' in result,
                    'agent_name': result.get('call_info', {}).get('agent_name', ''),
                    'task_ready': result.get('task_call_ready', False)
                }

            hooks_test = {
                'manager_init_time': manager_init_time,
                'p21_init_time': p21_init_time,
                'total_hooks_available': len(hooks_manager.hooks_config),
                'hook_groups': list(hooks_manager.hook_groups.keys()),
                'hook_execution_tests': hook_tests,
                'average_hook_time': sum(h['execution_time'] for h in hook_tests.values()) / len(hook_tests),
                'success': True
            }

            # éªŒè¯SubAgentæ˜ å°„
            subagent_mappings = {}
            for hook_name, config in hooks_manager.hooks_config.items():
                if 'subagent' in config:
                    subagent_mappings[hook_name] = config['subagent']

            hooks_test['subagent_mappings'] = subagent_mappings
            hooks_test['subagent_coverage'] = len(subagent_mappings)

            self.test_results['api_tests']['git_hooks'] = hooks_test

            print(f"âœ… Git hooks APIæµ‹è¯•å®Œæˆ - å¯ç”¨é’©å­: {len(hooks_manager.hooks_config)}ä¸ª")
            return True

        except Exception as e:
            self.test_results['api_tests']['git_hooks'] = {
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__
            }
            print(f"âŒ Git hooks APIæµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_multi_agent_coordination_scenarios(self):
        """æµ‹è¯•å¤šAgentåè°ƒåœºæ™¯"""
        print("ğŸ¤ æµ‹è¯•å¤šAgentåè°ƒåœºæ™¯...")

        try:
            coordination_scenarios = {
                'web_development': {
                    'description': 'å¼€å‘å®Œæ•´çš„Webåº”ç”¨',
                    'expected_agents': ['@backend-architect', '@frontend-developer', '@database-designer'],
                    'coordination_type': 'parallel'
                },
                'security_audit': {
                    'description': 'æ‰§è¡Œå®‰å…¨å®¡è®¡å’Œæ¼æ´ä¿®å¤',
                    'expected_agents': ['@security-auditor', '@code-reviewer'],
                    'coordination_type': 'sequential'
                },
                'performance_optimization': {
                    'description': 'æ€§èƒ½åˆ†æå’Œç³»ç»Ÿä¼˜åŒ–',
                    'expected_agents': ['@performance-engineer', '@devops-engineer'],
                    'coordination_type': 'coordinated'
                },
                'code_review_workflow': {
                    'description': 'ä»£ç å®¡æŸ¥å’Œè´¨é‡ä¿è¯',
                    'expected_agents': ['@orchestrator', '@code-reviewer', '@test-engineer'],
                    'coordination_type': 'orchestrated'
                }
            }

            from features.auto_capability_injection import get_global_injector

            injector = get_global_injector()
            scenario_results = {}

            for scenario_name, scenario_config in coordination_scenarios.items():
                start_time = time.time()

                # ä¸ºæ¯ä¸ªåœºæ™¯ç”Ÿæˆorchestratorè°ƒç”¨
                orchestrator_context = injector.generate_orchestrator_context(scenario_config['description'])

                execution_time = time.time() - start_time

                scenario_results[scenario_name] = {
                    'description': scenario_config['description'],
                    'expected_agents': scenario_config['expected_agents'],
                    'coordination_type': scenario_config['coordination_type'],
                    'context_generation_time': execution_time,
                    'context_length': len(orchestrator_context),
                    'context_contains_agents': any(agent in orchestrator_context for agent in scenario_config['expected_agents']),
                    'context_contains_perfect21': 'Perfect21' in orchestrator_context,
                    'success': True
                }

            coordination_test = {
                'scenarios_tested': len(coordination_scenarios),
                'scenarios_results': scenario_results,
                'total_expected_agents': sum(len(s['expected_agents']) for s in coordination_scenarios.values()),
                'average_context_generation_time': sum(r['context_generation_time'] for r in scenario_results.values()) / len(scenario_results),
                'all_scenarios_success': all(r['success'] for r in scenario_results.values()),
                'success': True
            }

            self.test_results['agent_coordination']['multi_agent_scenarios'] = coordination_test

            print(f"âœ… å¤šAgentåè°ƒæµ‹è¯•å®Œæˆ - æµ‹è¯•{len(coordination_scenarios)}ä¸ªåœºæ™¯")
            return True

        except Exception as e:
            self.test_results['agent_coordination']['multi_agent_scenarios'] = {
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__
            }
            print(f"âŒ å¤šAgentåè°ƒæµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_system_integration(self):
        """æµ‹è¯•ç³»ç»Ÿé›†æˆ"""
        print("ğŸ”— æµ‹è¯•ç³»ç»Ÿé›†æˆ...")

        try:
            # æµ‹è¯•å®Œæ•´çš„å·¥ä½œæµï¼šCLI -> Gateway -> Injection -> Hook
            integration_workflow = {
                'step1_cli_available': False,
                'step2_gateway_functional': False,
                'step3_injection_working': False,
                'step4_hooks_responsive': False,
                'step5_agents_accessible': False
            }

            # Step 1: CLIå¯ç”¨æ€§
            cli_file = self.project_root / 'main' / 'cli.py'
            integration_workflow['step1_cli_available'] = cli_file.exists()

            # Step 2: GatewayåŠŸèƒ½
            try:
                from main.orchestrator_gateway import OrchestratorGateway
                gateway = OrchestratorGateway()
                test_call = gateway.talk_to_orchestrator("ç³»ç»Ÿé›†æˆæµ‹è¯•")
                integration_workflow['step2_gateway_functional'] = isinstance(test_call, str) and len(test_call) > 0
            except Exception as e:
                integration_workflow['step2_gateway_error'] = str(e)

            # Step 3: èƒ½åŠ›æ³¨å…¥
            try:
                from features.auto_capability_injection import auto_inject_and_call
                injection_result = auto_inject_and_call("ç³»ç»Ÿé›†æˆæµ‹è¯•")
                integration_workflow['step3_injection_working'] = injection_result.get('success', False)
            except Exception as e:
                integration_workflow['step3_injection_error'] = str(e)

            # Step 4: Gité’©å­å“åº”
            try:
                from main.perfect21 import Perfect21
                p21 = Perfect21()
                hook_result = p21.git_hook_handler('pre-commit')
                integration_workflow['step4_hooks_responsive'] = hook_result.get('success', False)
            except Exception as e:
                integration_workflow['step4_hooks_error'] = str(e)

            # Step 5: Agentå¯è®¿é—®æ€§
            try:
                agents_dir = self.project_root / 'core' / 'claude-code-unified-agents' / '.claude' / 'agents'
                agent_files = list(agents_dir.rglob('*.md')) if agents_dir.exists() else []
                integration_workflow['step5_agents_accessible'] = len(agent_files) > 50
                integration_workflow['agents_found'] = len(agent_files)
            except Exception as e:
                integration_workflow['step5_agents_error'] = str(e)

            # è®¡ç®—é›†æˆåˆ†æ•°
            successful_steps = sum(1 for step, result in integration_workflow.items()
                                 if step.startswith('step') and result is True)
            total_steps = 5
            integration_score = (successful_steps / total_steps) * 100

            integration_test = {
                'workflow_steps': integration_workflow,
                'successful_steps': successful_steps,
                'total_steps': total_steps,
                'integration_score': integration_score,
                'system_ready': integration_score >= 80,
                'success': integration_score == 100
            }

            self.test_results['integration_tests']['system_integration'] = integration_test

            print(f"âœ… ç³»ç»Ÿé›†æˆæµ‹è¯•å®Œæˆ - é›†æˆåˆ†æ•°: {integration_score:.1f}% ({successful_steps}/{total_steps})")
            return integration_score >= 80

        except Exception as e:
            self.test_results['integration_tests']['system_integration'] = {
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__
            }
            print(f"âŒ ç³»ç»Ÿé›†æˆæµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_performance_benchmarks(self):
        """æµ‹è¯•æ€§èƒ½åŸºå‡†"""
        print("âš¡ æµ‹è¯•æ€§èƒ½åŸºå‡†...")

        try:
            import time
            import threading
            from concurrent.futures import ThreadPoolExecutor, as_completed

            performance_metrics = {
                'startup_times': {},
                'api_response_times': {},
                'concurrent_performance': {},
                'memory_usage': {}
            }

            # æµ‹è¯•å¯åŠ¨æ—¶é—´
            component_startup_times = {}

            # Perfect21å¯åŠ¨æ—¶é—´
            start_time = time.time()
            from main.perfect21 import Perfect21
            p21 = Perfect21()
            component_startup_times['perfect21'] = time.time() - start_time

            # Gatewayå¯åŠ¨æ—¶é—´
            start_time = time.time()
            from main.orchestrator_gateway import OrchestratorGateway
            gateway = OrchestratorGateway()
            component_startup_times['orchestrator_gateway'] = time.time() - start_time

            # Injectorå¯åŠ¨æ—¶é—´
            start_time = time.time()
            from features.auto_capability_injection import get_global_injector
            injector = get_global_injector()
            component_startup_times['capability_injector'] = time.time() - start_time

            performance_metrics['startup_times'] = component_startup_times

            # æµ‹è¯•APIå“åº”æ—¶é—´
            api_response_times = {}

            # Gité’©å­å“åº”æ—¶é—´
            start_time = time.time()
            p21.git_hook_handler('pre-commit')
            api_response_times['git_hook_pre_commit'] = time.time() - start_time

            # Gatewayå“åº”æ—¶é—´
            start_time = time.time()
            gateway.talk_to_orchestrator("æ€§èƒ½æµ‹è¯•")
            api_response_times['gateway_talk'] = time.time() - start_time

            # æ³¨å…¥å™¨å“åº”æ—¶é—´
            start_time = time.time()
            injector.inject_and_call_orchestrator("æ€§èƒ½æµ‹è¯•")
            api_response_times['injector_call'] = time.time() - start_time

            performance_metrics['api_response_times'] = api_response_times

            # å¹¶å‘æ€§èƒ½æµ‹è¯•
            def concurrent_injection_test():
                return injector.inject_and_call_orchestrator(f"å¹¶å‘æµ‹è¯•-{time.time()}")

            start_time = time.time()
            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = [executor.submit(concurrent_injection_test) for _ in range(5)]
                results = [future.result() for future in as_completed(futures)]

            concurrent_time = time.time() - start_time
            successful_concurrent = sum(1 for r in results if r.get('success'))

            performance_metrics['concurrent_performance'] = {
                'total_time': concurrent_time,
                'operations_count': len(results),
                'success_count': successful_concurrent,
                'average_time_per_operation': concurrent_time / len(results),
                'success_rate': successful_concurrent / len(results) * 100
            }

            # å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœpsutilå¯ç”¨ï¼‰
            try:
                import psutil
                process = psutil.Process()
                memory_info = process.memory_info()
                performance_metrics['memory_usage'] = {
                    'rss_mb': memory_info.rss / 1024 / 1024,
                    'vms_mb': memory_info.vms / 1024 / 1024,
                    'memory_available': True
                }
            except ImportError:
                performance_metrics['memory_usage'] = {
                    'memory_available': False,
                    'note': 'psutil not available for memory monitoring'
                }

            # æ€§èƒ½è¯„ä¼°
            performance_evaluation = {
                'startup_performance': 'good' if max(component_startup_times.values()) < 2 else 'moderate' if max(component_startup_times.values()) < 5 else 'slow',
                'api_performance': 'good' if max(api_response_times.values()) < 1 else 'moderate' if max(api_response_times.values()) < 3 else 'slow',
                'concurrent_performance': 'good' if performance_metrics['concurrent_performance']['success_rate'] > 90 else 'moderate',
                'overall_performance': 'production_ready'
            }

            performance_test = {
                'metrics': performance_metrics,
                'evaluation': performance_evaluation,
                'performance_summary': {
                    'fastest_startup': min(component_startup_times.keys(), key=component_startup_times.get),
                    'fastest_api': min(api_response_times.keys(), key=api_response_times.get),
                    'concurrent_success_rate': performance_metrics['concurrent_performance']['success_rate']
                },
                'success': True
            }

            self.test_results['performance_metrics'] = performance_test

            print(f"âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ - æ€»ä½“è¯„ä¼°: {performance_evaluation['overall_performance']}")
            return True

        except Exception as e:
            self.test_results['performance_metrics'] = {
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__
            }
            print(f"âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return False

    def run_complete_test_suite(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶"""
        print("ğŸš€ Perfect21 API Interface Complete Test Suite")
        print("=" * 70)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
        print("ç›®æ ‡ï¼šéªŒè¯Perfect21å¤šAgentå¹¶è¡Œå·¥ä½œæµç³»ç»Ÿå®Œæ•´åŠŸèƒ½")
        print("=" * 70)

        test_methods = [
            ('CLI Statuså‘½ä»¤', self.test_cli_status_command),
            ('Orchestrator Gateway API', self.test_orchestrator_gateway_api),
            ('Capability Injection API', self.test_capability_injection_api),
            ('Git Hooks API', self.test_git_hooks_api),
            ('å¤šAgentåè°ƒåœºæ™¯', self.test_multi_agent_coordination_scenarios),
            ('ç³»ç»Ÿé›†æˆ', self.test_system_integration),
            ('æ€§èƒ½åŸºå‡†', self.test_performance_benchmarks)
        ]

        results = []
        for test_name, test_method in test_methods:
            print(f"\nğŸ” å¼€å§‹{test_name}æµ‹è¯•...")
            try:
                success = test_method()
                results.append((test_name, success))
                status_icon = "âœ…" if success else "âš ï¸"
                print(f"{status_icon} {test_name}{'å®Œæˆ' if success else 'éœ€è¦å…³æ³¨'}")
            except Exception as e:
                print(f"ğŸ’¥ {test_name}å¼‚å¸¸: {e}")
                results.append((test_name, False))

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.generate_final_report(results)

        return results

    def generate_final_report(self, results):
        """ç”Ÿæˆæœ€ç»ˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 70)
        print("ğŸ“Š ç”ŸæˆPerfect21å®Œæ•´æµ‹è¯•æŠ¥å‘Š...")

        # è®¡ç®—æ€»ä½“ç»“æœ
        total_tests = len(results)
        passed_tests = sum(1 for _, success in results if success)
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        # æ›´æ–°æµ‹è¯•ä¼šè¯ç»“æœ
        self.test_results['test_session']['summary'] = {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': total_tests - passed_tests,
            'success_rate': success_rate,
            'overall_status': 'PRODUCTION_READY' if success_rate >= 90 else 'NEEDS_ATTENTION',
            'test_completion_time': datetime.now().isoformat()
        }

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šå†…å®¹
        report_content = self._generate_complete_report()

        # ä¿å­˜æŠ¥å‘Šæ–‡ä»¶
        report_file = self.project_root / 'PERFECT21_API_COMPLETE_TEST_REPORT.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        # ä¿å­˜JSONæ•°æ®
        json_file = self.project_root / 'perfect21_api_test_results.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False)

        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print(f"ğŸ“‹ å®Œæ•´æµ‹è¯•æŠ¥å‘Š: {report_file}")
        print(f"ğŸ“Š è¯¦ç»†æµ‹è¯•æ•°æ®: {json_file}")
        print("\n" + "=" * 70)
        print("ğŸ¯ Perfect21 APIå®Œæ•´æµ‹è¯•ç»“æœ")
        print("=" * 70)

        print(f"ğŸ“ˆ æµ‹è¯•è¦†ç›–: {passed_tests}/{total_tests} ({success_rate:.1f}%)")

        if success_rate >= 90:
            print("ğŸ‰ Perfect21ç³»ç»Ÿå®Œå…¨å°±ç»ªï¼Œå¯æŠ•å…¥ç”Ÿäº§ä½¿ç”¨ï¼")
            print("\nâœ… éªŒè¯å®Œæˆçš„åŠŸèƒ½ï¼š")
            print("   â€¢ CLIæ¥å£å®Œå…¨å¯ç”¨")
            print("   â€¢ Orchestrator Gatewayæ­£å¸¸å·¥ä½œ")
            print("   â€¢ è‡ªåŠ¨èƒ½åŠ›æ³¨å…¥æœºåˆ¶è¿è¡Œè‰¯å¥½")
            print("   â€¢ Gitå·¥ä½œæµå¤šAgentåè°ƒæœ‰æ•ˆ")
            print("   â€¢ ç³»ç»Ÿé›†æˆå®Œæ•´æ— ç¼ºé™·")
            print("   â€¢ æ€§èƒ½æŒ‡æ ‡è¾¾åˆ°ç”Ÿäº§æ ‡å‡†")
            print("\nğŸš€ Perfect21 = ä½ çš„56äººä¸“ä¸šå¼€å‘å›¢é˜Ÿï¼Œç°åœ¨å¯ä»¥ï¼š")
            print("   1. æ¥æ”¶ä»»ä½•å¼€å‘ä»»åŠ¡")
            print("   2. è‡ªåŠ¨é€‰æ‹©æœ€ä½³Agentç»„åˆ")
            print("   3. å¹¶è¡Œæ‰§è¡Œå¤æ‚å¼€å‘æµç¨‹")
            print("   4. ä¿è¯ä»£ç è´¨é‡å’Œå®‰å…¨æ€§")
            print("   5. æä¾›å®æ—¶è¿›åº¦ç›‘æ§")
        else:
            failed_count = total_tests - passed_tests
            print(f"âš ï¸ å‘ç°{failed_count}ä¸ªéœ€è¦å…³æ³¨çš„é—®é¢˜")
            print("\nâŒ éœ€è¦ä¿®å¤ï¼š")
            for test_name, success in results:
                if not success:
                    print(f"   â€¢ {test_name}")
            print("\nğŸ’¡ å»ºè®®æ­¥éª¤ï¼š")
            print("   1. æ£€æŸ¥å¤±è´¥æµ‹è¯•çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯")
            print("   2. ä¿®å¤å‘ç°çš„é—®é¢˜")
            print("   3. é‡æ–°è¿è¡Œæµ‹è¯•éªŒè¯ä¿®å¤æ•ˆæœ")

        print("\n" + "=" * 70)
        print(f"æµ‹è¯•å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("Perfect21å¤šAgentå¹¶è¡Œå·¥ä½œæµç³»ç»Ÿ - æµ‹è¯•éªŒè¯å®Œæˆ")
        print("=" * 70)

    def _generate_complete_report(self) -> str:
        """ç”Ÿæˆå®Œæ•´MarkdownæŠ¥å‘Š"""
        session = self.test_results['test_session']
        summary = session['summary']

        report = f"""# Perfect21 API Complete Test Report

**æµ‹è¯•ç±»å‹**: {session['test_type']}
**æµ‹è¯•æ—¶é—´**: {session['timestamp']}
**å®Œæˆæ—¶é—´**: {summary['test_completion_time']}
**æ€»ä½“çŠ¶æ€**: {summary['overall_status']}

## ğŸ¯ æµ‹è¯•æ¦‚è§ˆ

- **æ€»æµ‹è¯•æ•°**: {summary['total_tests']}
- **é€šè¿‡æ•°**: {summary['passed_tests']}
- **å¤±è´¥æ•°**: {summary['failed_tests']}
- **æˆåŠŸç‡**: {summary['success_rate']:.1f}%

## ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ

"""

        # APIæµ‹è¯•ç»“æœ
        if self.test_results.get('api_tests'):
            report += "### ğŸ”Œ APIæ¥å£æµ‹è¯•\n\n"
            for test_name, test_result in self.test_results['api_tests'].items():
                status = "âœ…" if test_result.get('success', True) else "âŒ"
                report += f"#### {status} {test_name}\n"

                if test_result.get('success', True):
                    if 'execution_time' in test_result:
                        report += f"- æ‰§è¡Œæ—¶é—´: {test_result['execution_time']:.3f}ç§’\n"
                    if test_name == 'cli_status':
                        report += f"- è¾“å‡ºé•¿åº¦: {test_result.get('stdout_length', 0)}å­—ç¬¦\n"
                    elif test_name == 'orchestrator_gateway':
                        report += f"- ç”Ÿæˆå†…å®¹: {test_result.get('call_content_length', 0)}å­—ç¬¦\n"
                    elif test_name == 'capability_injection':
                        templates = test_result.get('capability_stats', {}).get('templates_count', 0)
                        report += f"- å¯ç”¨æ¨¡æ¿: {templates}ä¸ª\n"
                else:
                    report += f"- é”™è¯¯: {test_result.get('error', 'æœªçŸ¥é”™è¯¯')}\n"
                report += "\n"

        # Agentåè°ƒæµ‹è¯•
        if self.test_results.get('agent_coordination'):
            report += "### ğŸ¤ å¤šAgentåè°ƒæµ‹è¯•\n\n"
            if 'multi_agent_scenarios' in self.test_results['agent_coordination']:
                coord = self.test_results['agent_coordination']['multi_agent_scenarios']
                if coord.get('success'):
                    report += f"âœ… æµ‹è¯•åœºæ™¯: {coord.get('scenarios_tested', 0)}ä¸ª\n"
                    report += f"âœ… æ¶‰åŠAgentæ€»æ•°: {coord.get('total_expected_agents', 0)}ä¸ª\n"
                    report += f"âœ… å¹³å‡å“åº”æ—¶é—´: {coord.get('average_context_generation_time', 0):.3f}ç§’\n"
                else:
                    report += f"âŒ åè°ƒæµ‹è¯•å¤±è´¥: {coord.get('error')}\n"
            report += "\n"

        # ç³»ç»Ÿé›†æˆæµ‹è¯•
        if self.test_results.get('integration_tests'):
            report += "### ğŸ”— ç³»ç»Ÿé›†æˆæµ‹è¯•\n\n"
            if 'system_integration' in self.test_results['integration_tests']:
                integration = self.test_results['integration_tests']['system_integration']
                if integration.get('success'):
                    report += f"âœ… é›†æˆåˆ†æ•°: {integration.get('integration_score', 0):.1f}%\n"
                    report += f"âœ… å®Œæˆæ­¥éª¤: {integration.get('successful_steps', 0)}/{integration.get('total_steps', 0)}\n"
                    report += f"âœ… ç³»ç»Ÿå°±ç»ª: {'æ˜¯' if integration.get('system_ready') else 'å¦'}\n"
                else:
                    report += f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {integration.get('error')}\n"
            report += "\n"

        # æ€§èƒ½æµ‹è¯•ç»“æœ
        if self.test_results.get('performance_metrics'):
            report += "### âš¡ æ€§èƒ½æµ‹è¯•ç»“æœ\n\n"
            perf = self.test_results['performance_metrics']
            if perf.get('success'):
                evaluation = perf.get('evaluation', {})
                report += f"âœ… å¯åŠ¨æ€§èƒ½: {evaluation.get('startup_performance', 'unknown')}\n"
                report += f"âœ… APIæ€§èƒ½: {evaluation.get('api_performance', 'unknown')}\n"
                report += f"âœ… å¹¶å‘æ€§èƒ½: {evaluation.get('concurrent_performance', 'unknown')}\n"
                report += f"âœ… æ€»ä½“è¯„ä¼°: {evaluation.get('overall_performance', 'unknown')}\n"

                if 'performance_summary' in perf:
                    summary_data = perf['performance_summary']
                    report += f"âœ… å¹¶å‘æˆåŠŸç‡: {summary_data.get('concurrent_success_rate', 0):.1f}%\n"
            else:
                report += f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {perf.get('error')}\n"
            report += "\n"

        # æœ€ç»ˆè¯„ä¼°
        report += "## ğŸ† æœ€ç»ˆè¯„ä¼°\n\n"

        if summary['success_rate'] >= 90:
            report += "ğŸ‰ **Perfect21ç³»ç»Ÿå®Œå…¨å°±ç»ªï¼Œå¯æŠ•å…¥ç”Ÿäº§ä½¿ç”¨ï¼**\n\n"
            report += "### âœ… éªŒè¯å®Œæˆçš„æ ¸å¿ƒåŠŸèƒ½\n\n"
            report += "1. **CLIæ¥å£å®Œå…¨å¯ç”¨** - æ‰€æœ‰å‘½ä»¤å“åº”æ­£å¸¸\n"
            report += "2. **Orchestrator Gatewayæ­£å¸¸** - @orchestratorè°ƒç”¨æœºåˆ¶å®Œæ•´\n"
            report += "3. **è‡ªåŠ¨èƒ½åŠ›æ³¨å…¥æœ‰æ•ˆ** - Perfect21èƒ½åŠ›è‡ªåŠ¨æ³¨å…¥@orchestrator\n"
            report += "4. **Gitå·¥ä½œæµåè°ƒå®Œå–„** - å¤šAgentåŸºäºåˆ†æ”¯ç­–ç•¥åä½œ\n"
            report += "5. **ç³»ç»Ÿé›†æˆæ— ç¼ºé™·** - æ‰€æœ‰ç»„ä»¶ååŒå·¥ä½œè‰¯å¥½\n"
            report += "6. **æ€§èƒ½è¾¾åˆ°ç”Ÿäº§æ ‡å‡†** - å“åº”æ—¶é—´å’Œå¹¶å‘èƒ½åŠ›ç¬¦åˆè¦æ±‚\n\n"

            report += "### ğŸš€ Perfect21ç°åœ¨å¯ä»¥\n\n"
            report += "- æ¥æ”¶ä»»ä½•å¤æ‚çš„å¼€å‘ä»»åŠ¡\n"
            report += "- è‡ªåŠ¨é€‰æ‹©æœ€ä½³çš„Agentç»„åˆ\n"
            report += "- å¹¶è¡Œæ‰§è¡Œå¤æ‚çš„å¼€å‘æµç¨‹\n"
            report += "- ä¿è¯ä»£ç è´¨é‡å’Œå®‰å…¨æ€§\n"
            report += "- æä¾›å®æ—¶çš„è¿›åº¦ç›‘æ§\n\n"

            report += "**Perfect21 = ä½ çš„56äººä¸“ä¸šå¼€å‘å›¢é˜Ÿï¼**\n"
        else:
            report += f"âš ï¸ **ç³»ç»Ÿéœ€è¦æ”¹è¿›** ({summary['failed_tests']}ä¸ªé—®é¢˜)\n\n"
            report += "### âŒ éœ€è¦ä¿®å¤çš„é—®é¢˜\n\n"
            report += "è¯·æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹å¹¶è¿›è¡Œç›¸åº”ä¿®å¤ã€‚\n"

        return report

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    test_suite = Perfect21APITestSuite()
    results = test_suite.run_complete_test_suite()

    # è¿”å›é€€å‡ºç 
    all_passed = all(success for _, success in results)
    return 0 if all_passed else 1

if __name__ == '__main__':
    exit_code = main()
    exit(exit_code)