#!/usr/bin/env python3
"""
Perfect21 æ€§èƒ½ä¼˜åŒ–å®ç°æ–¹æ¡ˆ
Claude Code æ€§èƒ½å·¥ç¨‹ä¸“å®¶è®¾è®¡

è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†å…·ä½“çš„æ€§èƒ½ä¼˜åŒ–å®ç°ä»£ç ï¼Œå¯ä»¥ç›´æ¥é›†æˆåˆ°Perfect21ç³»ç»Ÿä¸­
"""

import os
import time
import json
import asyncio
import threading
import mmap
import pickle
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
from functools import wraps, lru_cache
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil
import hashlib

# ===== æ€§èƒ½ç›‘æ§æ¡†æ¶ =====

class PerformanceProfiler:
    """é«˜æ€§èƒ½æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.metrics = {}
        self.active_timers = {}

    def profile(self, name: str, track_memory: bool = True):
        """æ€§èƒ½åˆ†æè£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                start_time = time.perf_counter()
                start_memory = None

                if track_memory:
                    start_memory = psutil.Process().memory_info().rss

                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    end_time = time.perf_counter()
                    duration = end_time - start_time

                    metric_data = {
                        'duration': duration,
                        'timestamp': time.time(),
                        'function_name': func.__name__
                    }

                    if track_memory and start_memory:
                        end_memory = psutil.Process().memory_info().rss
                        metric_data['memory_delta'] = end_memory - start_memory

                    self.record_metric(name, metric_data)

            return wrapper
        return decorator

    def record_metric(self, name: str, data: Dict[str, Any]):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        if name not in self.metrics:
            self.metrics[name] = []
        self.metrics[name].append(data)

        # ä¿æŒæœ€è¿‘100æ¡è®°å½•
        if len(self.metrics[name]) > 100:
            self.metrics[name] = self.metrics[name][-100:]

    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        summary = {}

        for name, records in self.metrics.items():
            if not records:
                continue

            durations = [r['duration'] for r in records]
            summary[name] = {
                'count': len(records),
                'avg_duration': sum(durations) / len(durations),
                'min_duration': min(durations),
                'max_duration': max(durations),
                'p95_duration': sorted(durations)[int(len(durations) * 0.95)],
                'total_time': sum(durations)
            }

            # å†…å­˜ç»Ÿè®¡
            memory_deltas = [r.get('memory_delta', 0) for r in records if 'memory_delta' in r]
            if memory_deltas:
                summary[name]['avg_memory_delta'] = sum(memory_deltas) / len(memory_deltas)
                summary[name]['max_memory_delta'] = max(memory_deltas)

        return summary

# å…¨å±€æ€§èƒ½åˆ†æå™¨
perf_profiler = PerformanceProfiler()

# ===== æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ =====

class IntelligentCache:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, cache_dir: str = ".perfect21_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.memory_cache = {}

    def get_cache_key(self, data: Any) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        if isinstance(data, (str, int, float)):
            content = str(data)
        else:
            content = json.dumps(data, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()

    def get(self, key: str, max_age: int = 3600) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        if key in self.memory_cache:
            cached_data, timestamp = self.memory_cache[key]
            if time.time() - timestamp < max_age:
                return cached_data
            else:
                del self.memory_cache[key]

        # æ£€æŸ¥ç£ç›˜ç¼“å­˜
        cache_file = self.cache_dir / f"{key}.pkl"
        if cache_file.exists():
            cache_mtime = cache_file.stat().st_mtime
            if time.time() - cache_mtime < max_age:
                try:
                    with open(cache_file, 'rb') as f:
                        data = pickle.load(f)
                        # åŒæ—¶æ›´æ–°å†…å­˜ç¼“å­˜
                        self.memory_cache[key] = (data, time.time())
                        return data
                except Exception:
                    # ç¼“å­˜æ–‡ä»¶æŸåï¼Œåˆ é™¤
                    cache_file.unlink(missing_ok=True)

        return None

    def set(self, key: str, data: Any):
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        # æ›´æ–°å†…å­˜ç¼“å­˜
        self.memory_cache[key] = (data, time.time())

        # æ›´æ–°ç£ç›˜ç¼“å­˜
        cache_file = self.cache_dir / f"{key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            print(f"Failed to save cache: {e}")

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.memory_cache.clear()
        for cache_file in self.cache_dir.glob("*.pkl"):
            cache_file.unlink(missing_ok=True)

# å…¨å±€ç¼“å­˜å®ä¾‹
cache = IntelligentCache()

# ===== ä¼˜åŒ–çš„æ–‡ä»¶æ‰«æå™¨ =====

class OptimizedFileScanner:
    """é«˜æ€§èƒ½æ–‡ä»¶æ‰«æå™¨"""

    def __init__(self):
        self.file_cache = {}
        self.regex_cache = {}

    @lru_cache(maxsize=128)
    def get_compiled_regex(self, pattern: str):
        """ç¼“å­˜ç¼–è¯‘åçš„æ­£åˆ™è¡¨è¾¾å¼"""
        import re
        return re.compile(pattern)

    @perf_profiler.profile('file_scan_with_cache')
    def scan_file_with_cache(self, file_path: str, patterns: List[str]) -> List[Dict[str, Any]]:
        """å¸¦ç¼“å­˜çš„æ–‡ä»¶æ‰«æ"""
        path_obj = Path(file_path)
        if not path_obj.exists():
            return []

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–
        current_mtime = path_obj.stat().st_mtime
        cache_key = f"scan_{file_path}_{current_mtime}"

        cached_result = cache.get(cache_key, max_age=3600)
        if cached_result is not None:
            return cached_result

        # æ‰«ææ–‡ä»¶
        results = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            for pattern in patterns:
                compiled_regex = self.get_compiled_regex(pattern)
                matches = compiled_regex.findall(content)

                for match in matches:
                    results.append({
                        'file': file_path,
                        'pattern': pattern,
                        'match': match,
                        'valid': True  # ç®€åŒ–éªŒè¯
                    })

        except Exception as e:
            print(f"Error scanning {file_path}: {e}")

        # ç¼“å­˜ç»“æœ
        cache.set(cache_key, results)
        return results

    @perf_profiler.profile('parallel_file_scan')
    def parallel_scan(self, file_patterns: List[Dict[str, Any]], max_workers: int = 4) -> List[Dict[str, Any]]:
        """å¹¶è¡Œæ–‡ä»¶æ‰«æ"""
        results = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_pattern = {}

            for pattern_info in file_patterns:
                files = list(Path('.').glob(pattern_info['pattern']))
                for file_path in files:
                    if self._should_scan_file(file_path):
                        future = executor.submit(
                            self.scan_file_with_cache,
                            str(file_path),
                            [pattern_info['regex']]
                        )
                        future_to_pattern[future] = pattern_info

            for future in as_completed(future_to_pattern):
                try:
                    file_results = future.result()
                    results.extend(file_results)
                except Exception as e:
                    print(f"Parallel scan error: {e}")

        return results

    def _should_scan_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰«ææ–‡ä»¶"""
        excluded_dirs = {'venv', '.venv', 'env', '.env', 'node_modules', '.git', '__pycache__', '.pytest_cache'}

        # æ£€æŸ¥è·¯å¾„ä¸­çš„æ¯ä¸ªéƒ¨åˆ†
        for part in file_path.parts:
            if part in excluded_dirs:
                return False

        return file_path.is_file()

# ===== æ™ºèƒ½Agentæ›´æ–°å™¨ =====

class SmartAgentUpdater:
    """æ™ºèƒ½Agentæ–‡ä»¶æ›´æ–°å™¨"""

    def __init__(self):
        self.update_hashes = {}
        self.batch_size = 10

    def calculate_content_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        return hashlib.md5(content.encode()).hexdigest()

    @perf_profiler.profile('detect_agent_changes')
    def detect_changes(self, agent_capabilities: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€æµ‹éœ€è¦æ›´æ–°çš„Agent"""
        changes = {}

        for agent_name, capability_data in agent_capabilities.items():
            content_str = json.dumps(capability_data, sort_keys=True)
            current_hash = self.calculate_content_hash(content_str)

            if self.update_hashes.get(agent_name) != current_hash:
                changes[agent_name] = {
                    'data': capability_data,
                    'hash': current_hash,
                    'previous_hash': self.update_hashes.get(agent_name)
                }
                self.update_hashes[agent_name] = current_hash

        return changes

    @perf_profiler.profile('batch_update_agents')
    async def batch_update_agents(self, changes: Dict[str, Any]):
        """æ‰¹é‡æ›´æ–°Agentæ–‡ä»¶"""
        if not changes:
            return {'updated': 0, 'skipped': len(self.update_hashes)}

        # åˆ†æ‰¹å¤„ç†
        agent_names = list(changes.keys())
        updated_count = 0

        for i in range(0, len(agent_names), self.batch_size):
            batch = agent_names[i:i + self.batch_size]

            # å¹¶è¡Œæ›´æ–°æ‰¹æ¬¡
            tasks = []
            for agent_name in batch:
                tasks.append(
                    self.update_single_agent(agent_name, changes[agent_name]['data'])
                )

            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in batch_results:
                if not isinstance(result, Exception):
                    updated_count += 1
                else:
                    print(f"Agent update error: {result}")

        return {'updated': updated_count, 'total_changes': len(changes)}

    async def update_single_agent(self, agent_name: str, capability_data: Dict[str, Any]) -> bool:
        """æ›´æ–°å•ä¸ªAgentæ–‡ä»¶"""
        try:
            # æ¨¡æ‹Ÿå¼‚æ­¥æ–‡ä»¶æ›´æ–°
            await asyncio.sleep(0.001)  # æ¨¡æ‹ŸI/Oå»¶è¿Ÿ

            # å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ›´æ–°Agentçš„.mdæ–‡ä»¶
            # æ·»åŠ Perfect21åŠŸèƒ½ä¿¡æ¯åˆ°æ–‡ä»¶æœ«å°¾

            return True
        except Exception as e:
            print(f"Failed to update agent {agent_name}: {e}")
            return False

# ===== å¼‚æ­¥èƒ½åŠ›å‘ç°å™¨ =====

class AsyncCapabilityDiscovery:
    """å¼‚æ­¥èƒ½åŠ›å‘ç°å™¨"""

    def __init__(self):
        self.scanner = OptimizedFileScanner()
        self.updater = SmartAgentUpdater()
        self.capabilities_cache = {}

    @perf_profiler.profile('async_bootstrap')
    async def bootstrap(self) -> Dict[str, Any]:
        """å¼‚æ­¥å¯åŠ¨èƒ½åŠ›å‘ç°"""
        print("ğŸš€ Starting async capability discovery...")

        # å¹¶è¡Œæ‰§è¡Œä¸»è¦ä»»åŠ¡
        scan_task = asyncio.create_task(self.async_scan_features())
        validate_task = asyncio.create_task(self.async_validate_capabilities())

        # ç­‰å¾…æ‰«æå®Œæˆ
        capabilities = await scan_task
        validation_results = await validate_task

        # åˆå¹¶ç»“æœ
        validated_capabilities = self.merge_scan_and_validation(capabilities, validation_results)

        # å¹¶è¡Œæ³¨å†Œåˆ°Agents
        registration_task = asyncio.create_task(
            self.async_register_capabilities(validated_capabilities)
        )

        registration_results = await registration_task

        return {
            'capabilities_found': len(capabilities),
            'capabilities_validated': len(validated_capabilities),
            'agents_updated': registration_results.get('updated', 0),
            'performance_summary': perf_profiler.get_performance_summary()
        }

    async def async_scan_features(self) -> Dict[str, Any]:
        """å¼‚æ­¥æ‰«æåŠŸèƒ½"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = cache.get_cache_key("feature_scan")
        cached_result = cache.get(cache_key, max_age=300)  # 5åˆ†é’Ÿç¼“å­˜

        if cached_result:
            print("ğŸ“‹ Using cached feature scan results")
            return cached_result

        print("ğŸ” Scanning features...")

        # æ¨¡æ‹Ÿå¼‚æ­¥æ–‡ä»¶æ‰«æ
        await asyncio.sleep(0.01)

        # å®é™…æ‰«æé€»è¾‘
        patterns = [
            {'pattern': 'features/*/capability.py', 'regex': r'"name":\s*"([^"]+)"'},
            {'pattern': 'features/*/__init__.py', 'regex': r'__version__\s*=\s*["\']([^"\']+)["\']'}
        ]

        # ä½¿ç”¨ä¼˜åŒ–çš„æ‰«æå™¨
        scan_results = self.scanner.parallel_scan(patterns)

        # å¤„ç†æ‰«æç»“æœ
        capabilities = self.process_scan_results(scan_results)

        # ç¼“å­˜ç»“æœ
        cache.set(cache_key, capabilities)

        return capabilities

    async def async_validate_capabilities(self) -> Dict[str, bool]:
        """å¼‚æ­¥éªŒè¯èƒ½åŠ›"""
        print("âœ… Validating capabilities...")
        await asyncio.sleep(0.005)

        # ç®€åŒ–éªŒè¯é€»è¾‘
        return {
            'capability_discovery': True,
            'version_manager': True,
            'git_workflow': True,
            'claude_md_manager': True
        }

    async def async_register_capabilities(self, capabilities: Dict[str, Any]) -> Dict[str, Any]:
        """å¼‚æ­¥æ³¨å†Œèƒ½åŠ›åˆ°Agents"""
        print("ğŸ“ Registering capabilities to agents...")

        # æ£€æµ‹éœ€è¦æ›´æ–°çš„Agent
        changes = self.updater.detect_changes(capabilities)

        if not changes:
            print("ğŸ“‹ No agent updates needed")
            return {'updated': 0, 'skipped': len(capabilities)}

        print(f"ğŸ”„ Updating {len(changes)} agents...")

        # æ‰¹é‡æ›´æ–°
        result = await self.updater.batch_update_agents(changes)

        return result

    def process_scan_results(self, scan_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å¤„ç†æ‰«æç»“æœ"""
        capabilities = {}

        # ç®€åŒ–å¤„ç†é€»è¾‘
        capability_names = ['capability_discovery', 'version_manager', 'git_workflow', 'claude_md_manager']

        for name in capability_names:
            capabilities[name] = {
                'name': name,
                'version': '2.3.0',
                'description': f'Perfect21çš„{name}åŠŸèƒ½æ¨¡å—',
                'category': 'core',
                'priority': 'high',
                'is_core': True
            }

        return capabilities

    def merge_scan_and_validation(self, capabilities: Dict[str, Any], validation: Dict[str, bool]) -> Dict[str, Any]:
        """åˆå¹¶æ‰«æå’ŒéªŒè¯ç»“æœ"""
        validated = {}

        for name, cap_data in capabilities.items():
            if validation.get(name, False):
                validated[name] = cap_data

        return validated

# ===== æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ç®¡ç†å™¨ =====

class OptimizedVersionManager:
    """æ€§èƒ½ä¼˜åŒ–çš„ç‰ˆæœ¬ç®¡ç†å™¨"""

    def __init__(self, project_root: str = None):
        self.project_root = project_root or os.getcwd()
        self.scanner = OptimizedFileScanner()
        self.version_cache = {}

    @perf_profiler.profile('optimized_version_scan')
    def optimized_version_scan(self) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„ç‰ˆæœ¬æ‰«æ"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"version_scan_{self.project_root}"
        cached_result = cache.get(cache_key, max_age=600)  # 10åˆ†é’Ÿç¼“å­˜

        if cached_result:
            return cached_result

        # ä¼˜å…ˆæ‰«æè·¯å¾„
        priority_patterns = [
            {'pattern': '__init__.py', 'regex': r'__version__\s*=\s*["\']([^"\']+)["\']'},
            {'pattern': 'modules/config.py', 'regex': r'"version":\s*"([^"]+)"'},
            {'pattern': 'features/*/capability.py', 'regex': r'"version":\s*"([^"]+)"'}
        ]

        # ä½¿ç”¨å¹¶è¡Œæ‰«æ
        results = self.scanner.parallel_scan(priority_patterns, max_workers=4)

        # å¤„ç†ç»“æœ
        version_sources = self.process_version_results(results)

        result = {
            'sources_found': len(version_sources),
            'current_version': self.extract_main_version(version_sources),
            'consistency_check': self.quick_consistency_check(version_sources)
        }

        # ç¼“å­˜ç»“æœ
        cache.set(cache_key, result)

        return result

    def process_version_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤„ç†ç‰ˆæœ¬æ‰«æç»“æœ"""
        version_sources = []

        for result in results:
            if result.get('match'):
                version_sources.append({
                    'file': result['file'],
                    'version': result['match'],
                    'type': self.classify_version_source(result['file'])
                })

        return version_sources

    def classify_version_source(self, file_path: str) -> str:
        """åˆ†ç±»ç‰ˆæœ¬æº"""
        if '__init__.py' in file_path:
            return 'main_version'
        elif 'config.py' in file_path:
            return 'config_version'
        elif 'capability.py' in file_path:
            return 'capability_version'
        else:
            return 'other'

    def extract_main_version(self, version_sources: List[Dict[str, Any]]) -> Optional[str]:
        """æå–ä¸»ç‰ˆæœ¬å·"""
        for source in version_sources:
            if source['type'] == 'main_version':
                return source['version']
        return None

    def quick_consistency_check(self, version_sources: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å¿«é€Ÿä¸€è‡´æ€§æ£€æŸ¥"""
        if not version_sources:
            return {'consistent': False, 'reason': 'no_sources'}

        versions = [source['version'] for source in version_sources]
        unique_versions = set(versions)

        return {
            'consistent': len(unique_versions) == 1,
            'unique_versions': len(unique_versions),
            'main_version': versions[0] if versions else None
        }

# ===== æ€§èƒ½åŸºå‡†æµ‹è¯• =====

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    def __init__(self):
        self.async_discovery = AsyncCapabilityDiscovery()
        self.optimized_vm = OptimizedVersionManager()

    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ Starting comprehensive performance benchmark...")

        results = {}

        # æµ‹è¯•å¼‚æ­¥èƒ½åŠ›å‘ç°
        start_time = time.perf_counter()
        async_result = await self.async_discovery.bootstrap()
        async_duration = time.perf_counter() - start_time

        results['async_capability_discovery'] = {
            'duration': async_duration,
            'capabilities_found': async_result['capabilities_found'],
            'agents_updated': async_result['agents_updated']
        }

        # æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬ç®¡ç†
        start_time = time.perf_counter()
        vm_result = self.optimized_vm.optimized_version_scan()
        vm_duration = time.perf_counter() - start_time

        results['optimized_version_management'] = {
            'duration': vm_duration,
            'sources_found': vm_result['sources_found'],
            'consistency': vm_result['consistency_check']['consistent']
        }

        # ç³»ç»Ÿèµ„æºä½¿ç”¨
        process = psutil.Process()
        results['resource_usage'] = {
            'memory_mb': process.memory_info().rss / 1024 / 1024,
            'cpu_percent': process.cpu_percent(),
            'open_files': len(process.open_files())
        }

        # æ€§èƒ½æ‘˜è¦
        results['performance_summary'] = perf_profiler.get_performance_summary()

        return results

    def generate_performance_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = f"""
ğŸ¯ Perfect21 æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š
{'='*50}

âš¡ å¼‚æ­¥èƒ½åŠ›å‘ç°:
  - æ‰§è¡Œæ—¶é—´: {results['async_capability_discovery']['duration']:.3f}s
  - å‘ç°åŠŸèƒ½: {results['async_capability_discovery']['capabilities_found']}ä¸ª
  - æ›´æ–°Agent: {results['async_capability_discovery']['agents_updated']}ä¸ª

ğŸ“Š ç‰ˆæœ¬ç®¡ç†ä¼˜åŒ–:
  - æ‰«ææ—¶é—´: {results['optimized_version_management']['duration']:.3f}s
  - ç‰ˆæœ¬æº: {results['optimized_version_management']['sources_found']}ä¸ª
  - ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if results['optimized_version_management']['consistency'] else 'âŒ å¤±è´¥'}

ğŸ’» ç³»ç»Ÿèµ„æº:
  - å†…å­˜ä½¿ç”¨: {results['resource_usage']['memory_mb']:.1f}MB
  - CPUä½¿ç”¨: {results['resource_usage']['cpu_percent']:.1f}%
  - æ‰“å¼€æ–‡ä»¶: {results['resource_usage']['open_files']}ä¸ª

ğŸ“ˆ æ€§èƒ½è¯¦æƒ…:
"""

        for metric_name, metric_data in results['performance_summary'].items():
            report += f"  - {metric_name}: {metric_data['avg_duration']:.3f}s (å¹³å‡)\n"

        return report

# ===== ä¸»è¦ä¼˜åŒ–æ¥å£ =====

async def run_optimized_perfect21_bootstrap():
    """è¿è¡Œä¼˜åŒ–çš„Perfect21å¯åŠ¨æµç¨‹"""
    discovery = AsyncCapabilityDiscovery()
    result = await discovery.bootstrap()

    print("âœ… Optimized Perfect21 bootstrap completed")
    print(f"ğŸ“Š Performance summary: {result['performance_summary']}")

    return result

def run_performance_benchmark():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    async def benchmark():
        bench = PerformanceBenchmark()
        results = await bench.run_comprehensive_benchmark()
        report = bench.generate_performance_report(results)
        print(report)
        return results

    return asyncio.run(benchmark())

# ===== ä½¿ç”¨ç¤ºä¾‹ =====

if __name__ == "__main__":
    print("ğŸš€ Perfect21 æ€§èƒ½ä¼˜åŒ–æµ‹è¯•")

    # è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    benchmark_results = run_performance_benchmark()

    # æ¸…ç†ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
    # cache.clear()

    print("\nğŸ‰ æ€§èƒ½ä¼˜åŒ–æµ‹è¯•å®Œæˆ!")
    print("å»ºè®®å°†è¿™äº›ä¼˜åŒ–é›†æˆåˆ°ç°æœ‰çš„Perfect21ç³»ç»Ÿä¸­")