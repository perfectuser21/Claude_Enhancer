#!/usr/bin/env python3
"""
å®æ—¶æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿
ç›‘æ§Claude Enhancerç³»ç»Ÿæ€§èƒ½å¹¶æä¾›å®æ—¶ä¼˜åŒ–å»ºè®®
"""

import asyncio
import json
import time
import psutil
import threading
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
import subprocess


@dataclass
class PerformanceMetrics:
    timestamp: str
    cpu_usage: float
    memory_usage: float
    disk_usage: float
    active_hooks: int
    avg_response_time: float
    success_rate: float
    concurrent_jobs: int
    error_count: int


@dataclass
class OptimizationSuggestion:
    type: str
    priority: str  # high, medium, low
    message: str
    action: str
    estimated_improvement: str


class PerformanceMonitor:
    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
        self.cache_dir = Path("/tmp/.claude_perf_cache")
        self.cache_dir.mkdir(exist_ok=True)

        self.log_files = {
            "performance": self.cache_dir / "performance.log",
            "errors": self.cache_dir / "errors.log",
            "metrics": self.cache_dir / "metrics.log",
        }

        # æ€§èƒ½é˜ˆå€¼
        self.thresholds = {
            "cpu_high": 70.0,
            "memory_high": 80.0,
            "disk_high": 90.0,
            "response_slow": 1000.0,  # ms
            "success_rate_low": 90.0,
            "error_rate_high": 5.0,
        }

        self.optimization_suggestions: List[OptimizationSuggestion] = []
        self.is_monitoring = False

    def collect_system_metrics(self) -> Dict:
        """æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=0.1)

            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            memory_percent = memory.percent

            # ç£ç›˜ä½¿ç”¨ç‡
            disk = psutil.disk_usage("/")
            disk_percent = disk.percent

            # è¿›ç¨‹ä¿¡æ¯
            processes = list(psutil.process_iter(["pid", "name", "cmdline"]))
            claude_processes = [
                p
                for p in processes
                if any(
                    "claude" in str(item).lower() for item in p.info["cmdline"] or []
                )
            ]

            return {
                "cpu_usage": cpu_percent,
                "memory_usage": memory_percent,
                "disk_usage": disk_percent,
                "active_processes": len(claude_processes),
                "total_processes": len(processes),
            }
        except Exception as e:
            print(f"âŒ æ”¶é›†ç³»ç»ŸæŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            return {}

    def parse_performance_logs(self) -> Dict:
        """è§£ææ€§èƒ½æ—¥å¿—æ–‡ä»¶"""
        metrics = {
            "avg_response_time": 0.0,
            "total_requests": 0,
            "error_count": 0,
            "success_count": 0,
        }

        try:
            # è§£ææ€§èƒ½æ—¥å¿—
            if self.log_files["performance"].exists():
                with open(self.log_files["performance"], "r") as f:
                    lines = f.readlines()[-100:]  # åªçœ‹æœ€è¿‘100æ¡

                response_times = []
                for line in lines:
                    if "|" in line and "ms" in line:
                        parts = line.strip().split("|")
                        if len(parts) >= 3:
                            time_part = parts[2].replace("ms", "")
                            try:
                                response_times.append(float(time_part))
                            except ValueError:
                                continue

                if response_times:
                    metrics["avg_response_time"] = sum(response_times) / len(
                        response_times
                    )
                    metrics["total_requests"] = len(response_times)

            # è§£æé”™è¯¯æ—¥å¿—
            if self.log_files["errors"].exists():
                with open(self.log_files["errors"], "r") as f:
                    lines = f.readlines()[-50:]  # æœ€è¿‘50æ¡é”™è¯¯

                for line in lines:
                    if "ERROR" in line:
                        metrics["error_count"] += 1
                    elif "SUCCESS" in line:
                        metrics["success_count"] += 1

        except Exception as e:
            print(f"âŒ è§£ææ—¥å¿—æ—¶å‡ºé”™: {e}")

        return metrics

    def calculate_success_rate(self, success_count: int, error_count: int) -> float:
        """è®¡ç®—æˆåŠŸç‡"""
        total = success_count + error_count
        if total == 0:
            return 100.0
        return (success_count / total) * 100.0

    def analyze_performance(
        self, metrics: PerformanceMetrics
    ) -> List[OptimizationSuggestion]:
        """åˆ†ææ€§èƒ½å¹¶ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []

        # CPUä½¿ç”¨ç‡è¿‡é«˜
        if metrics.cpu_usage > self.thresholds["cpu_high"]:
            suggestions.append(
                OptimizationSuggestion(
                    type="cpu_optimization",
                    priority="high",
                    message=f"CPUä½¿ç”¨ç‡è¿‡é«˜ ({metrics.cpu_usage:.1f}%)",
                    action="å‡å°‘å¹¶å‘æ•°é‡æˆ–ä¼˜åŒ–è®¡ç®—å¯†é›†å‹æ“ä½œ",
                    estimated_improvement="å‡å°‘20-30%çš„CPUä½¿ç”¨ç‡",
                )
            )

        # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜
        if metrics.memory_usage > self.thresholds["memory_high"]:
            suggestions.append(
                OptimizationSuggestion(
                    type="memory_optimization",
                    priority="high",
                    message=f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({metrics.memory_usage:.1f}%)",
                    action="æ¸…ç†ç¼“å­˜æ–‡ä»¶æˆ–å¢åŠ å†…å­˜é™åˆ¶",
                    estimated_improvement="é‡Šæ”¾10-20%çš„å†…å­˜ç©ºé—´",
                )
            )

        # å“åº”æ—¶é—´è¿‡æ…¢
        if metrics.avg_response_time > self.thresholds["response_slow"]:
            suggestions.append(
                OptimizationSuggestion(
                    type="response_optimization",
                    priority="medium",
                    message=f"å¹³å‡å“åº”æ—¶é—´è¿‡æ…¢ ({metrics.avg_response_time:.0f}ms)",
                    action="å¯ç”¨ç¼“å­˜æœºåˆ¶æˆ–ä¼˜åŒ–è„šæœ¬ç®—æ³•",
                    estimated_improvement="æå‡50-70%çš„å“åº”é€Ÿåº¦",
                )
            )

        # æˆåŠŸç‡è¿‡ä½
        if metrics.success_rate < self.thresholds["success_rate_low"]:
            suggestions.append(
                OptimizationSuggestion(
                    type="reliability_optimization",
                    priority="high",
                    message=f"æˆåŠŸç‡è¿‡ä½ ({metrics.success_rate:.1f}%)",
                    action="å¯ç”¨é”™è¯¯é‡è¯•æœºåˆ¶æˆ–ä¿®å¤å¤±è´¥çš„Hook",
                    estimated_improvement="æå‡æˆåŠŸç‡åˆ°95%ä»¥ä¸Š",
                )
            )

        # å¹¶å‘ä»»åŠ¡è¿‡å¤š
        if metrics.concurrent_jobs > 10:
            suggestions.append(
                OptimizationSuggestion(
                    type="concurrency_optimization",
                    priority="medium",
                    message=f"å¹¶å‘ä»»åŠ¡è¿‡å¤š ({metrics.concurrent_jobs}ä¸ª)",
                    action="é™åˆ¶æœ€å¤§å¹¶å‘æ•°æˆ–å®æ–½ä»»åŠ¡é˜Ÿåˆ—",
                    estimated_improvement="æå‡ç³»ç»Ÿç¨³å®šæ€§",
                )
            )

        return suggestions

    def collect_metrics(self) -> PerformanceMetrics:
        """æ”¶é›†å®Œæ•´çš„æ€§èƒ½æŒ‡æ ‡"""
        system_metrics = self.collect_system_metrics()
        log_metrics = self.parse_performance_logs()

        # è®¡ç®—æˆåŠŸç‡
        success_rate = self.calculate_success_rate(
            log_metrics.get("success_count", 0), log_metrics.get("error_count", 0)
        )

        return PerformanceMetrics(
            timestamp=datetime.now().isoformat(),
            cpu_usage=system_metrics.get("cpu_usage", 0),
            memory_usage=system_metrics.get("memory_usage", 0),
            disk_usage=system_metrics.get("disk_usage", 0),
            active_hooks=system_metrics.get("active_processes", 0),
            avg_response_time=log_metrics.get("avg_response_time", 0),
            success_rate=success_rate,
            concurrent_jobs=log_metrics.get("total_requests", 0),
            error_count=log_metrics.get("error_count", 0),
        )

    def display_dashboard(
        self, metrics: PerformanceMetrics, suggestions: List[OptimizationSuggestion]
    ):
        """æ˜¾ç¤ºå®æ—¶ä»ªè¡¨æ¿"""
        # æ¸…å±
        print("\033[2J\033[H")

        print("ğŸš€ Claude Enhancer å®æ—¶æ€§èƒ½ç›‘æ§")
        print("=" * 60)
        print(f"ğŸ“… æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()

        # ç³»ç»ŸæŒ‡æ ‡
        print("ğŸ“Š ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡:")
        print(
            f"  ğŸ–¥ï¸  CPUä½¿ç”¨ç‡:     {metrics.cpu_usage:6.1f}% {'ğŸ”´' if metrics.cpu_usage > 70 else 'ğŸŸ¢'}"
        )
        print(
            f"  ğŸ’¾ å†…å­˜ä½¿ç”¨ç‡:     {metrics.memory_usage:6.1f}% {'ğŸ”´' if metrics.memory_usage > 80 else 'ğŸŸ¢'}"
        )
        print(
            f"  ğŸ’¿ ç£ç›˜ä½¿ç”¨ç‡:     {metrics.disk_usage:6.1f}% {'ğŸ”´' if metrics.disk_usage > 90 else 'ğŸŸ¢'}"
        )
        print(
            f"  âš¡ å¹³å‡å“åº”æ—¶é—´:   {metrics.avg_response_time:6.0f}ms {'ğŸ”´' if metrics.avg_response_time > 1000 else 'ğŸŸ¢'}"
        )
        print(
            f"  âœ… æˆåŠŸç‡:         {metrics.success_rate:6.1f}% {'ğŸ”´' if metrics.success_rate < 90 else 'ğŸŸ¢'}"
        )
        print(f"  ğŸ”„ æ´»è·ƒHook:       {metrics.active_hooks:6d}ä¸ª")
        print(f"  ğŸ“ å¹¶å‘ä»»åŠ¡:       {metrics.concurrent_jobs:6d}ä¸ª")
        print(f"  âŒ é”™è¯¯è®¡æ•°:       {metrics.error_count:6d}ä¸ª")
        print()

        # ä¼˜åŒ–å»ºè®®
        if suggestions:
            print("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, suggestion in enumerate(suggestions[:5], 1):
                priority_icon = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}[
                    suggestion.priority
                ]
                print(f"  {i}. {priority_icon} {suggestion.message}")
                print(f"     ğŸ”§ å»ºè®®: {suggestion.action}")
                print(f"     ğŸ“ˆ é¢„æœŸ: {suggestion.estimated_improvement}")
                print()
        else:
            print("âœ¨ ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œæ— éœ€ä¼˜åŒ–å»ºè®®")
            print()

        # å†å²è¶‹åŠ¿
        if len(self.metrics_history) > 1:
            print("ğŸ“ˆ æ€§èƒ½è¶‹åŠ¿ (æœ€è¿‘5åˆ†é’Ÿ):")
            recent_metrics = self.metrics_history[-10:]

            cpu_trend = (
                "ä¸Šå‡"
                if recent_metrics[-1].cpu_usage > recent_metrics[0].cpu_usage
                else "ä¸‹é™"
            )
            memory_trend = (
                "ä¸Šå‡"
                if recent_metrics[-1].memory_usage > recent_metrics[0].memory_usage
                else "ä¸‹é™"
            )

            print(f"  ğŸ“Š CPUè¶‹åŠ¿: {cpu_trend}")
            print(f"  ğŸ“Š å†…å­˜è¶‹åŠ¿: {memory_trend}")
            print(f"  ğŸ“Š æ•°æ®ç‚¹: {len(recent_metrics)}ä¸ª")

    def save_metrics(self, metrics: PerformanceMetrics):
        """ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        try:
            metrics_file = self.cache_dir / "realtime_metrics.jsonl"
            with open(metrics_file, "a") as f:
                f.write(json.dumps(asdict(metrics)) + "\n")
        except Exception as e:
            print(f"âŒ ä¿å­˜æŒ‡æ ‡æ—¶å‡ºé”™: {e}")

    async def monitor_loop(self):
        """ä¸»ç›‘æ§å¾ªç¯"""
        print("ğŸš€ å¯åŠ¨å®æ—¶æ€§èƒ½ç›‘æ§...")
        self.is_monitoring = True

        while self.is_monitoring:
            try:
                # æ”¶é›†æŒ‡æ ‡
                metrics = self.collect_metrics()
                self.metrics_history.append(metrics)

                # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
                if len(self.metrics_history) > 100:
                    self.metrics_history = self.metrics_history[-50:]

                # åˆ†ææ€§èƒ½
                suggestions = self.analyze_performance(metrics)
                self.optimization_suggestions = suggestions

                # æ˜¾ç¤ºä»ªè¡¨æ¿
                self.display_dashboard(metrics, suggestions)

                # ä¿å­˜æŒ‡æ ‡
                self.save_metrics(metrics)

                # ç­‰å¾…ä¸‹æ¬¡æ›´æ–°
                await asyncio.sleep(3)

            except KeyboardInterrupt:
                print("\nğŸ›‘ ç›‘æ§å·²åœæ­¢")
                self.is_monitoring = False
                break
            except Exception as e:
                print(f"âŒ ç›‘æ§å¾ªç¯å‡ºé”™: {e}")
                await asyncio.sleep(5)

    def export_report(self, hours: int = 1) -> str:
        """å¯¼å‡ºæ€§èƒ½æŠ¥å‘Š"""
        report_file = (
            self.cache_dir
            / f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )

        # è¯»å–æœ€è¿‘çš„æŒ‡æ ‡
        metrics_file = self.cache_dir / "realtime_metrics.jsonl"
        recent_metrics = []

        if metrics_file.exists():
            cutoff_time = datetime.now() - timedelta(hours=hours)

            with open(metrics_file, "r") as f:
                for line in f:
                    try:
                        metric = json.loads(line)
                        metric_time = datetime.fromisoformat(metric["timestamp"])
                        if metric_time > cutoff_time:
                            recent_metrics.append(metric)
                    except Exception:
                        continue

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "report_generated": datetime.now().isoformat(),
            "time_range_hours": hours,
            "total_metrics": len(recent_metrics),
            "summary": {
                "avg_cpu": sum(m["cpu_usage"] for m in recent_metrics)
                / len(recent_metrics)
                if recent_metrics
                else 0,
                "avg_memory": sum(m["memory_usage"] for m in recent_metrics)
                / len(recent_metrics)
                if recent_metrics
                else 0,
                "avg_response_time": sum(m["avg_response_time"] for m in recent_metrics)
                / len(recent_metrics)
                if recent_metrics
                else 0,
                "avg_success_rate": sum(m["success_rate"] for m in recent_metrics)
                / len(recent_metrics)
                if recent_metrics
                else 0,
                "total_errors": sum(m["error_count"] for m in recent_metrics),
            },
            "optimization_suggestions": [
                asdict(s) for s in self.optimization_suggestions
            ],
            "raw_metrics": recent_metrics,
        }

        with open(report_file, "w") as f:
            json.dump(report, f, indent=2)

        print(f"ğŸ“„ æ€§èƒ½æŠ¥å‘Šå·²å¯¼å‡º: {report_file}")
        return str(report_file)


def main():
    import sys

    monitor = PerformanceMonitor()

    if len(sys.argv) > 1 and sys.argv[1] == "report":
        hours = int(sys.argv[2]) if len(sys.argv) > 2 else 1
        monitor.export_report(hours)
    else:
        try:
            asyncio.run(monitor.monitor_loop())
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç›‘æ§å·²åœæ­¢")


if __name__ == "__main__":
    main()
