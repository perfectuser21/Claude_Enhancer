#!/usr/bin/env python3
"""
Claude Enhancer æ€§èƒ½éªŒè¯å¥—ä»¶
éªŒè¯ä¼˜åŒ–åçš„ç³»ç»Ÿæ˜¯å¦è¾¾åˆ°é¢„æœŸæ€§èƒ½æŒ‡æ ‡
"""

import asyncio
import json
import time
import statistics
import subprocess
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass, asdict
import tempfile
import sys


@dataclass
class TestResult:
    test_name: str
    success: bool
    duration_ms: float
    error_message: str = ""
    metrics: Dict = None


@dataclass
class BenchmarkSuite:
    name: str
    baseline_performance: Dict
    target_performance: Dict
    actual_performance: Dict
    improvement_ratio: float
    passed: bool


class PerformanceValidator:
    def __init__(self):
        self.claude_dir = Path(__file__).parent.parent
        self.hooks_dir = self.claude_dir / "hooks"
        self.scripts_dir = self.claude_dir / "scripts"
        self.test_results: List[TestResult] = []

        # æ€§èƒ½ç›®æ ‡
        self.performance_targets = {
            "hook_success_rate": 95.0,  # 95%ä»¥ä¸ŠæˆåŠŸç‡
            "hook_response_time": 200.0,  # 200msä»¥ä¸‹å“åº”æ—¶é—´
            "concurrent_success_rate": 95.0,  # 95%ä»¥ä¸Šå¹¶å‘æˆåŠŸç‡
            "script_execution_time": 1000.0,  # 1ç§’ä»¥ä¸‹è„šæœ¬æ‰§è¡Œæ—¶é—´
            "memory_usage": 50.0,  # 50MBä»¥ä¸‹å†…å­˜ä½¿ç”¨
            "cpu_usage": 30.0,  # 30%ä»¥ä¸‹CPUä½¿ç”¨ç‡
        }

    def run_hook_performance_test(
        self, hook_script: str, iterations: int = 20
    ) -> TestResult:
        """æµ‹è¯•å•ä¸ªHookçš„æ€§èƒ½"""
        start_time = time.time()
        success_count = 0
        response_times = []
        errors = []

        hook_path = self.hooks_dir / hook_script

        if not hook_path.exists():
            return TestResult(
                test_name=f"hook_{hook_script}",
                success=False,
                duration_ms=0,
                error_message=f"Hookæ–‡ä»¶ä¸å­˜åœ¨: {hook_path}",
            )

        for i in range(iterations):
            iteration_start = time.time()

            try:
                pass  # Auto-fixed empty block
                # å‡†å¤‡æµ‹è¯•è¾“å…¥
                test_input = json.dumps(
                    {
                        "tool": "test_tool",
                        "prompt": f"æµ‹è¯•Hookæ€§èƒ½ - è¿­ä»£ {i+1}",
                        "iteration": i + 1,
                    }
                )

                # æ‰§è¡ŒHook
                result = subprocess.run(
                    ["bash", str(hook_path)],
                    input=test_input,
                    text=True,
                    capture_output=True,
                    timeout=2,  # 2ç§’è¶…æ—¶
                )

                iteration_time = (time.time() - iteration_start) * 1000

                if result.returncode == 0:
                    success_count += 1
                else:
                    errors.append(f"è¿­ä»£{i+1}: {result.stderr[:100]}")

                response_times.append(iteration_time)

            except subprocess.TimeoutExpired:
                response_times.append(2000)  # è¶…æ—¶è®¡ä¸º2000ms
                errors.append(f"è¿­ä»£{i+1}: è¶…æ—¶")
            except Exception as e:
                response_times.append(2000)
                errors.append(f"è¿­ä»£{i+1}: {str(e)}")

        # è®¡ç®—æŒ‡æ ‡
        success_rate = (success_count / iterations) * 100
        avg_response_time = statistics.mean(response_times) if response_times else 2000
        total_duration = (time.time() - start_time) * 1000

        # åˆ¤æ–­æ˜¯å¦é€šè¿‡
        passed = (
            success_rate >= self.performance_targets["hook_success_rate"]
            and avg_response_time <= self.performance_targets["hook_response_time"]
        )

        return TestResult(
            test_name=f"hook_{hook_script}",
            success=passed,
            duration_ms=total_duration,
            metrics={
                "success_rate": success_rate,
                "avg_response_time": avg_response_time,
                "min_response_time": min(response_times) if response_times else 0,
                "max_response_time": max(response_times) if response_times else 0,
                "total_iterations": iterations,
                "successful_iterations": success_count,
                "errors": errors[:5],  # åªä¿ç•™å‰5ä¸ªé”™è¯¯
            },
        )

    def run_concurrent_stress_test(
        self, workers_list: List[int] = [5, 10, 15, 20]
    ) -> TestResult:
        """è¿è¡Œå¹¶å‘å‹åŠ›æµ‹è¯•"""
        start_time = time.time()
        concurrent_results = {}

        for worker_count in workers_list:
            print(f"  ğŸ”„ æµ‹è¯• {worker_count} å¹¶å‘worker...")

            worker_start = time.time()
            success_count = 0
            total_tasks = worker_count * 3  # æ¯ä¸ªworkerå¤„ç†3ä¸ªä»»åŠ¡

            with ThreadPoolExecutor(max_workers=worker_count) as executor:
                pass  # Auto-fixed empty block
                # æäº¤ä»»åŠ¡
                futures = []
                for i in range(total_tasks):
                    future = executor.submit(
                        self._single_concurrent_task, i, worker_count
                    )
                    futures.append(future)

                # æ”¶é›†ç»“æœ
                for future in as_completed(futures, timeout=30):
                    try:
                        result = future.result(timeout=5)
                        if result:
                            success_count += 1
                    except Exception:
                        pass

            worker_duration = time.time() - worker_start
            success_rate = (success_count / total_tasks) * 100
            throughput = total_tasks / worker_duration

            concurrent_results[f"{worker_count}_workers"] = {
                "success_rate": success_rate,
                "duration": worker_duration,
                "throughput": throughput,
                "total_tasks": total_tasks,
                "successful_tasks": success_count,
            }

        # è¯„ä¼°æ•´ä½“å¹¶å‘æ€§èƒ½
        worst_success_rate = min(
            result["success_rate"] for result in concurrent_results.values()
        )

        passed = (
            worst_success_rate >= self.performance_targets["concurrent_success_rate"]
        )
        total_duration = (time.time() - start_time) * 1000

        return TestResult(
            test_name="concurrent_stress_test",
            success=passed,
            duration_ms=total_duration,
            metrics={
                "worst_success_rate": worst_success_rate,
                "detailed_results": concurrent_results,
                "target_success_rate": self.performance_targets[
                    "concurrent_success_rate"
                ],
            },
        )

    def _single_concurrent_task(self, task_id: int, worker_count: int) -> bool:
        """å•ä¸ªå¹¶å‘ä»»åŠ¡"""
        try:
            pass  # Auto-fixed empty block
            # æ¨¡æ‹ŸHookè°ƒç”¨
            test_input = json.dumps(
                {
                    "tool": "concurrent_test",
                    "task_id": task_id,
                    "worker_count": worker_count,
                    "timestamp": time.time(),
                }
            )

            # éšæœºé€‰æ‹©ä¸€ä¸ªä¼˜åŒ–åçš„Hookè¿›è¡Œæµ‹è¯•
            import random

            hooks = ["optimized_performance_monitor.sh", "ultra_fast_agent_selector.sh"]
            selected_hook = random.choice(hooks)
            hook_path = self.hooks_dir / selected_hook

            if not hook_path.exists():
                return False

            result = subprocess.run(
                ["bash", str(hook_path)],
                input=test_input,
                text=True,
                capture_output=True,
                timeout=1,
            )

            return result.returncode == 0

        except Exception:
            return False

    def run_script_performance_test(self) -> TestResult:
        """æµ‹è¯•è„šæœ¬æ‰§è¡Œæ€§èƒ½"""
        start_time = time.time()

        # æµ‹è¯•æ€§èƒ½ä¼˜åŒ–è„šæœ¬æœ¬èº«
        test_scripts = ["ultra_performance_optimizer.sh", "deploy_optimizations.sh"]

        script_results = {}

        for script_name in test_scripts:
            script_path = self.scripts_dir / script_name

            if not script_path.exists():
                continue

            print(f"  ğŸ“œ æµ‹è¯•è„šæœ¬: {script_name}")

            try:
                script_start = time.time()

                # è¿è¡Œè„šæœ¬çš„åˆ†ææ¨¡å¼ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
                result = subprocess.run(
                    ["bash", str(script_path), "analyze", "/tmp"],
                    capture_output=True,
                    text=True,
                    timeout=10,
                )

                script_duration = (time.time() - script_start) * 1000
                script_success = result.returncode == 0

                script_results[script_name] = {
                    "duration_ms": script_duration,
                    "success": script_success,
                    "output_size": len(result.stdout) if result.stdout else 0,
                }

            except subprocess.TimeoutExpired:
                script_results[script_name] = {
                    "duration_ms": 10000,  # è¶…æ—¶
                    "success": False,
                    "error": "timeout",
                }
            except Exception as e:
                script_results[script_name] = {
                    "duration_ms": 10000,
                    "success": False,
                    "error": str(e),
                }

        # è¯„ä¼°è„šæœ¬æ€§èƒ½
        if script_results:
            avg_duration = statistics.mean(
                result["duration_ms"] for result in script_results.values()
            )
            success_count = sum(
                1 for result in script_results.values() if result["success"]
            )
            success_rate = (success_count / len(script_results)) * 100
        else:
            avg_duration = 0
            success_rate = 0

        passed = (
            avg_duration <= self.performance_targets["script_execution_time"]
            and success_rate >= 80
        )  # 80%çš„è„šæœ¬æµ‹è¯•æˆåŠŸç‡

        total_duration = (time.time() - start_time) * 1000

        return TestResult(
            test_name="script_performance_test",
            success=passed,
            duration_ms=total_duration,
            metrics={
                "avg_script_duration": avg_duration,
                "script_success_rate": success_rate,
                "detailed_results": script_results,
                "target_duration": self.performance_targets["script_execution_time"],
            },
        )

    def run_system_resource_test(self) -> TestResult:
        """æµ‹è¯•ç³»ç»Ÿèµ„æºä½¿ç”¨"""
        start_time = time.time()

        try:
            import psutil

            # ç›‘æ§èµ„æºä½¿ç”¨
            cpu_readings = []
            memory_readings = []

            # åœ¨è´Ÿè½½ä¸‹ç›‘æ§èµ„æº
            def monitor_resources():
                for _ in range(10):  # ç›‘æ§10æ¬¡ï¼Œæ¯æ¬¡é—´éš”0.5ç§’
                    cpu_readings.append(psutil.cpu_percent(interval=0.5))
                    memory = psutil.virtual_memory()
                    memory_readings.append(memory.percent)

            # å¯åŠ¨ç›‘æ§
            monitor_thread = threading.Thread(target=monitor_resources)
            monitor_thread.start()

            # åŒæ—¶è¿è¡Œä¸€äº›è´Ÿè½½
            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = []
                for i in range(10):
                    future = executor.submit(self._resource_load_task, i)
                    futures.append(future)

                # ç­‰å¾…å®Œæˆ
                for future in as_completed(futures):
                    try:
                        future.result()
                    except Exception:
                        pass

            monitor_thread.join()

            # è®¡ç®—å¹³å‡èµ„æºä½¿ç”¨
            avg_cpu = statistics.mean(cpu_readings) if cpu_readings else 0
            avg_memory = statistics.mean(memory_readings) if memory_readings else 0

            # è¯„ä¼°èµ„æºä½¿ç”¨
            cpu_passed = avg_cpu <= self.performance_targets["cpu_usage"]
            memory_passed = avg_memory <= self.performance_targets["memory_usage"]
            passed = cpu_passed and memory_passed

            total_duration = (time.time() - start_time) * 1000

            return TestResult(
                test_name="system_resource_test",
                success=passed,
                duration_ms=total_duration,
                metrics={
                    "avg_cpu_usage": avg_cpu,
                    "avg_memory_usage": avg_memory,
                    "max_cpu_usage": max(cpu_readings) if cpu_readings else 0,
                    "max_memory_usage": max(memory_readings) if memory_readings else 0,
                    "cpu_target": self.performance_targets["cpu_usage"],
                    "memory_target": self.performance_targets["memory_usage"],
                    "cpu_passed": cpu_passed,
                    "memory_passed": memory_passed,
                },
            )

        except ImportError:
            return TestResult(
                test_name="system_resource_test",
                success=False,
                duration_ms=0,
                error_message="psutil æ¨¡å—æœªå®‰è£…",
            )

    def _resource_load_task(self, task_id: int):
        """åˆ›å»ºä¸€äº›è®¡ç®—è´Ÿè½½"""
        # ç®€å•çš„è®¡ç®—ä»»åŠ¡
        total = 0
        for i in range(100000):
            total += i * task_id
        return total

    def run_full_validation_suite(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„éªŒè¯å¥—ä»¶"""
        print("ğŸ§ª å¯åŠ¨Claude Enhanceræ€§èƒ½éªŒè¯å¥—ä»¶")
        print("=" * 50)

        suite_start = time.time()

        # 1. Hookæ€§èƒ½æµ‹è¯•
        print("\nğŸ“‹ 1. Hookæ€§èƒ½æµ‹è¯•")
        hook_tests = [
            "optimized_performance_monitor.sh",
            "ultra_fast_agent_selector.sh",
            "smart_error_recovery.sh",
            "concurrent_optimizer.sh",
        ]

        for hook in hook_tests:
            print(f"  ğŸ”§ æµ‹è¯• {hook}...")
            result = self.run_hook_performance_test(hook)
            self.test_results.append(result)

        # 2. å¹¶å‘å‹åŠ›æµ‹è¯•
        print("\nğŸ“‹ 2. å¹¶å‘å‹åŠ›æµ‹è¯•")
        concurrent_result = self.run_concurrent_stress_test()
        self.test_results.append(concurrent_result)

        # 3. è„šæœ¬æ€§èƒ½æµ‹è¯•
        print("\nğŸ“‹ 3. è„šæœ¬æ€§èƒ½æµ‹è¯•")
        script_result = self.run_script_performance_test()
        self.test_results.append(script_result)

        # 4. ç³»ç»Ÿèµ„æºæµ‹è¯•
        print("\nğŸ“‹ 4. ç³»ç»Ÿèµ„æºæµ‹è¯•")
        resource_result = self.run_system_resource_test()
        self.test_results.append(resource_result)

        suite_duration = time.time() - suite_start

        # ç”ŸæˆæŠ¥å‘Š
        return self.generate_validation_report(suite_duration)

    def generate_validation_report(self, suite_duration: float) -> Dict:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result.success)
        overall_success_rate = (
            (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        )

        report = {
            "validation_summary": {
                "timestamp": time.time(),
                "total_duration_seconds": suite_duration,
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": total_tests - passed_tests,
                "overall_success_rate": overall_success_rate,
                "validation_passed": overall_success_rate >= 80,  # 80%æ•´ä½“é€šè¿‡ç‡
            },
            "performance_targets": self.performance_targets,
            "detailed_results": [asdict(result) for result in self.test_results],
            "recommendations": self.generate_recommendations(),
        }

        return report

    def generate_recommendations(self) -> List[str]:
        """åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®"""
        recommendations = []

        # åˆ†æHookæ€§èƒ½
        hook_results = [r for r in self.test_results if r.test_name.startswith("hook_")]
        failed_hooks = [r for r in hook_results if not r.success]

        if failed_hooks:
            recommendations.append(f"ğŸ”§ æœ‰ {len(failed_hooks)} ä¸ªHookæœªè¾¾åˆ°æ€§èƒ½ç›®æ ‡ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")

        # åˆ†æå¹¶å‘æ€§èƒ½
        concurrent_result = next(
            (r for r in self.test_results if r.test_name == "concurrent_stress_test"),
            None,
        )

        if concurrent_result and not concurrent_result.success:
            recommendations.append("ğŸ”„ å¹¶å‘æ€§èƒ½æœªè¾¾æ ‡ï¼Œå»ºè®®è°ƒæ•´å¹¶å‘é™åˆ¶æˆ–ä¼˜åŒ–èµ„æºç®¡ç†")

        # åˆ†æèµ„æºä½¿ç”¨
        resource_result = next(
            (r for r in self.test_results if r.test_name == "system_resource_test"),
            None,
        )

        if resource_result and resource_result.metrics:
            if not resource_result.metrics.get("cpu_passed", True):
                recommendations.append(
                    f"ğŸ’» CPUä½¿ç”¨ç‡è¿‡é«˜ ({resource_result.metrics['avg_cpu_usage']:.1f}%)ï¼Œå»ºè®®ä¼˜åŒ–è®¡ç®—å¯†é›†å‹æ“ä½œ"
                )

            if not resource_result.metrics.get("memory_passed", True):
                recommendations.append(
                    f"ğŸ’¾ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({resource_result.metrics['avg_memory_usage']:.1f}%)ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†"
                )

        if not recommendations:
            recommendations.append("âœ¨ æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡éƒ½è¾¾åˆ°äº†é¢„æœŸç›®æ ‡ï¼")

        return recommendations

    def save_report(self, report: Dict, filename: str = None):
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        if filename is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"/tmp/claude_enhancer_validation_{timestamp}.json"

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        return filename

    def print_summary(self, report: Dict):
        """æ‰“å°éªŒè¯æ€»ç»“"""
        summary = report["validation_summary"]

        print("\n" + "=" * 50)
        print("ğŸ† Claude Enhancer æ€§èƒ½éªŒè¯æ€»ç»“")
        print("=" * 50)

        # æ•´ä½“ç»“æœ
        status = "âœ… é€šè¿‡" if summary["validation_passed"] else "âŒ æœªé€šè¿‡"
        print(f"ğŸ“Š æ•´ä½“ç»“æœ: {status}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['overall_success_rate']:.1f}%")
        print(f"â±ï¸ æ€»è€—æ—¶: {summary['total_duration_seconds']:.2f}ç§’")
        print(f"ğŸ§ª æµ‹è¯•æ•°é‡: {summary['passed_tests']}/{summary['total_tests']}")

        # è¯¦ç»†ç»“æœ
        print("\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for result in self.test_results:
            status_icon = "âœ…" if result.success else "âŒ"
            print(f"  {status_icon} {result.test_name}: {result.duration_ms:.0f}ms")

            if result.metrics:
                if "success_rate" in result.metrics:
                    print(f"     æˆåŠŸç‡: {result.metrics['success_rate']:.1f}%")
                if "avg_response_time" in result.metrics:
                    print(f"     å“åº”æ—¶é—´: {result.metrics['avg_response_time']:.0f}ms")

        # å»ºè®®
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for recommendation in report["recommendations"]:
            print(f"  â€¢ {recommendation}")


def main():
    validator = PerformanceValidator()

    if len(sys.argv) > 1 and sys.argv[1] == "quick":
        pass  # Auto-fixed empty block
        # å¿«é€ŸéªŒè¯æ¨¡å¼
        print("âš¡ å¿«é€ŸéªŒè¯æ¨¡å¼")
        result = validator.run_hook_performance_test(
            "optimized_performance_monitor.sh", 5
        )
        print(f"ç»“æœ: {'âœ… é€šè¿‡' if result.success else 'âŒ å¤±è´¥'}")
        if result.metrics:
            print(f"æˆåŠŸç‡: {result.metrics['success_rate']:.1f}%")
            print(f"å“åº”æ—¶é—´: {result.metrics['avg_response_time']:.0f}ms")
    else:
        pass  # Auto-fixed empty block
        # å®Œæ•´éªŒè¯å¥—ä»¶
        report = validator.run_full_validation_suite()
        validator.print_summary(report)
        report_file = validator.save_report(report)

        print(f"\nğŸ¯ ä¸‹ä¸€æ­¥:")
        print(f"  ğŸ“„ æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: cat {report_file}")
        print(f"  ğŸ”„ é‡æ–°è¿è¡ŒéªŒè¯: python3 {__file__}")
        print(f"  âš¡ å¿«é€ŸéªŒè¯: python3 {__file__} quick")


if __name__ == "__main__":
    main()
