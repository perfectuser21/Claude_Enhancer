#!/usr/bin/env python3
"""
Claude Enhancer å†…å­˜ä¼˜åŒ–å·¥å…·
å®ç°æ™ºèƒ½å†…å­˜ç®¡ç†ã€ç¼“å­˜ä¼˜åŒ–å’Œèµ„æºæ± ç®¡ç†

æ ¸å¿ƒåŠŸèƒ½:
1. å†…å­˜ä½¿ç”¨ç›‘æ§å’Œä¼˜åŒ–
2. æ™ºèƒ½åƒåœ¾å›æ”¶
3. Agentå®ä¾‹æ± ç®¡ç†
4. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
5. å†…å­˜æ³„æ¼æ£€æµ‹
"""

import gc
import time
import threading
import weakref
import psutil
import os
import sys
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, OrderedDict
import json
import pickle
import gzip
from concurrent.futures import ThreadPoolExecutor


@dataclass
class MemorySnapshot:
    """å†…å­˜å¿«ç…§æ•°æ®ç»“æ„"""
    timestamp: float
    rss_mb: float
    vms_mb: float
    percent: float
    cache_size: int
    loaded_agents: int
    thread_count: int
    file_descriptors: int


class IntelligentMemoryManager:
    """æ™ºèƒ½å†…å­˜ç®¡ç†å™¨"""

    def __init__(self, target_memory_mb: int = 50):
        self.target_memory = target_memory_mb * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
        self.process = psutil.Process()

        # å†…å­˜ç›‘æ§
        self.snapshots: List[MemorySnapshot] = []
        self.monitoring_active = False
        self.monitor_thread = None

        # æ¸…ç†ç­–ç•¥
        self.cleanup_strategies = [
            self._cleanup_caches,
            self._force_garbage_collection,
            self._clear_import_cache,
            self._compact_data_structures,
        ]

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "cleanups_performed": 0,
            "memory_saved_mb": 0.0,
            "gc_collections": 0,
            "cache_evictions": 0,
        }

    def start_monitoring(self, interval_seconds: int = 5):
        """å¯åŠ¨å†…å­˜ç›‘æ§"""
        if self.monitoring_active:
            return

        self.monitoring_active = True
        self.monitor_thread = threading.Thread(
            target=self._monitoring_loop,
            args=(interval_seconds,),
            daemon=True
        )
        self.monitor_thread.start()
        print(f"ğŸ” å¯åŠ¨å†…å­˜ç›‘æ§ (é—´éš”: {interval_seconds}ç§’)")

    def stop_monitoring(self):
        """åœæ­¢å†…å­˜ç›‘æ§"""
        self.monitoring_active = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2)

    def _monitoring_loop(self, interval: int):
        """å†…å­˜ç›‘æ§å¾ªç¯"""
        while self.monitoring_active:
            try:
                snapshot = self._capture_memory_snapshot()
                self.snapshots.append(snapshot)

                # ä¿æŒæœ€è¿‘50ä¸ªå¿«ç…§
                if len(self.snapshots) > 50:
                    self.snapshots.pop(0)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
                if snapshot.rss_mb > self.target_memory / 1024 / 1024:
                    self._trigger_cleanup(snapshot)

                time.sleep(interval)

            except Exception as e:
                print(f"âš ï¸ å†…å­˜ç›‘æ§å‡ºé”™: {e}")
                time.sleep(interval)

    def _capture_memory_snapshot(self) -> MemorySnapshot:
        """æ•è·å½“å‰å†…å­˜å¿«ç…§"""
        memory_info = self.process.memory_info()

        # è·å–æ–‡ä»¶æè¿°ç¬¦æ•°é‡
        try:
            fd_count = len(self.process.open_files())
        except:
            fd_count = 0

        return MemorySnapshot(
            timestamp=time.time(),
            rss_mb=memory_info.rss / 1024 / 1024,
            vms_mb=memory_info.vms / 1024 / 1024,
            percent=self.process.memory_percent(),
            cache_size=len(gc.get_objects()),
            loaded_agents=0,  # éœ€è¦ä»å¤–éƒ¨ä¼ å…¥
            thread_count=threading.active_count(),
            file_descriptors=fd_count,
        )

    def _trigger_cleanup(self, snapshot: MemorySnapshot):
        """è§¦å‘å†…å­˜æ¸…ç†"""
        print(f"ğŸ§¹ å†…å­˜ä½¿ç”¨è¿‡é«˜ ({snapshot.rss_mb:.1f}MB), å¼€å§‹æ¸…ç†...")

        initial_memory = snapshot.rss_mb

        for i, strategy in enumerate(self.cleanup_strategies):
            try:
                before_memory = self.process.memory_info().rss / 1024 / 1024
                strategy()
                after_memory = self.process.memory_info().rss / 1024 / 1024

                memory_saved = before_memory - after_memory
                if memory_saved > 0:
                    print(f"  âœ… ç­–ç•¥{i+1}: é‡Šæ”¾ {memory_saved:.1f}MB")
                    self.stats["memory_saved_mb"] += memory_saved

                # å¦‚æœå†…å­˜å·²ç»é™åˆ°ç›®æ ‡ä»¥ä¸‹ï¼Œåœæ­¢æ¸…ç†
                if after_memory < self.target_memory / 1024 / 1024:
                    break

            except Exception as e:
                print(f"  âŒ ç­–ç•¥{i+1} å¤±è´¥: {e}")

        final_memory = self.process.memory_info().rss / 1024 / 1024
        total_saved = initial_memory - final_memory

        if total_saved > 0:
            print(f"ğŸ¯ æ¸…ç†å®Œæˆï¼Œæ€»å…±é‡Šæ”¾ {total_saved:.1f}MB")

        self.stats["cleanups_performed"] += 1

    def _cleanup_caches(self):
        """æ¸…ç†å„ç§ç¼“å­˜"""
        # æ¸…ç†Pythonå†…éƒ¨ç¼“å­˜
        if hasattr(sys, '_getframe'):
            frame = sys._getframe()
            while frame:
                if hasattr(frame, 'f_locals'):
                    frame.f_locals.clear()
                frame = frame.f_back

        # æ¸…ç†æ¨¡å—ç¼“å­˜
        import importlib
        importlib.invalidate_caches()

    def _force_garbage_collection(self):
        """å¼ºåˆ¶åƒåœ¾å›æ”¶"""
        # å¤šè½®åƒåœ¾å›æ”¶
        for generation in range(3):
            collected = gc.collect()
            if collected > 0:
                print(f"    ğŸ—‘ï¸ GCç¬¬{generation}ä»£: æ¸…ç†äº†{collected}ä¸ªå¯¹è±¡")

        self.stats["gc_collections"] += 1

    def _clear_import_cache(self):
        """æ¸…ç†å¯¼å…¥ç¼“å­˜"""
        # æ¸…ç† __pycache__ ç›¸å…³ç¼“å­˜
        import importlib.util
        importlib.util.cache_from_source.cache_clear()

    def _compact_data_structures(self):
        """å‹ç¼©æ•°æ®ç»“æ„"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ ç‰¹å®šäºåº”ç”¨çš„æ•°æ®ç»“æ„å‹ç¼©é€»è¾‘
        pass

    def get_memory_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå†…å­˜ä½¿ç”¨æŠ¥å‘Š"""
        if not self.snapshots:
            return {"error": "æ²¡æœ‰å†…å­˜ç›‘æ§æ•°æ®"}

        current = self.snapshots[-1]

        # è®¡ç®—è¶‹åŠ¿
        if len(self.snapshots) > 5:
            recent_avg = sum(s.rss_mb for s in self.snapshots[-5:]) / 5
            older_avg = sum(s.rss_mb for s in self.snapshots[-10:-5]) / 5
            trend = "ä¸Šå‡" if recent_avg > older_avg else "ä¸‹é™"
        else:
            trend = "ç¨³å®š"

        # å†…å­˜åˆ†æ
        peak_memory = max(s.rss_mb for s in self.snapshots)
        min_memory = min(s.rss_mb for s in self.snapshots)
        avg_memory = sum(s.rss_mb for s in self.snapshots) / len(self.snapshots)

        return {
            "current_status": {
                "rss_mb": current.rss_mb,
                "percent": current.percent,
                "trend": trend,
                "health": "è‰¯å¥½" if current.rss_mb < self.target_memory/1024/1024 else "éœ€è¦å…³æ³¨",
            },
            "statistics": {
                "peak_memory_mb": peak_memory,
                "min_memory_mb": min_memory,
                "avg_memory_mb": avg_memory,
                "memory_range_mb": peak_memory - min_memory,
            },
            "system_info": {
                "thread_count": current.thread_count,
                "file_descriptors": current.file_descriptors,
                "cache_objects": current.cache_size,
            },
            "optimization_stats": self.stats,
            "recommendations": self._generate_recommendations(current),
        }

    def _generate_recommendations(self, snapshot: MemorySnapshot) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        if snapshot.rss_mb > self.target_memory / 1024 / 1024 * 1.5:
            recommendations.append("å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå»ºè®®ç«‹å³ä¼˜åŒ–")

        if snapshot.thread_count > 20:
            recommendations.append("çº¿ç¨‹æ•°é‡è¿‡å¤šï¼Œå»ºè®®ä½¿ç”¨çº¿ç¨‹æ± ")

        if snapshot.file_descriptors > 100:
            recommendations.append("æ–‡ä»¶æè¿°ç¬¦è¿‡å¤šï¼Œæ£€æŸ¥æ˜¯å¦æœ‰èµ„æºæ³„æ¼")

        if len(self.snapshots) > 10:
            recent_growth = self.snapshots[-1].rss_mb - self.snapshots[-10].rss_mb
            if recent_growth > 20:
                recommendations.append("æ£€æµ‹åˆ°å†…å­˜æŒç»­å¢é•¿ï¼Œå¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼")

        if not recommendations:
            recommendations.append("å†…å­˜ä½¿ç”¨çŠ¶å†µè‰¯å¥½")

        return recommendations


class AgentInstancePool:
    """Agentå®ä¾‹æ± ç®¡ç†å™¨"""

    def __init__(self, max_instances: int = 20):
        self.max_instances = max_instances
        self.pool: Dict[str, List[weakref.ref]] = defaultdict(list)
        self.active_instances: Dict[str, weakref.ref] = {}
        self.creation_count = 0
        self.reuse_count = 0
        self.lock = threading.RLock()

    def get_agent_instance(self, agent_name: str, factory_func) -> Any:
        """è·å–Agentå®ä¾‹ï¼ˆå¤ç”¨æˆ–åˆ›å»ºï¼‰"""
        with self.lock:
            # å°è¯•ä»æ± ä¸­è·å–å¯ç”¨å®ä¾‹
            if agent_name in self.pool:
                while self.pool[agent_name]:
                    weak_ref = self.pool[agent_name].pop()
                    instance = weak_ref()
                    if instance is not None:
                        self.active_instances[f"{agent_name}_{id(instance)}"] = weak_ref
                        self.reuse_count += 1
                        return instance

            # æ± ä¸­æ²¡æœ‰å¯ç”¨å®ä¾‹ï¼Œåˆ›å»ºæ–°çš„
            if len(self.active_instances) < self.max_instances:
                instance = factory_func()
                weak_ref = weakref.ref(instance, self._cleanup_callback)
                self.active_instances[f"{agent_name}_{id(instance)}"] = weak_ref
                self.creation_count += 1
                return instance

            # å®ä¾‹æ•°è¾¾åˆ°ä¸Šé™ï¼Œå¼ºåˆ¶å›æ”¶æœ€è€çš„å®ä¾‹
            self._force_cleanup()
            return self.get_agent_instance(agent_name, factory_func)

    def return_agent_instance(self, agent_name: str, instance: Any):
        """å½’è¿˜Agentå®ä¾‹åˆ°æ± ä¸­"""
        with self.lock:
            instance_key = f"{agent_name}_{id(instance)}"
            if instance_key in self.active_instances:
                weak_ref = self.active_instances.pop(instance_key)

                # å°†å®ä¾‹æ”¾å›æ± ä¸­ä¾›å¤ç”¨
                if len(self.pool[agent_name]) < 5:  # æ¯ç§Agentæœ€å¤šç¼“å­˜5ä¸ªå®ä¾‹
                    self.pool[agent_name].append(weak_ref)

    def _cleanup_callback(self, weak_ref):
        """å®ä¾‹è¢«åƒåœ¾å›æ”¶æ—¶çš„å›è°ƒ"""
        with self.lock:
            # ä»æ´»åŠ¨å®ä¾‹ä¸­ç§»é™¤
            keys_to_remove = []
            for key, ref in self.active_instances.items():
                if ref is weak_ref:
                    keys_to_remove.append(key)

            for key in keys_to_remove:
                self.active_instances.pop(key, None)

    def _force_cleanup(self):
        """å¼ºåˆ¶æ¸…ç†ä¸€äº›å®ä¾‹"""
        with self.lock:
            # æ¸…ç†æ± ä¸­çš„å¼±å¼•ç”¨
            for agent_name, refs in list(self.pool.items()):
                self.pool[agent_name] = [ref for ref in refs if ref() is not None]

            # å¦‚æœè¿˜æ˜¯å¤ªå¤šï¼Œç›´æ¥æ¸…ç†ä¸€éƒ¨åˆ†
            if len(self.active_instances) >= self.max_instances:
                keys_to_remove = list(self.active_instances.keys())[:5]
                for key in keys_to_remove:
                    self.active_instances.pop(key, None)

    def get_pool_stats(self) -> Dict[str, Any]:
        """è·å–æ± ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            pool_sizes = {name: len(refs) for name, refs in self.pool.items()}
            active_count = len(self.active_instances)

            return {
                "creation_count": self.creation_count,
                "reuse_count": self.reuse_count,
                "active_instances": active_count,
                "pool_sizes": pool_sizes,
                "reuse_rate": f"{(self.reuse_count / max(1, self.creation_count + self.reuse_count)) * 100:.1f}%",
                "max_instances": self.max_instances,
            }


class CacheOptimizer:
    """ç¼“å­˜ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.caches: Dict[str, Any] = {}
        self.access_counts: Dict[str, int] = defaultdict(int)
        self.last_access: Dict[str, float] = {}
        self.cache_sizes: Dict[str, int] = defaultdict(int)

    def register_cache(self, name: str, cache_obj: Any):
        """æ³¨å†Œéœ€è¦ä¼˜åŒ–çš„ç¼“å­˜"""
        self.caches[name] = cache_obj
        print(f"ğŸ“‹ æ³¨å†Œç¼“å­˜: {name}")

    def optimize_all_caches(self):
        """ä¼˜åŒ–æ‰€æœ‰æ³¨å†Œçš„ç¼“å­˜"""
        print("ğŸ”§ å¼€å§‹ç¼“å­˜ä¼˜åŒ–...")

        for name, cache in self.caches.items():
            try:
                before_size = self._get_cache_size(cache)
                self._optimize_cache(name, cache)
                after_size = self._get_cache_size(cache)

                if before_size > after_size:
                    saved = before_size - after_size
                    print(f"  âœ… {name}: å‡å°‘ {saved} ä¸ªæ¡ç›®")

            except Exception as e:
                print(f"  âŒ {name} ä¼˜åŒ–å¤±è´¥: {e}")

    def _get_cache_size(self, cache: Any) -> int:
        """è·å–ç¼“å­˜å¤§å°"""
        if hasattr(cache, '__len__'):
            return len(cache)
        elif hasattr(cache, 'cache_info'):  # functools.lru_cache
            return cache.cache_info().currsize
        return 0

    def _optimize_cache(self, name: str, cache: Any):
        """ä¼˜åŒ–å•ä¸ªç¼“å­˜"""
        if hasattr(cache, 'clear'):
            # å¦‚æœæ˜¯å­—å…¸ç±»å‹çš„ç¼“å­˜
            if isinstance(cache, dict):
                self._optimize_dict_cache(cache)
            else:
                # å¯¹äºå…¶ä»–ç±»å‹ï¼Œç›´æ¥æ¸…ç†ä¸€åŠ
                if hasattr(cache, '__len__') and len(cache) > 100:
                    cache.clear()

    def _optimize_dict_cache(self, cache: dict):
        """ä¼˜åŒ–å­—å…¸ç±»å‹ç¼“å­˜"""
        if len(cache) <= 50:
            return

        # åŸºäºLRUç­–ç•¥ä¿ç•™æœ€æ–°çš„50%
        current_time = time.time()
        items = list(cache.items())

        # æŒ‰è®¿é—®æ—¶é—´æ’åºï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        sorted_items = sorted(items, key=lambda x: hash(x[0]))  # ç®€å•æ’åº

        # ä¿ç•™å‰50%
        keep_count = len(items) // 2
        cache.clear()

        for key, value in sorted_items[:keep_count]:
            cache[key] = value


def run_memory_optimization_demo():
    """è¿è¡Œå†…å­˜ä¼˜åŒ–æ¼”ç¤º"""
    print("ğŸ§  Claude Enhancer å†…å­˜ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºå†…å­˜ç®¡ç†å™¨
    memory_manager = IntelligentMemoryManager(target_memory_mb=40)

    # å¯åŠ¨ç›‘æ§
    memory_manager.start_monitoring(interval_seconds=2)

    # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨
    print("\nğŸ“ˆ æ¨¡æ‹Ÿå†…å­˜å¯†é›†å‹æ“ä½œ...")

    # åˆ›å»ºä¸€äº›å†…å­˜æ¶ˆè€—
    large_data = []
    for i in range(10):
        data = [j * i for j in range(100000)]  # åˆ›å»ºå¤§é‡æ•°æ®
        large_data.append(data)
        time.sleep(0.5)

    # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç›‘æ§å™¨å·¥ä½œ
    time.sleep(5)

    # æ˜¾ç¤ºå†…å­˜æŠ¥å‘Š
    report = memory_manager.get_memory_report()
    print("\nğŸ“Š å†…å­˜ä½¿ç”¨æŠ¥å‘Š:")
    print(json.dumps(report, indent=2, ensure_ascii=False))

    # æ¸…ç†å¤§æ•°æ®
    del large_data
    gc.collect()

    # å†ç­‰ä¸€æ®µæ—¶é—´
    time.sleep(3)

    # æœ€ç»ˆæŠ¥å‘Š
    final_report = memory_manager.get_memory_report()
    print("\nğŸ“Š ä¼˜åŒ–åå†…å­˜æŠ¥å‘Š:")
    print(f"å½“å‰å†…å­˜: {final_report['current_status']['rss_mb']:.1f}MB")
    print(f"ä¼˜åŒ–çŠ¶æ€: {final_report['current_status']['health']}")
    print(f"æ€»æ¸…ç†æ¬¡æ•°: {final_report['optimization_stats']['cleanups_performed']}")
    print(f"èŠ‚çœå†…å­˜: {final_report['optimization_stats']['memory_saved_mb']:.1f}MB")

    # åœæ­¢ç›‘æ§
    memory_manager.stop_monitoring()

    # Agentæ± æ¼”ç¤º
    print("\nğŸ”„ Agentå®ä¾‹æ± æ¼”ç¤º...")

    agent_pool = AgentInstancePool(max_instances=5)

    # æ¨¡æ‹ŸAgentåˆ›å»ºå’Œå¤ç”¨
    def create_mock_agent():
        return {"id": time.time(), "data": [1, 2, 3, 4, 5]}

    # åˆ›å»ºå’Œå½’è¿˜ä¸€äº›Agentå®ä¾‹
    agents = []
    for i in range(8):  # è¶…è¿‡æ± çš„æœ€å¤§å®¹é‡
        agent = agent_pool.get_agent_instance("test-agent", create_mock_agent)
        agents.append(agent)

    # å½’è¿˜éƒ¨åˆ†Agent
    for agent in agents[:3]:
        agent_pool.return_agent_instance("test-agent", agent)

    # å†æ¬¡è·å–ï¼ˆåº”è¯¥å¤ç”¨ä¹‹å‰çš„å®ä¾‹ï¼‰
    reused_agents = []
    for i in range(3):
        agent = agent_pool.get_agent_instance("test-agent", create_mock_agent)
        reused_agents.append(agent)

    pool_stats = agent_pool.get_pool_stats()
    print("ğŸ“Š Agentæ± ç»Ÿè®¡:")
    print(json.dumps(pool_stats, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "demo":
        run_memory_optimization_demo()
    else:
        # ç®€å•çš„å†…å­˜æ£€æŸ¥
        manager = IntelligentMemoryManager()
        snapshot = manager._capture_memory_snapshot()
        print(f"å½“å‰å†…å­˜ä½¿ç”¨: {snapshot.rss_mb:.1f}MB")
        print(f"å†…å­˜å ç”¨ç‡: {snapshot.percent:.1f}%")
        print(f"æ´»è·ƒçº¿ç¨‹æ•°: {snapshot.thread_count}")

        if snapshot.rss_mb > 100:
            print("âš ï¸ å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®è¿è¡Œä¼˜åŒ–: python3 memory_optimizer.py demo")