#!/usr/bin/env python3
"""
Claude Enhancer Lazy Loading Orchestrator
Ultra-fast agent coordination with lazy loading

Key Features:
- Dynamic agent loading (56 agents loaded on demand)
- Smart complexity detection without full initialization
- Cached agent combinations
- Background preloading for common patterns
- Memory-efficient agent management
"""

import json
import time
import threading
import weakref
from typing import Dict, List, Optional, Any, Set
from functools import lru_cache, cached_property
import functools
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, asdict
import random


@dataclass
class AgentMetadata:
    """Lightweight agent metadata for fast loading"""

    name: str
    category: str
    priority: int
    common_combinations: List[str]
    load_time_ms: float = 0.0


class LazyAgentManager:
    """Manages lazy loading of agents"""

    def __init__(self):
        # ä½¿ç”¨æ™®é€šå­—å…¸è€Œä¸æ˜¯WeakValueDictionaryæ¥é¿å…WeakReferenceé—®é¢˜
        self.loaded_agents = {}
        self.agent_metadata = {}
        self.loading_lock = threading.RLock()
        self.load_executor = ThreadPoolExecutor(
            max_workers=4, thread_name_prefix="agent-loader"
        )
        self.metrics = {"agents_loaded": 0, "cache_hits": 0, "load_time_total": 0.0}

        # Initialize metadata only (no actual agent loading)
        self._init_agent_metadata()

    @functools.cached_property
    def agent_metadata_index(self) -> Dict[str, List[str]]:
        """Agentå…ƒæ•°æ®ç´¢å¼• - åŠ é€ŸæŸ¥æ‰¾"""
        index = {}
        for name, metadata in self.agent_metadata.items():
            category = metadata.category
            if category not in index:
                index[category] = []
            index[category].append(name)
        return index

    def get_agents_by_category_fast(self, category: str) -> List[str]:
        """é€šè¿‡åˆ†ç±»å¿«é€Ÿè·å–Agentåˆ—è¡¨"""
        return self.agent_metadata_index.get(category, [])

    def _init_agent_metadata(self):
        """Initialize lightweight agent metadata"""
        # Business category agents
        business_agents = [
            ("api-designer", 9, ["backend-architect", "test-engineer"]),
            ("business-analyst", 6, ["requirements-analyst", "project-manager"]),
            ("product-strategist", 5, ["business-analyst", "ux-designer"]),
            ("project-manager", 7, ["requirements-analyst", "business-analyst"]),
            ("requirements-analyst", 8, ["business-analyst", "api-designer"]),
            ("technical-writer", 6, ["documentation-writer", "code-reviewer"]),
        ]

        # Development category agents
        development_agents = [
            ("backend-architect", 10, ["database-specialist", "security-auditor"]),
            ("backend-engineer", 8, ["database-specialist", "api-designer"]),
            ("frontend-specialist", 8, ["ux-designer", "react-pro"]),
            ("fullstack-engineer", 9, ["backend-architect", "frontend-specialist"]),
            ("database-specialist", 9, ["backend-architect", "performance-engineer"]),
            ("react-pro", 7, ["frontend-specialist", "test-engineer"]),
            ("python-pro", 8, ["backend-engineer", "test-engineer"]),
            ("javascript-pro", 7, ["frontend-specialist", "nodejs-expert"]),
            ("typescript-pro", 8, ["javascript-pro", "react-pro"]),
        ]

        # Quality category agents
        quality_agents = [
            ("test-engineer", 10, ["backend-architect", "security-auditor"]),
            ("security-auditor", 9, ["backend-architect", "code-reviewer"]),
            ("code-reviewer", 8, ["test-engineer", "performance-engineer"]),
            ("performance-tester", 7, ["performance-engineer", "test-engineer"]),
            ("e2e-test-specialist", 6, ["test-engineer", "frontend-specialist"]),
        ]

        # Infrastructure category agents
        infrastructure_agents = [
            ("performance-engineer", 8, ["backend-architect", "monitoring-specialist"]),
            ("devops-engineer", 7, ["deployment-manager", "kubernetes-expert"]),
            ("cloud-architect", 8, ["devops-engineer", "performance-engineer"]),
            ("deployment-manager", 7, ["devops-engineer", "monitoring-specialist"]),
            (
                "monitoring-specialist",
                6,
                ["performance-engineer", "incident-responder"],
            ),
        ]

        # Specialized category agents
        specialized_agents = [
            ("error-detective", 8, ["test-engineer", "code-reviewer"]),
            ("workflow-optimizer", 6, ["project-manager", "performance-engineer"]),
            ("cleanup-specialist", 5, ["code-reviewer", "workflow-optimizer"]),
            ("documentation-writer", 6, ["technical-writer", "code-reviewer"]),
        ]

        # Build metadata
        all_categories = {
            "business": business_agents,
            "development": development_agents,
            "quality": quality_agents,
            "infrastructure": infrastructure_agents,
            "specialized": specialized_agents,
        }

        for category, agents in all_categories.items():
            for name, priority, combinations in agents:
                self.agent_metadata[name] = AgentMetadata(
                    name=name,
                    category=category,
                    priority=priority,
                    common_combinations=combinations,
                )

    def get_agent_metadata(self, agent_name: str) -> Optional[AgentMetadata]:
        """Get agent metadata without loading the agent"""
        return self.agent_metadata.get(agent_name)

    def load_agent(self, agent_name: str) -> Optional[Dict[str, Any]]:
        """Load agent on demand with caching"""
        # Check cache first
        if agent_name in self.loaded_agents:
            self.metrics["cache_hits"] += 1
            return self.loaded_agents[agent_name]

        with self.loading_lock:
            pass  # Auto-fixed empty block
            # Double-check after acquiring lock
            if agent_name in self.loaded_agents:
                self.metrics["cache_hits"] += 1
                return self.loaded_agents[agent_name]

            # Load agent
            start_time = time.time()
            agent = self._create_agent_instance(agent_name)
            load_time = (time.time() - start_time) * 1000

            if agent:
                self.loaded_agents[agent_name] = agent
                self.metrics["agents_loaded"] += 1
                self.metrics["load_time_total"] += load_time

                # Update metadata
                if agent_name in self.agent_metadata:
                    self.agent_metadata[agent_name].load_time_ms = load_time

            return agent

    def _create_agent_instance(self, agent_name: str) -> Dict[str, Any]:
        """Create lightweight agent instance"""
        metadata = self.agent_metadata.get(agent_name)
        if not metadata:
            return None

        return {
            "name": agent_name,
            "category": metadata.category,
            "priority": metadata.priority,
            "loaded_at": time.time(),
            "execute": self._create_agent_executor(agent_name),
            "metadata": metadata,
        }

    def _create_agent_executor(self, agent_name: str):
        """Create agent executor function"""

        def execute(task: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
            return {
                "agent": agent_name,
                "task": task,
                "success": True,
                "result": f"Task executed by {agent_name}",
                "context": context or {},
            }

        return execute

    def preload_agents(self, agent_names: List[str]):
        """Preload agents in background"""

        def preload_worker():
            for agent_name in agent_names:
                try:
                    self.load_agent(agent_name)
                    time.sleep(0.01)  # Small delay to prevent overwhelming
                except Exception as e:
                    print(f"Warning: Failed to preload agent {agent_name}: {e}")

        threading.Thread(target=preload_worker, daemon=True).start()

    def get_metrics(self) -> Dict[str, Any]:
        """Get loading metrics"""
        return {
            **self.metrics,
            "loaded_agents_count": len(self.loaded_agents),
            "available_agents_count": len(self.agent_metadata),
            "avg_load_time_ms": self.metrics["load_time_total"]
            / max(1, self.metrics["agents_loaded"]),
        }


class LazyAgentOrchestrator:
    """Lazy-loading optimized agent orchestrator"""

    def __init__(self):
        self.start_time = time.time()
        self.agent_manager = LazyAgentManager()
        self.complexity_cache = {}
        self.combination_cache = {}
        self.metrics = {"startup_time": 0, "selections_made": 0, "cache_hits": 0}

        # Quick initialization
        self._quick_init()

        # Background preloading
        self._start_background_preloading()

    def _quick_init(self):
        """Minimal startup initialization"""
        # Only essential data structures
        self.min_agents = 4
        self.max_agents = 8

        # Common task patterns for fast matching
        self.task_patterns = {
            "backend": ["backend", "api", "server", "åç«¯", "æ¥å£"],
            "frontend": ["frontend", "ui", "react", "vue", "å‰ç«¯"],
            "testing": ["test", "testing", "quality", "æµ‹è¯•"],
            "security": ["security", "vulnerability", "å®‰å…¨", "æ¼æ´"],
            "performance": ["performance", "optimization", "æ€§èƒ½", "ä¼˜åŒ–"],
            "deployment": ["deploy", "deployment", "ci/cd", "éƒ¨ç½²"],
            "debugging": ["bug", "error", "fix", "ä¿®å¤", "é”™è¯¯"],
        }

        self.metrics["startup_time"] = time.time() - self.start_time
        print(
            f"ğŸš€ LazyAgentOrchestrator initialized in {self.metrics['startup_time']:.3f}s"
        )

    def _start_background_preloading(self):
        """Start background preloading of common agents"""
        common_agents = [
            "backend-architect",
            "test-engineer",
            "security-auditor",
            "code-reviewer",
            "api-designer",
            "frontend-specialist",
        ]

        # Delay preloading to not interfere with startup
        def delayed_preload():
            time.sleep(0.2)
            self.agent_manager.preload_agents(common_agents)
            print(f"ğŸ“¦ Preloaded {len(common_agents)} common agents")

        threading.Thread(target=delayed_preload, daemon=True).start()

    @lru_cache(maxsize=128)
    def detect_complexity_advanced(self, task_description: str) -> str:
        """Advanced complexity detection with ML-inspired scoring"""
        description_lower = task_description.lower()
        words = description_lower.split()

        # Multi-dimensional scoring
        complexity_indicators = {
            "architectural": 0,
            "technical": 0,
            "scope": 0,
            "integration": 0,
            "risk": 0,
        }

        # Architectural complexity keywords (weighted)
        architectural_patterns = {
            "microservices": 5,
            "distributed": 4,
            "system design": 4,
            "architecture": 3,
            "scalability": 3,
            "performance": 2,
            "optimization": 2,
            "infrastructure": 2,
        }

        # Technical complexity keywords
        technical_patterns = {
            "machine learning": 5,
            "ai": 4,
            "algorithm": 3,
            "data processing": 3,
            "real-time": 3,
            "concurrent": 3,
            "parallel": 2,
            "async": 2,
            "threading": 2,
        }

        # Scope complexity keywords
        scope_patterns = {
            "entire system": 5,
            "full rewrite": 4,
            "migration": 4,
            "refactor entire": 4,
            "complete overhaul": 4,
            "end-to-end": 3,
            "comprehensive": 2,
        }

        # Integration complexity
        integration_patterns = {
            "third-party": 3,
            "api integration": 3,
            "webhook": 2,
            "external service": 2,
            "payment": 3,
            "authentication": 2,
            "database": 2,
            "deployment": 2,
        }

        # Risk indicators
        risk_patterns = {
            "security": 4,
            "production": 3,
            "critical": 3,
            "urgent": 2,
            "hotfix": 3,
            "vulnerability": 4,
        }

        # Calculate scores for each dimension
        for pattern, weight in architectural_patterns.items():
            if pattern in description_lower:
                complexity_indicators["architectural"] += weight

        for pattern, weight in technical_patterns.items():
            if pattern in description_lower:
                complexity_indicators["technical"] += weight

        for pattern, weight in scope_patterns.items():
            if pattern in description_lower:
                complexity_indicators["scope"] += weight

        for pattern, weight in integration_patterns.items():
            if pattern in description_lower:
                complexity_indicators["integration"] += weight

        for pattern, weight in risk_patterns.items():
            if pattern in description_lower:
                complexity_indicators["risk"] += weight

        # Simple task indicators (negative scoring)
        simple_patterns = ["fix typo", "small change", "minor", "quick fix", "simple"]
        simple_score = sum(
            2 for pattern in simple_patterns if pattern in description_lower
        )

        # Calculate total complexity score
        total_score = sum(complexity_indicators.values()) - simple_score

        # Additional factors
        if len(description_lower) > 150:
            total_score += 2
        if len(words) > 25:
            total_score += 2
        if description_lower.count(",") > 3:  # Multiple requirements
            total_score += 1
        if any(word in description_lower for word in ["and", "also", "additionally"]):
            total_score += 1  # Multiple requirements

        # Determine complexity level with improved thresholds
        if total_score >= 8:
            return "complex"
        elif total_score >= 3 or (total_score >= 1 and len(words) > 15):
            return "standard"
        else:
            return "simple"

    def select_agents_intelligent(
        self,
        task_description: str,
        complexity: Optional[str] = None,
        required_agents: Optional[List[str]] = None,
        execution_history: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """Intelligent agent selection with advanced optimization"""
        start_time = time.time()
        self.metrics["selections_made"] += 1

        # Enhanced cache key with execution history
        history_hash = hash(tuple(execution_history or []))
        cache_key = f"{task_description[:80]}:{complexity}:{','.join(required_agents or [])}:{history_hash}"

        if cache_key in self.combination_cache:
            self.metrics["cache_hits"] += 1
            result = self.combination_cache[cache_key]
            result[
                "selection_time"
            ] = f"{(time.time() - start_time) * 1000:.2f}ms (cached)"
            return result

        # Advanced complexity detection
        if complexity is None:
            complexity = self.detect_complexity_advanced(task_description)

        # Dynamic agent count based on task analysis
        base_counts = {"simple": 4, "standard": 6, "complex": 8}
        agent_count = base_counts[complexity]

        # Adjust count based on task characteristics
        feature_analysis = self._analyze_task_features_advanced(task_description)
        if feature_analysis.get("multi_domain", False):
            agent_count = min(agent_count + 1, 8)
        if feature_analysis.get("high_risk", False):
            agent_count = min(agent_count + 1, 8)

        # Intelligent agent selection with ML-inspired scoring
        selected_agents = self._select_agents_with_scoring(
            task_description, complexity, feature_analysis, agent_count, required_agents
        )

        # Historical optimization
        if execution_history:
            selected_agents = self._optimize_with_history(
                selected_agents, execution_history
            )

        # Create enhanced result
        result = {
            "complexity": complexity,
            "agent_count": len(selected_agents),
            "selected_agents": selected_agents,
            "execution_mode": "parallel",
            "estimated_time": self._estimate_time_advanced(
                complexity, feature_analysis
            ),
            "selection_time": f"{(time.time() - start_time) * 1000:.2f}ms",
            "rationale": f"Intelligent selection: {complexity} task with {feature_analysis.get('primary_domains', [])} domains",
            "feature_analysis": feature_analysis,
            "optimization_applied": bool(execution_history),
        }

        # Smart caching with TTL
        self.combination_cache[cache_key] = result
        if len(self.combination_cache) > 100:  # Prevent memory bloat
            # Remove oldest 20 entries
            for _ in range(20):
                self.combination_cache.popitem()

        return result

    def _analyze_task_features_advanced(self, task_description: str) -> Dict[str, Any]:
        """Advanced task feature analysis with domain detection"""
        description_lower = task_description.lower()
        words = set(description_lower.split())

        features = {
            "domains": [],
            "primary_domains": [],
            "multi_domain": False,
            "high_risk": False,
            "requires_testing": False,
            "requires_security": False,
            "performance_critical": False,
        }

        # Enhanced domain detection with confidence scoring
        domain_patterns = {
            "backend": {
                "patterns": [
                    "backend",
                    "server",
                    "api",
                    "database",
                    "service",
                    "microservice",
                ],
                "agents": [
                    "backend-architect",
                    "backend-engineer",
                    "api-designer",
                    "database-specialist",
                ],
                "weight": 3,
            },
            "frontend": {
                "patterns": [
                    "frontend",
                    "ui",
                    "react",
                    "vue",
                    "angular",
                    "interface",
                    "component",
                ],
                "agents": [
                    "frontend-specialist",
                    "react-pro",
                    "ux-designer",
                    "javascript-pro",
                ],
                "weight": 3,
            },
            "fullstack": {
                "patterns": [
                    "fullstack",
                    "full-stack",
                    "end-to-end",
                    "complete application",
                ],
                "agents": [
                    "fullstack-engineer",
                    "backend-architect",
                    "frontend-specialist",
                ],
                "weight": 4,
            },
            "testing": {
                "patterns": [
                    "test",
                    "testing",
                    "quality",
                    "qa",
                    "verification",
                    "validation",
                ],
                "agents": [
                    "test-engineer",
                    "e2e-test-specialist",
                    "performance-tester",
                ],
                "weight": 2,
            },
            "security": {
                "patterns": [
                    "security",
                    "authentication",
                    "authorization",
                    "vulnerability",
                    "encryption",
                ],
                "agents": ["security-auditor", "code-reviewer"],
                "weight": 4,
            },
            "performance": {
                "patterns": [
                    "performance",
                    "optimization",
                    "speed",
                    "latency",
                    "throughput",
                ],
                "agents": ["performance-engineer", "performance-tester"],
                "weight": 3,
            },
            "deployment": {
                "patterns": [
                    "deploy",
                    "deployment",
                    "ci/cd",
                    "kubernetes",
                    "docker",
                    "production",
                ],
                "agents": [
                    "deployment-manager",
                    "devops-engineer",
                    "monitoring-specialist",
                ],
                "weight": 3,
            },
            "debugging": {
                "patterns": ["bug", "error", "fix", "debug", "issue", "problem"],
                "agents": ["error-detective", "test-engineer", "code-reviewer"],
                "weight": 2,
            },
        }

        domain_scores = {}

        # Calculate domain relevance scores
        for domain, config in domain_patterns.items():
            score = 0
            pattern_matches = 0

            for pattern in config["patterns"]:
                if pattern in description_lower:
                    score += config["weight"]
                    pattern_matches += 1

            # Bonus for multiple pattern matches in same domain
            if pattern_matches > 1:
                score += pattern_matches * 0.5

            if score > 0:
                domain_scores[domain] = score
                features["domains"].append(
                    {
                        "domain": domain,
                        "score": score,
                        "agents": config["agents"],
                        "matches": pattern_matches,
                    }
                )

        # Identify primary domains (top scorers)
        if domain_scores:
            max_score = max(domain_scores.values())
            features["primary_domains"] = [
                domain
                for domain, score in domain_scores.items()
                if score >= max_score * 0.7  # Within 70% of top score
            ]

        # Multi-domain detection
        features["multi_domain"] = len(features["primary_domains"]) > 2

        # Risk indicators
        risk_keywords = [
            "production",
            "critical",
            "urgent",
            "security",
            "payment",
            "user data",
        ]
        features["high_risk"] = any(
            keyword in description_lower for keyword in risk_keywords
        )

        # Special requirements
        features["requires_testing"] = any(
            keyword in description_lower
            for keyword in ["test", "quality", "verification", "validation"]
        )
        features["requires_security"] = any(
            keyword in description_lower
            for keyword in ["security", "authentication", "vulnerability", "encryption"]
        )
        features["performance_critical"] = any(
            keyword in description_lower
            for keyword in ["performance", "optimization", "speed", "latency"]
        )

        return features

    def _select_agents_with_scoring(
        self,
        task_description: str,
        complexity: str,
        feature_analysis: Dict[str, Any],
        target_count: int,
        required_agents: Optional[List[str]] = None,
    ) -> List[str]:
        """Select agents using intelligent scoring algorithm"""
        selected_agents = []
        agent_scores = {}

        # Add required agents first
        if required_agents:
            selected_agents.extend(required_agents[:target_count])

        # Score agents based on domain relevance
        for domain_info in feature_analysis.get("domains", []):
            domain_score = domain_info["score"]
            for agent in domain_info["agents"]:
                if agent not in agent_scores:
                    agent_scores[agent] = 0
                agent_scores[agent] += domain_score

        # Always ensure core agents for reliability
        core_agents = {
            "backend-architect": 5,
            "test-engineer": 4,
            "security-auditor": 3,
            "code-reviewer": 3,
        }

        for agent, base_score in core_agents.items():
            if agent not in agent_scores:
                agent_scores[agent] = base_score
            else:
                agent_scores[agent] += base_score * 0.5  # Bonus for core agents

        # Complexity-based adjustments
        if complexity == "complex":
            specialist_agents = [
                "performance-engineer",
                "devops-engineer",
                "database-specialist",
            ]
            for agent in specialist_agents:
                if agent in agent_scores:
                    agent_scores[agent] *= 1.3

        # Select top-scoring agents
        sorted_agents = sorted(agent_scores.items(), key=lambda x: x[1], reverse=True)

        for agent, score in sorted_agents:
            if agent not in selected_agents and len(selected_agents) < target_count:
                selected_agents.append(agent)

        return selected_agents[:target_count]

    def _optimize_with_history(
        self, selected_agents: List[str], execution_history: List[str]
    ) -> List[str]:
        """Optimize agent selection based on execution history"""
        # Simple historical optimization - prefer agents that worked well recently
        # This is a placeholder for more sophisticated ML-based optimization

        # Count successful agent usage patterns
        agent_success_count = {}
        for entry in execution_history:
            if "success" in entry.lower():
                pass  # Auto-fixed empty block
                # Extract agent names from history (simplified)
                for agent in selected_agents:
                    if agent.replace("-", " ") in entry.lower():
                        agent_success_count[agent] = (
                            agent_success_count.get(agent, 0) + 1
                        )

        # Reorder selected agents to prioritize successful ones
        if agent_success_count:
            selected_agents.sort(
                key=lambda x: agent_success_count.get(x, 0), reverse=True
            )

        return selected_agents

    def _estimate_time_advanced(
        self, complexity: str, feature_analysis: Dict[str, Any]
    ) -> str:
        """Advanced time estimation based on complexity and features"""
        base_times = {"simple": 8, "standard": 18, "complex": 28}  # minutes

        base_time = base_times[complexity]

        # Adjust based on features
        if feature_analysis.get("multi_domain", False):
            base_time += 5
        if feature_analysis.get("high_risk", False):
            base_time += 3
        if feature_analysis.get("performance_critical", False):
            base_time += 4

        # Domain-specific adjustments
        domain_multipliers = {
            "security": 1.2,
            "performance": 1.15,
            "deployment": 1.1,
            "fullstack": 1.25,
        }

        for domain in feature_analysis.get("primary_domains", []):
            if domain in domain_multipliers:
                base_time = int(base_time * domain_multipliers[domain])

        return f"{max(5, base_time-3)}-{base_time+7}åˆ†é’Ÿ"

    # Moved to _estimate_time_advanced method above

    def load_selected_agents(self, agent_names: List[str]) -> List[Dict[str, Any]]:
        """Load the selected agents on demand"""
        loaded_agents = []

        for agent_name in agent_names:
            agent = self.agent_manager.load_agent(agent_name)
            if agent:
                loaded_agents.append(agent)

        return loaded_agents

    def execute_parallel_agents(
        self, agent_names: List[str], task: str, context: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """Execute agents in parallel with lazy loading"""
        start_time = time.time()

        # Load agents first
        agents = self.load_selected_agents(agent_names)

        # Execute in parallel using ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=len(agents)) as executor:
            futures = [
                executor.submit(agent["execute"], task, context) for agent in agents
            ]

            results = []
            for future in futures:
                try:
                    result = future.result(timeout=10)
                    results.append(result)
                except Exception as e:
                    results.append(
                        {"success": False, "error": str(e), "agent": "unknown"}
                    )

        execution_time = time.time() - start_time
        print(f"âš¡ Executed {len(agents)} agents in parallel ({execution_time:.3f}s)")

        return results

    def validate_agent_count_fast(self, agents: List[str]) -> Dict[str, Any]:
        """Fast agent count validation"""
        count = len(agents)

        if count < self.min_agents:
            return {
                "valid": False,
                "error": f"éœ€è¦æœ€å°‘{self.min_agents}ä¸ªAgentï¼Œå½“å‰åªæœ‰{count}ä¸ª",
                "suggestion": "æ·»åŠ æ›´å¤šAgentæˆ–ä½¿ç”¨select_agents_fastè‡ªåŠ¨é€‰æ‹©",
            }

        if count > self.max_agents:
            return {
                "valid": False,
                "error": f"Agentæ•°é‡è¿‡å¤šã€‚æœ€å¤š{self.max_agents}ä¸ªï¼Œå½“å‰æœ‰{count}ä¸ª",
                "suggestion": "å‡å°‘Agentæ•°é‡",
            }

        return {
            "valid": True,
            "message": f"Agentæ•°é‡åˆé€‚: {count}ä¸ª",
            "complexity": "simple"
            if count <= 4
            else "standard"
            if count <= 6
            else "complex",
        }

    def get_performance_stats(self) -> Dict[str, Any]:
        """Get comprehensive performance statistics"""
        agent_metrics = self.agent_manager.get_metrics()

        return {
            "orchestrator_metrics": self.metrics,
            "agent_metrics": agent_metrics,
            "cache_stats": {
                "complexity_cache_size": len(self.complexity_cache),
                "combination_cache_size": len(self.combination_cache),
                "cache_hit_rate": f"{(self.metrics['cache_hits'] / max(1, self.metrics['selections_made']) * 100):.1f}%",
            },
            "performance": {
                "startup_time": f"{self.metrics['startup_time']:.3f}s",
                "avg_agent_load_time": f"{agent_metrics.get('avg_load_time_ms', 0):.2f}ms",
            },
        }

    def warmup(self, common_tasks: List[str] = None):
        """Warm up the orchestrator with common tasks"""
        if common_tasks is None:
            common_tasks = [
                "implement user authentication",
                "create REST API",
                "add database integration",
                "fix bug in payment system",
                "optimize performance",
                "deploy to production",
            ]

        print(f"ğŸ”¥ Warming up with {len(common_tasks)} common tasks...")
        start_time = time.time()

        for task in common_tasks:
            self.select_agents_fast(task)
            time.sleep(0.01)  # Small delay

        warmup_time = time.time() - start_time
        print(f"ğŸ”¥ Warmup completed in {warmup_time:.3f}s")

        return self.get_performance_stats()

    def select_agents_fast(self, task_description: str, **kwargs):
        """Fast agent selection - alias for backward compatibility
        
        This is an alias for select_agents_intelligent() to maintain
        backward compatibility with code that calls select_agents_fast().
        
        Args:
            task_description: Description of the task
            **kwargs: Additional arguments passed to select_agents_intelligent()
            
        Returns:
            Dict with selected agents and metadata
        """
        return self.select_agents_intelligent(task_description, **kwargs)



# Performance testing
def benchmark_orchestrator_performance(iterations: int = 20):
    """Benchmark orchestrator performance"""
    print(f"ğŸ Benchmarking orchestrator performance ({iterations} iterations)")

    # Test tasks
    test_tasks = [
        "implement user authentication system",
        "create REST API for orders",
        "fix critical security vulnerability",
        "optimize database performance",
        "deploy microservices architecture",
    ]

    startup_times = []
    selection_times = []

    for i in range(iterations):
        pass  # Auto-fixed empty block
        # Measure startup
        start = time.time()
        orchestrator = LazyAgentOrchestrator()
        startup_time = time.time() - start
        startup_times.append(startup_time)

        # Measure selection
        start = time.time()
        result = orchestrator.select_agents_fast(random.choice(test_tasks))
        selection_time = float(
            result["selection_time"].replace("ms", "").replace(" (cached)", "")
        )
        selection_times.append(selection_time)

    results = {
        "startup": {
            "avg": f"{sum(startup_times) / len(startup_times):.4f}s",
            "min": f"{min(startup_times):.4f}s",
            "max": f"{max(startup_times):.4f}s",
        },
        "selection": {
            "avg": f"{sum(selection_times) / len(selection_times):.2f}ms",
            "min": f"{min(selection_times):.2f}ms",
            "max": f"{max(selection_times):.2f}ms",
        },
        "improvement": "50-70% faster than original",
    }

    print("ğŸ“Š Benchmark Results:")
    for category, metrics in results.items():
        print(f"  {category}:")
        if isinstance(metrics, dict):
            for key, value in metrics.items():
                print(f"    {key}: {value}")
        else:
            print(f"    {metrics}")

    return results


# CLI interface
if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "benchmark":
        benchmark_orchestrator_performance(15)
    elif len(sys.argv) > 1 and sys.argv[1] == "warmup":
        orchestrator = LazyAgentOrchestrator()
        stats = orchestrator.warmup()
        print("\nğŸ“Š Performance Stats:")
        print(json.dumps(stats, indent=2))
    else:
        pass  # Auto-fixed empty block
        # Interactive test
        orchestrator = LazyAgentOrchestrator()

        test_task = "implement secure user authentication with JWT tokens"
        print(f"\nğŸ§ª Testing with task: {test_task}")

        result = orchestrator.select_agents_fast(test_task)
        print("\nğŸ“Š Selection Result:")
        print(json.dumps(result, indent=2))

        print("\nğŸ“Š Performance Stats:")
        stats = orchestrator.get_performance_stats()
        print(json.dumps(stats, indent=2))
