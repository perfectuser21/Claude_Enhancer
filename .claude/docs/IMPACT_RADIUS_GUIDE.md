# 影响半径使用指南 (Impact Radius Assessment Guide)

> 为非技术用户准备的完整使用手册 (v1.3)

## 🎯 什么是影响半径？

**简单类比**：
- 想象你在湖里扔石头，水波纹会向外扩散
- 石头越大（风险越高），波纹越大（影响越广）
- 影响半径就是衡量"这个改动会造成多大波纹"

**技术定义 (v1.3)**：
影响半径是一个0-100分的评分系统，用于评估任务的：
- **风险级别**：出问题的严重程度
- **复杂度**：需要写多少代码
- **影响面**：会影响多少用户和功能

**准确率**: 86% (26/30 validated) ✅
**最大Agents**: 8个（合理使用，不浪费）

---

## 🤔 为什么需要影响半径评估？

### 问题背景

在Claude Enhancer v6.5之前，存在一个问题：

```
❌ 旧方式（固定策略）：
用户："帮我修复一个typo"
AI：好的，我用6个Agent并行处理...
      ↑
    浪费资源！2个Agent就够了

用户："帮我修复一个CVE安全漏洞"
AI：好的，我用3个Agent处理...
      ↑
    风险太高！应该用6个Agent多人审查
```

### 解决方案

**影响半径自动评估系统**：

```
✅ 新方式（智能策略 v1.3）：
用户："帮我修复一个typo"
AI：[自动评估] 影响半径=15分 → 0个Agent (AI自行处理)

用户："帮我修复一个CVE安全漏洞"
AI：[自动评估] 影响半径=70分 → 8个Agent（极高风险）

用户："帮我修复一个workflow bug"
AI：[自动评估] 影响半径=63分 → 6个Agent（高风险）
```

**好处**：
- ✅ 简单任务不浪费资源
- ✅ 复杂任务确保质量
- ✅ AI自动决策，用户无需手动指定
- ✅ 降低返工风险

---

## 📋 影响半径评估流程

### 自动流程（AI执行）

```
第1步：用户提出需求
"帮我实现登录功能"
       ↓
第2步：AI分析任务描述
- 提取关键词："登录"、"认证"
- 识别风险点："安全"、"权限"
- 估算代码量："新功能"
       ↓
第3步：三维评分
- 风险级别：7分（认证系统）
- 复杂度：6分（中等功能）
- 影响面：8分（所有用户）
       ↓
第4步：计算影响半径 (v1.3公式)
影响半径 = (7×5) + (6×3) + (8×2) = 69分
       ↓
第5步：选择Agent数量
69分 >= 50分 但 < 70分 → 使用6个Agent (高风险，非极高风险)
       ↓
第6步：AI向用户报告
"📊 影响半径评估: 69分（高风险任务）
 💡 建议使用6个Agent并行执行：
    - backend-architect（架构设计）
    - security-auditor（安全审查）
    - test-engineer（测试设计）
    - api-designer（API设计）
    - database-specialist（数据库）
    - technical-writer（文档）

 请确认？"
       ↓
第7步：用户确认后执行
用户："确认"
AI：开始执行...
```

### 手动调整流程（高级用户）

如果AI评估不准确，可以手动干预：

```bash
# 1. 查看详细评估
bash .claude/scripts/impact_radius_assessor.sh --verbose <<< "你的任务描述"

# 输出示例：
影响半径评估详情:
  关键词匹配:
    - "CVE" → 风险+3分
    - "安全漏洞" → 风险+2分
    - "单文件" → 复杂度-2分

  三维评分:
    - 风险级别: 9/10 (CRITICAL)
    - 复杂度: 4/10 (SIMPLE)
    - 影响面: 8/10 (WIDE)

  影响半径: 43分
  推荐策略: 6个Agent

# 2. 如果你认为评估过高，手动调整
export MANUAL_RISK=7        # 降低风险（9→7）
export MANUAL_COMPLEXITY=3  # 降低复杂度（4→3）
export MANUAL_SCOPE=6       # 降低影响面（8→6）

# 3. 重新计算
bash .claude/scripts/impact_radius_assessor.sh --manual <<< "你的任务描述"

# 新结果：
影响半径: 30分（从43分降低）
推荐策略: 仍然是6个Agent（但接近临界点）
```

---

## 🎓 如何理解评估结果？

### 评分解读表 (v1.3 - 4级系统)

| 影响半径分数 | 任务类型 | Agent数量 | 类比 |
|-------------|---------|----------|-----|
| **70-100分** | 极高风险任务 | 8个Agent | 建核电站（需要最完整专业团队+安全专家） |
| **50-69分** | 高风险任务 | 6个Agent | 装修房子（需要设计师、工人、监理） |
| **30-49分** | 中风险任务 | 4个Agent | 修理水管（需要水电工、助手、检查员、质检） |
| **0-29分** | 低风险任务 | 0个Agent | 换灯泡（一个人就够，AI自行处理） |

**注意**:
- 0个Agent表示AI自行处理简单任务，无需调用子Agent
- 8个Agent仅用于真正极高风险任务（多CVE、核心引擎重写），确保合理使用

### 实际案例对比

#### 案例1：修复CVE安全漏洞（高风险）

```
任务描述：
"修复Pre-commit Hook中的命令注入漏洞（CVE-2025-CE-001）"

AI评估过程：
1. 关键词识别：
   - "CVE" → 风险+3分
   - "安全漏洞" → 风险+2分
   - "命令注入" → 风险+2分
   - "Pre-commit Hook" → 影响面+3分

2. 三维评分：
   - 风险级别：10/10（CVE安全漏洞）
   - 复杂度：4/10（单文件，50行代码）
   - 影响面：4/10（所有commit操作）

3. 计算 (v1.3公式)：
   影响半径 = (10×5) + (4×3) + (4×2) = 70分

4. 决策 (v1.3)：
   70分 >= 70分 → 8个Agent (极高风险)

5. 推荐组合（8个Agent - v1.3新增）：
   核心团队：
   - backend-architect（修复代码）
   - security-auditor（安全审查）
   - test-engineer（安全测试）

   质量保障：
   - code-reviewer（代码审查）
   - performance-engineer（性能影响评估）

   专项支持：
   - devops-engineer（部署验证和回滚）
   - api-designer（API兼容性验证）
   - technical-writer（安全公告和文档）

为什么需要8个Agent？
- ✅ CVE安全漏洞是极高风险（影响半径正好70分）
- ✅ 需要最全面的审查和验证
- ✅ 需要完整的安全测试和文档
- ✅ 需要评估性能和API兼容性影响
- ✅ 这是合理使用8个Agent的典型场景
```

#### 案例2：修复文档Typo（低风险）

```
任务描述：
"修复README.md中的拼写错误（'Claude' 误写为 'Claud'）"

AI评估过程：
1. 关键词识别：
   - "拼写错误" → 风险-2分
   - "README.md" → 影响面+1分（核心文档）
   - "Typo" → 复杂度-2分

2. 三维评分：
   - 风险级别：2/10（仅影响文档）
   - 复杂度：1/10（<5行）
   - 影响面：1/10（单个文档）

3. 计算 (v1.2公式)：
   影响半径 = (2×5) + (1×3) + (1×2) = 15分

4. 决策：
   15分 < 30分 → 0个Agent（AI自行处理）

5. 执行方式：
   - AI直接修改（无需调用子Agent）
   - 快速完成（<1分钟）
   - 自动commit

为什么是0个Agent？
- ✅ 简单typo修复
- ✅ 不涉及代码逻辑
- ✅ 影响范围有限
- ✅ AI可以独立完成
```

#### 案例3：性能优化（中风险）

```
任务描述：
"优化Hook执行速度，目标从2秒降低到500ms"

AI评估过程：
1. 关键词识别：
   - "性能优化" → 复杂度+2分
   - "Hook" → 风险+2分
   - "目标明确" → 复杂度+1分

2. 三维评分：
   - 风险级别：6/10（可能引入bug）
   - 复杂度：7/10（算法优化）
   - 影响面：5/10（所有Hook，但不影响功能）

3. 计算 (v1.3公式)：
   影响半径 = (6×5) + (7×3) + (5×2) = 61分

4. 决策 (v1.3)：
   61分 >= 50分 但 < 70分 → 6个Agent (高风险，非极高风险)

5. 推荐组合（6个Agent）：
   - performance-engineer（性能优化）
   - backend-architect（代码重构）
   - test-engineer（基准测试）
   - code-reviewer（代码审查）
   - devops-engineer（部署验证）
   - technical-writer（文档更新）

为什么是6个Agent而非8个？
- ✅ 61分属于高风险区间（50-69分）
- ✅ 性能优化虽然复杂，但不是CVE级别的极高风险
- ✅ 6个Agent已经提供完整质量保障
- ✅ 合理使用原则：8个Agent仅用于70+分的极端情况
```

---

## 🛠️ 实际操作指南

### 场景1：我想修改一个简单的配置

**你的操作**：
```
你："帮我把Hook超时从3秒改为5秒"
```

**AI的处理**：
```
AI自动评估：
  - 风险：4分（配置修改，影响Hook行为）
  - 复杂度：1分（单行修改）
  - 影响面：6分（所有Hook）

  影响半径：(4×3)+(1×2)+(6×1) = 20分
  决策：20分 >= 8分 → 6个Agent

AI回复：
"📊 影响半径评估: 20分（中风险）

 虽然只改一行配置，但影响所有Hook的超时行为。
 为确保安全，我建议使用6个Agent：

 1. devops-engineer - 修改配置
 2. test-engineer - 测试新超时是否合理
 3. code-reviewer - 审查配置值
 4. backend-architect - 评估影响
 5. security-auditor - 验证安全性
 6. technical-writer - 更新文档

 预计耗时：15分钟

 请确认？或者您可以：
 A. 确认执行（推荐）
 B. 使用3个Agent（快速模式，风险较高）
 C. 先讨论方案，不执行"
```

**你的选择**：
- 选A：安全第一，按推荐执行
- 选B：你很确定这个改动很安全
- 选C：你想先了解为什么需要这么多Agent

### 场景2：紧急修复生产Bug

**你的操作**：
```
你："[URGENT] 生产环境Hook挂了，用户无法commit，紧急修复！"
```

**AI的处理**：
```
AI自动评估（紧急模式 - v1.3）：
  - 风险：8分（生产影响）+2分（紧急）= 10分
  - 复杂度：5分（未知原因）
  - 影响面：6分（所有用户，但是Hook问题不是核心引擎）

  影响半径：(10×5)+(5×3)+(6×2) = 77分
  决策（v1.3）：77分 >= 70分 → 8个Agent + 紧急流程

AI回复：
"🚨 紧急情况检测！影响半径: 77分（极高风险 - v1.3）

 我将立即启动紧急修复流程：

 1. 立即回滚到上一个稳定版本（5分钟内）
 2. 同时分析问题原因
 3. 使用8个Agent并行修复（紧急+极高风险）
 4. 快速但完整的测试后重新部署
 5. 全面验证和监控

 8个Agent紧急团队配置：
 - 核心3人：backend-architect, security-auditor, test-engineer
 - 质量2人：code-reviewer, performance-engineer
 - 专项3人：devops-engineer, incident-responder, technical-writer

 预计恢复时间：45分钟

 我现在开始执行：
 ✓ 步骤1: 执行回滚命令...
 ✓ 步骤2: 通知用户临时使用 --no-verify...
 ⏳ 步骤3: 8个Agent并行分析日志..."
```

### 场景3：实验性功能开发

**你的操作**：
```
你："在experiment/new-feature分支上，我想试试用AI自动生成commit message"
```

**AI的处理**：
```
AI自动评估（实验模式 - v1.3）：
  - 风险：3分（实验分支）-2分（隔离）= 1分
  - 复杂度：6分（新功能）
  - 影响面：2分（实验环境）

  影响半径：(1×5)+(6×3)+(2×2) = 27分
  决策（v1.3）：27分 < 30分 → 0个Agent（但可协商）

AI回复：
"📊 影响半径评估: 27分（实验环境 - v1.3）

 虽然是实验功能，但涉及AI集成，有一定复杂度。

 我建议三种方案：

 方案A（完整）：使用4个Agent
   - 完整开发流程
   - 预计耗时：2小时
   - 质量有保障
   - 如果实验成功，可直接merge到main

 方案B（标准）：AI自行处理 + 重点审查
   - 快速原型
   - 预计耗时：1小时
   - 仅用于验证可行性
   - 后续需要重构才能merge

 方案C（最快）：AI完全自主
   - 最快原型
   - 预计耗时：30分钟
   - 仅用于技术验证
   - 不适合直接merge

 您的选择？"
```

---

## 📊 评估准确性验证

### 如何知道评估是否准确？

**方法1：事后复盘**

完成任务后，回顾：
```
任务：修复Hook性能问题
AI评估：影响半径 39分 → 6个Agent
实际执行：用了6个Agent，耗时2小时

复盘：
✓ Agent数量：刚好（如果少用会遗漏测试）
✓ 时间：符合预期
✓ 质量：无返工
✓ 结论：评估准确 ✅
```

**方法2：对比类似任务**

```
任务A：修复CVE（影响半径43分 → 6个Agent）
任务B：修复安全bug（影响半径35分 → 6个Agent）

对比：
- 两个任务都是安全问题
- 评分接近，Agent数量一致
- 结论：评估一致性好 ✅
```

**方法3：统计分析**

```bash
# 查看历史评估统计
bash .claude/scripts/impact_radius_stats.sh

输出：
最近30天任务统计:
  平均影响半径: 18.5分
  中位数: 12分

Agent使用分布:
  2个Agent: 20%（6个任务）
  3个Agent: 30%（9个任务）
  6个Agent: 50%（15个任务）

准确性指标:
  无返工率: 90%（27/30任务）
  过度配置: 10%（3个任务用了6个，其实3个就够）
  配置不足: 0%（无任务因Agent不足返工）

结论：系统偏向保守（宁可多用，不可少用） ✅
```

---

## 🎯 常见问题解答

### Q1: 什么时候会用到8个Agent？

**A**: v1.3新增的8-Agent配置，仅用于极高风险任务（影响半径 >= 70分）。

**典型场景**：
```
✅ 合理使用8个Agent的场景：
- 修复多个CVE安全漏洞
- 核心引擎重写（workflow引擎、Phase系统）
- 架构大重构（涉及>10个模块）
- 紧急生产事故（影响所有用户+核心功能）

❌ 不应该用8个Agent的场景：
- 单个CVE修复但影响范围有限（用6个）
- 新功能开发（通常6个就够）
- 性能优化（通常6个就够）
- 普通Bug修复（4个）
```

**合理使用原则**：
- 10是极限，8是可接受的
- **重点在于8个是不是合理的使用，而不是故意使用没有意义**
- 当影响半径>=70分时，AI会推荐8个Agent
- 但最终还是要评估：这真的需要8个人的专业视角吗？

### Q2: AI的评估总是偏高，怎么办？

**A**: 这是设计上的保守策略。你可以：

**选项1：信任AI**（推荐）
- 多用Agent总比返工好
- Claude Code Max 20X用户不担心token成本

**选项2：手动调整**
```bash
# 每次都降低评分
export CE_IMPACT_BIAS=-5  # 全局降低5分

# 或者针对特定类型
export CE_DOC_RISK_BIAS=-2  # 文档任务风险-2分
```

**选项3：训练AI**（未来功能）
- 记录你的调整偏好
- AI学习你的风险偏好
- 自动适应你的风格

### Q3: 我能强制指定Agent数量吗？

**A**: 可以，但不推荐。

**方法1：在任务描述中明确**
```
你："帮我修复typo，只用2个Agent"
AI：好的，我将使用2个Agent...
   （跳过影响半径评估）
```

**方法2：设置环境变量**
```bash
export CE_FORCE_AGENTS=3
# 之后所有任务都用3个Agent（不推荐！）
```

**风险提示**：
- ❌ 可能因Agent不足导致质量问题
- ❌ 可能需要返工
- ❌ 失去自动化的好处

### Q4: 影响半径评估会增加多少时间？

**A**: 几乎没有增加。

**时间分解**：
```
传统流程（无评估）:
  用户提需求 → AI直接选Agent → 开始执行
  耗时：0秒（但可能选错）

新流程（有评估）:
  用户提需求 → AI评估影响半径(2秒) → 选Agent → 开始执行
  耗时：+2秒

如果选错Agent需要返工:
  返工成本：30分钟 - 2小时

结论：2秒换30分钟，非常值得！
```

### Q5: 评估结果会保存吗？

**A**: 会，用于持续改进。

**保存位置**：
```
.workflow/impact_assessments/
├── 2025-10-16_task_001.json
├── 2025-10-16_task_002.json
└── 2025-10-16_task_003.json
```

**数据内容**：
```json
{
  "task_id": "task_001",
  "timestamp": "2025-10-16T10:30:00Z",
  "description": "修复Pre-commit Hook的命令注入漏洞",
  "assessment": {
    "risk": 9,
    "complexity": 4,
    "scope": 8,
    "total": 43
  },
  "decision": {
    "agent_count": 6,
    "agents": ["backend-architect", "security-auditor", ...]
  },
  "outcome": {
    "actual_agents_used": 6,
    "rework_needed": false,
    "duration_minutes": 120
  }
}
```

**用途**：
- 统计分析
- 机器学习训练
- 审计追踪
- 持续优化

### Q6: 我是新手，应该如何使用这个功能？

**A**: 完全自动，无需学习。

**新手模式**（推荐）：
```
你只需要：
1. 正常描述你的需求
2. AI自动评估
3. AI告诉你建议
4. 你说"确认"
5. AI开始执行

就这么简单！
```

**进阶使用**（可选）：
- 理解评估逻辑（阅读本文档）
- 学会手动调整（提高效率）
- 参与评分校准（改进系统）

---

## 🚀 最佳实践

### 实践1：清晰描述任务

**❌ 不好的描述**：
```
"帮我改一下代码"
↑
AI无法准确评估，可能过度配置
```

**✅ 好的描述**：
```
"修复Pre-commit Hook中的命令注入漏洞（CVE-2025-CE-001），
影响所有commit操作，需要修改约50行代码"
↑
AI可以准确评估：风险9、复杂度4、影响面8
```

### 实践2：使用标签辅助

**紧急任务**：
```
"[URGENT] 生产环境Bug，用户无法登录"
→ AI自动提高风险评分
```

**实验任务**：
```
"[EXPERIMENT] 在实验分支尝试新算法"
→ AI自动降低风险评分
```

**文档任务**：
```
"[DOCS] 更新API文档"
→ AI识别为文档任务
```

### 实践3：信任但验证

**信任AI的评估**：
- 大多数情况下，AI评估是准确的
- 保守策略（多用Agent）总比冒险好

**但要验证**：
- 检查AI的推理过程
- 如果明显不合理，提出质疑
- 记录异常案例，帮助改进

### 实践4：积累经验

**建立个人基准**：
```
我的任务类型          典型影响半径
- 文档更新           5-10分
- Bug修复            10-20分
- 新功能开发         20-40分
- 安全修复           30-50分
- 架构重构           40-60分
```

**定期复盘**：
- 每周回顾任务评估
- 记录准确性
- 调整个人偏好
- 反馈给系统

---

## 📚 延伸阅读

### 深入理解

- **[影响半径评分矩阵](./IMPACT_RADIUS_MATRIX.md)** - 评分标准详细说明
- **[真实案例集](./IMPACT_RADIUS_CASES.md)** - 5个完整案例分析
- **[Agent策略说明](../AGENT_STRATEGY.md)** - Agent选择算法

### 工具文档

- **评估脚本使用**: `.claude/scripts/impact_radius_assessor.sh --help`
- **统计工具**: `.claude/scripts/impact_radius_stats.sh`
- **校准工具**: `.claude/scripts/impact_radius_calibrate.sh`

### 视频教程

（未来计划）
- 影响半径评估原理讲解（10分钟）
- 手动调整评分实操（15分钟）
- 高级技巧：评分校准（20分钟）

---

## 💡 小贴士

### 提示1：不要过度优化
```
❌ 错误心态：
"我要把每个任务的影响半径都降到最低，节省Agent"

✅ 正确心态：
"我信任自动评估，只在明显不合理时调整"

理由：
- 过度优化浪费时间
- 评估本身只需2秒
- 返工成本远高于多用Agent
```

### 提示2：记录特殊情况
```
遇到AI评估不准的情况：
1. 记录任务描述
2. 记录AI的评分
3. 记录你认为正确的评分
4. 提交反馈

这些数据帮助改进系统！
```

### 提示3：利用评估作为检查点
```
AI评估结果可以帮你发现问题：

例如：
你："帮我改个小typo"
AI："影响半径35分，建议6个Agent"
你："咦？为什么这么高？"
   ↓
检查后发现：这个typo在关键配置文件中！
感谢AI提醒了风险！
```

---

## 🎖️ 总结

### 关键要点

1. **影响半径是什么**
   - 0-100分的评分系统
   - 衡量风险、复杂度、影响面
   - 自动决定Agent数量（0/4/6/8）

2. **为什么重要**
   - 避免资源浪费（简单任务用太多Agent）
   - 避免质量风险（复杂任务用太少Agent）
   - 提高开发效率

3. **如何使用**
   - 大多数情况：完全自动，无需干预
   - 特殊情况：可以手动调整
   - 持续改进：记录反馈，帮助优化

4. **最佳实践**
   - 清晰描述任务
   - 信任AI评估
   - 验证合理性
   - 积累经验

### 记住

**影响半径评估是你的助手，不是负担**
- ✅ 它帮你做出更好的决策
- ✅ 它节省你的思考时间
- ✅ 它降低返工风险
- ✅ 它让开发更高效

**开始使用吧！** 🚀

---

**最后更新**: 2025-10-16
**维护者**: Claude Enhancer Team
**版本**: v1.3.0
**准确率**: 86% (26/30 validated) ✅
**最大Agents**: 8个（合理使用原则）
**帮助**: 如有问题，请查阅 [FAQ](#-常见问题解答) 或提交Issue
