# Perfect21 æ€§èƒ½è°ƒä¼˜æŒ‡å—

> âš¡ **ä¸“ä¸šæ€§èƒ½è°ƒä¼˜ç­–ç•¥ - è®© Perfect21 è¿è¡Œå¦‚é£**
>
> ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–ã€ç›‘æ§å’Œæ•…éšœæ’é™¤çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ

## ğŸ“– ç›®å½•

- [æ€§èƒ½åŸºå‡†](#æ€§èƒ½åŸºå‡†)
- [ç³»ç»Ÿèµ„æºä¼˜åŒ–](#ç³»ç»Ÿèµ„æºä¼˜åŒ–)
- [APIæ€§èƒ½ä¼˜åŒ–](#APIæ€§èƒ½ä¼˜åŒ–)
- [æ•°æ®åº“æ€§èƒ½è°ƒä¼˜](#æ•°æ®åº“æ€§èƒ½è°ƒä¼˜)
- [ç¼“å­˜ç­–ç•¥ä¼˜åŒ–](#ç¼“å­˜ç­–ç•¥ä¼˜åŒ–)
- [å¹¶å‘æ€§èƒ½ä¼˜åŒ–](#å¹¶å‘æ€§èƒ½ä¼˜åŒ–)
- [ç½‘ç»œæ€§èƒ½ä¼˜åŒ–](#ç½‘ç»œæ€§èƒ½ä¼˜åŒ–)
- [ç›‘æ§ä¸å‘Šè­¦](#ç›‘æ§ä¸å‘Šè­¦)
- [æ€§èƒ½æµ‹è¯•](#æ€§èƒ½æµ‹è¯•)

## ğŸ“Š æ€§èƒ½åŸºå‡†

### ç›®æ ‡æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ç±»å‹ | ç›®æ ‡å€¼ | ä¼˜ç§€å€¼ | è¯´æ˜ |
|---------|--------|--------|------|
| **APIå“åº”æ—¶é—´** | P95 < 200ms | P95 < 100ms | 95%è¯·æ±‚å“åº”æ—¶é—´ |
| **å¹¶å‘è¯·æ±‚** | 1000 req/s | 2000+ req/s | ç¨³å®šå¤„ç†èƒ½åŠ› |
| **å†…å­˜ä½¿ç”¨** | < 512MB | < 256MB | è¿›ç¨‹å†…å­˜å ç”¨ |
| **CPUä½¿ç”¨ç‡** | < 70% | < 50% | å¹³å‡CPUä½¿ç”¨ç‡ |
| **æ•°æ®åº“è¿æ¥** | < 100ms | < 50ms | è¿æ¥å»ºç«‹æ—¶é—´ |
| **ç¼“å­˜å‘½ä¸­ç‡** | > 85% | > 95% | Redisç¼“å­˜å‘½ä¸­ç‡ |

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```bash
#!/bin/bash
# Perfect21 æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬

echo "âš¡ Perfect21 æ€§èƒ½åŸºå‡†æµ‹è¯•"
echo "========================"

# 1. APIå“åº”æ—¶é—´æµ‹è¯•
echo "ğŸ“¡ APIå“åº”æ—¶é—´æµ‹è¯•:"
ab -n 1000 -c 10 http://localhost:8000/health | grep "Time per request"

# 2. å¹¶å‘å¤„ç†æµ‹è¯•
echo -e "\nğŸš€ å¹¶å‘å¤„ç†æµ‹è¯•:"
ab -n 5000 -c 100 http://localhost:8000/api/auth/health | grep "Requests per second"

# 3. å†…å­˜ä½¿ç”¨æµ‹è¯•
echo -e "\nğŸ’¾ å†…å­˜ä½¿ç”¨æµ‹è¯•:"
echo "å¼€å§‹å‰å†…å­˜:" $(ps -o rss= -p $(pgrep -f "perfect21") | awk '{sum+=$1} END {print sum/1024 "MB"}')

# æ‰§è¡Œå‹åŠ›æµ‹è¯•
ab -n 10000 -c 50 http://localhost:8000/health > /dev/null 2>&1

echo "æµ‹è¯•åå†…å­˜:" $(ps -o rss= -p $(pgrep -f "perfect21") | awk '{sum+=$1} END {print sum/1024 "MB"}')

# 4. æ•°æ®åº“è¿æ¥æµ‹è¯•
echo -e "\nğŸ—„ï¸ æ•°æ®åº“è¿æ¥æµ‹è¯•:"
python3 -c "
import time
from features.auth_system import AuthManager

times = []
for i in range(10):
    start = time.time()
    auth = AuthManager()
    auth.health_check()
    end = time.time()
    times.append((end-start)*1000)

print(f'å¹³å‡è¿æ¥æ—¶é—´: {sum(times)/len(times):.2f}ms')
print(f'æœ€å¤§è¿æ¥æ—¶é—´: {max(times):.2f}ms')
print(f'æœ€å°è¿æ¥æ—¶é—´: {min(times):.2f}ms')
"

# 5. ç¼“å­˜æ€§èƒ½æµ‹è¯•
echo -e "\nğŸ”„ ç¼“å­˜æ€§èƒ½æµ‹è¯•:"
redis-cli eval "
local hits = redis.call('info', 'stats'):match('keyspace_hits:(%d+)')
local misses = redis.call('info', 'stats'):match('keyspace_misses:(%d+)')
local hit_rate = hits / (hits + misses) * 100
return string.format('ç¼“å­˜å‘½ä¸­ç‡: %.2f%%', hit_rate)
" 0

echo -e "\nâœ… åŸºå‡†æµ‹è¯•å®Œæˆ"
```

## ğŸ–¥ï¸ ç³»ç»Ÿèµ„æºä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–ç­–ç•¥

```python
#!/usr/bin/env python3
"""
Perfect21 å†…å­˜ä¼˜åŒ–æ¨¡å—
"""

import gc
import sys
import psutil
import threading
from functools import wraps
from typing import Dict, Any, Optional
import weakref

class MemoryOptimizer:
    """å†…å­˜ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.object_pools: Dict[str, list] = {}
        self.memory_threshold = 500 * 1024 * 1024  # 500MB
        self.cleanup_interval = 300  # 5åˆ†é’Ÿ
        self.start_cleanup_timer()

    def start_cleanup_timer(self):
        """å¯åŠ¨å®šæœŸå†…å­˜æ¸…ç†"""
        def cleanup():
            self.cleanup_memory()
            # é‡æ–°å¯åŠ¨è®¡æ—¶å™¨
            timer = threading.Timer(self.cleanup_interval, cleanup)
            timer.daemon = True
            timer.start()

        timer = threading.Timer(self.cleanup_interval, cleanup)
        timer.daemon = True
        timer.start()

    def cleanup_memory(self):
        """æ‰§è¡Œå†…å­˜æ¸…ç†"""
        try:
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            collected = gc.collect()

            # æ¸…ç†å¯¹è±¡æ± 
            for pool_name in list(self.object_pools.keys()):
                if len(self.object_pools[pool_name]) > 100:
                    self.object_pools[pool_name] = self.object_pools[pool_name][:50]

            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024

            print(f"ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ: å›æ”¶{collected}ä¸ªå¯¹è±¡, å½“å‰å†…å­˜: {memory_mb:.1f}MB")

        except Exception as e:
            print(f"âŒ å†…å­˜æ¸…ç†å¤±è´¥: {e}")

    def get_object_pool(self, pool_name: str, create_func):
        """è·å–å¯¹è±¡æ± """
        if pool_name not in self.object_pools:
            self.object_pools[pool_name] = []

        pool = self.object_pools[pool_name]
        if pool:
            return pool.pop()
        else:
            return create_func()

    def return_to_pool(self, pool_name: str, obj):
        """å½’è¿˜å¯¹è±¡åˆ°æ± """
        if pool_name not in self.object_pools:
            self.object_pools[pool_name] = []

        pool = self.object_pools[pool_name]
        if len(pool) < 50:  # é™åˆ¶æ± å¤§å°
            # æ¸…ç†å¯¹è±¡çŠ¶æ€
            if hasattr(obj, 'reset'):
                obj.reset()
            pool.append(obj)

# å…¨å±€å†…å­˜ä¼˜åŒ–å™¨
memory_optimizer = MemoryOptimizer()

def memory_efficient(pool_name: str):
    """å†…å­˜æ•ˆç‡è£…é¥°å™¨"""
    def decorator(cls):
        original_new = cls.__new__

        def new_with_pool(cls_ref, *args, **kwargs):
            obj = memory_optimizer.get_object_pool(
                pool_name,
                lambda: original_new(cls_ref)
            )
            return obj

        cls.__new__ = new_with_pool
        return cls
    return decorator

class SlottedClass:
    """ä½¿ç”¨__slots__çš„åŸºç±»å‡å°‘å†…å­˜ä½¿ç”¨"""
    __slots__ = ()

    def __init__(self, **kwargs):
        for key, value in kwargs.items():
            if key in self.__slots__:
                setattr(self, key, value)

# å†…å­˜ç›‘æ§è£…é¥°å™¨
def monitor_memory(func):
    """ç›‘æ§å‡½æ•°å†…å­˜ä½¿ç”¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        process = psutil.Process()
        start_memory = process.memory_info().rss

        try:
            result = func(*args, **kwargs)
            return result
        finally:
            end_memory = process.memory_info().rss
            memory_diff = (end_memory - start_memory) / 1024 / 1024

            if memory_diff > 10:  # è¶…è¿‡10MBå¢é•¿
                print(f"âš ï¸ {func.__name__} å†…å­˜å¢é•¿: {memory_diff:.1f}MB")

    return wrapper

# æ™ºèƒ½ç¼“å­˜ç±»
class SmartCache:
    """æ™ºèƒ½ç¼“å­˜ï¼Œæ”¯æŒLRUå’Œå¤§å°é™åˆ¶"""

    def __init__(self, max_size=1000, max_memory_mb=100):
        self.max_size = max_size
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self.cache = {}
        self.access_order = []
        self.current_memory = 0

    def get(self, key):
        if key in self.cache:
            # æ›´æ–°è®¿é—®é¡ºåº
            self.access_order.remove(key)
            self.access_order.append(key)
            return self.cache[key]
        return None

    def set(self, key, value):
        # ä¼°ç®—å€¼çš„å¤§å°
        value_size = sys.getsizeof(value)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
        while (len(self.cache) >= self.max_size or
               self.current_memory + value_size > self.max_memory_bytes):
            if not self.access_order:
                break

            # ç§»é™¤æœ€ä¹…æœªä½¿ç”¨çš„é¡¹
            old_key = self.access_order.pop(0)
            if old_key in self.cache:
                old_value = self.cache.pop(old_key)
                self.current_memory -= sys.getsizeof(old_value)

        # æ·»åŠ æ–°å€¼
        self.cache[key] = value
        self.access_order.append(key)
        self.current_memory += value_size

# ä½¿ç”¨ç¤ºä¾‹
@memory_efficient("user_objects")
class User(SlottedClass):
    __slots__ = ['id', 'username', 'email', 'created_at']

    def reset(self):
        """é‡ç½®å¯¹è±¡çŠ¶æ€"""
        self.id = None
        self.username = None
        self.email = None
        self.created_at = None

# å…¨å±€æ™ºèƒ½ç¼“å­˜å®ä¾‹
smart_cache = SmartCache(max_size=5000, max_memory_mb=50)
```

### 2. CPUä¼˜åŒ–é…ç½®

```python
# CPUä¼˜åŒ–é…ç½®
import multiprocessing
import concurrent.futures
import asyncio
from functools import lru_cache

# ç³»ç»Ÿé…ç½®
CPU_COUNT = multiprocessing.cpu_count()
OPTIMAL_WORKERS = min(CPU_COUNT * 2, 8)  # æœ€å¤š8ä¸ªworker
THREAD_POOL_SIZE = CPU_COUNT * 4

# FastAPIä¼˜åŒ–é…ç½®
API_CONFIG = {
    "host": "0.0.0.0",
    "port": 8000,
    "workers": OPTIMAL_WORKERS,
    "worker_class": "uvicorn.workers.UvicornWorker",
    "worker_connections": 1000,
    "max_requests": 10000,
    "max_requests_jitter": 1000,
    "keepalive": 5,
    "preload_app": True,
}

# å¼‚æ­¥æ‰§è¡Œå™¨é…ç½®
class OptimizedExecutor:
    def __init__(self):
        self.thread_pool = concurrent.futures.ThreadPoolExecutor(
            max_workers=THREAD_POOL_SIZE
        )
        self.process_pool = concurrent.futures.ProcessPoolExecutor(
            max_workers=CPU_COUNT
        )

    async def run_cpu_bound_task(self, func, *args, **kwargs):
        """æ‰§è¡ŒCPUå¯†é›†å‹ä»»åŠ¡"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.process_pool, func, *args, **kwargs)

    async def run_io_bound_task(self, func, *args, **kwargs):
        """æ‰§è¡ŒIOå¯†é›†å‹ä»»åŠ¡"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.thread_pool, func, *args, **kwargs)

# è®¡ç®—å¯†é›†å‹ä»»åŠ¡ä¼˜åŒ–
@lru_cache(maxsize=1000)
def expensive_calculation(param):
    """ç¼“å­˜è®¡ç®—ç»“æœ"""
    # å¤æ‚è®¡ç®—é€»è¾‘
    return result
```

## ğŸš€ APIæ€§èƒ½ä¼˜åŒ–

### 1. å“åº”æ—¶é—´ä¼˜åŒ–

```python
#!/usr/bin/env python3
"""
APIæ€§èƒ½ä¼˜åŒ–æ¨¡å—
"""

import time
import asyncio
from fastapi import FastAPI, Request, Response
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.middleware.cors import CORSMiddleware
import uvloop
import httpx
from typing import Dict, Any
import json

# ä½¿ç”¨é«˜æ€§èƒ½äº‹ä»¶å¾ªç¯
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

class PerformanceMiddleware:
    """æ€§èƒ½ä¼˜åŒ–ä¸­é—´ä»¶"""

    def __init__(self, app: FastAPI):
        self.app = app
        self.response_cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

    async def __call__(self, request: Request, call_next):
        start_time = time.time()

        # æ£€æŸ¥ç¼“å­˜
        if request.method == "GET":
            cache_key = f"{request.url.path}?{request.url.query}"
            cached_response = self.get_cached_response(cache_key)
            if cached_response:
                return cached_response

        response = await call_next(request)

        # è®¡ç®—å“åº”æ—¶é—´
        process_time = time.time() - start_time
        response.headers["X-Process-Time"] = str(process_time)

        # ç¼“å­˜GETè¯·æ±‚çš„å“åº”
        if request.method == "GET" and response.status_code == 200:
            cache_key = f"{request.url.path}?{request.url.query}"
            self.cache_response(cache_key, response)

        # æ€§èƒ½å‘Šè­¦
        if process_time > 1.0:  # è¶…è¿‡1ç§’
            print(f"âš ï¸ æ…¢è¯·æ±‚: {request.url.path} è€—æ—¶ {process_time:.2f}s")

        return response

    def get_cached_response(self, cache_key: str) -> Response:
        """è·å–ç¼“å­˜çš„å“åº”"""
        if cache_key in self.response_cache:
            cached_data, timestamp = self.response_cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                return Response(
                    content=cached_data["content"],
                    status_code=cached_data["status_code"],
                    headers={"X-Cache": "HIT", **cached_data["headers"]},
                    media_type=cached_data["media_type"]
                )
        return None

    def cache_response(self, cache_key: str, response: Response):
        """ç¼“å­˜å“åº”"""
        # ç®€åŒ–çš„ç¼“å­˜å®ç°
        self.response_cache[cache_key] = (
            {
                "content": response.body,
                "status_code": response.status_code,
                "headers": dict(response.headers),
                "media_type": response.media_type
            },
            time.time()
        )

# æ‰¹é‡å¤„ç†ä¼˜åŒ–
class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""

    def __init__(self, batch_size=100, max_wait_time=0.1):
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests = []
        self.processing_lock = asyncio.Lock()

    async def add_request(self, request_data):
        """æ·»åŠ è¯·æ±‚åˆ°æ‰¹æ¬¡"""
        async with self.processing_lock:
            self.pending_requests.append(request_data)

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹æ¬¡å¤§å°æˆ–è¶…æ—¶
            if (len(self.pending_requests) >= self.batch_size or
                self.should_process_batch()):
                return await self.process_batch()

            # ç­‰å¾…æ›´å¤šè¯·æ±‚æˆ–è¶…æ—¶
            await asyncio.sleep(self.max_wait_time)
            if self.pending_requests:
                return await self.process_batch()

    def should_process_batch(self):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¤„ç†æ‰¹æ¬¡"""
        if not self.pending_requests:
            return False

        # æ£€æŸ¥æœ€æ—©è¯·æ±‚çš„ç­‰å¾…æ—¶é—´
        oldest_request = self.pending_requests[0]
        wait_time = time.time() - oldest_request.get('timestamp', time.time())
        return wait_time > self.max_wait_time

    async def process_batch(self):
        """å¤„ç†æ‰¹æ¬¡è¯·æ±‚"""
        if not self.pending_requests:
            return []

        batch = self.pending_requests.copy()
        self.pending_requests.clear()

        # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡ä¸­çš„è¯·æ±‚
        tasks = [self.process_single_request(req) for req in batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        return results

    async def process_single_request(self, request_data):
        """å¤„ç†å•ä¸ªè¯·æ±‚"""
        # å®é™…çš„è¯·æ±‚å¤„ç†é€»è¾‘
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        return {"processed": True, "data": request_data}

# HTTPå®¢æˆ·ç«¯ä¼˜åŒ–
class OptimizedHTTPClient:
    """ä¼˜åŒ–çš„HTTPå®¢æˆ·ç«¯"""

    def __init__(self):
        # è¿æ¥æ± é…ç½®
        limits = httpx.Limits(
            max_keepalive_connections=50,
            max_connections=100,
            keepalive_expiry=30.0
        )

        # è¶…æ—¶é…ç½®
        timeout = httpx.Timeout(
            connect=5.0,
            read=30.0,
            write=10.0,
            pool=5.0
        )

        self.client = httpx.AsyncClient(
            limits=limits,
            timeout=timeout,
            http2=True  # å¯ç”¨HTTP/2
        )

    async def batch_requests(self, urls):
        """æ‰¹é‡è¯·æ±‚"""
        tasks = [self.client.get(url) for url in urls]
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        return responses

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.client.aclose()

# é¢„çƒ­æœºåˆ¶
class APIWarmer:
    """APIé¢„çƒ­å™¨"""

    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.warmup_endpoints = [
            "/health",
            "/api/auth/health",
            "/api/metrics"
        ]

    async def warmup(self):
        """æ‰§è¡Œé¢„çƒ­"""
        print("ğŸ”¥ å¼€å§‹APIé¢„çƒ­...")

        async with OptimizedHTTPClient() as client:
            tasks = []
            for endpoint in self.warmup_endpoints:
                url = f"{self.base_url}{endpoint}"
                # æ¯ä¸ªç«¯ç‚¹é¢„çƒ­3æ¬¡
                for _ in range(3):
                    tasks.append(client.client.get(url))

            results = await asyncio.gather(*tasks, return_exceptions=True)
            success_count = sum(1 for r in results if not isinstance(r, Exception))

        print(f"âœ… APIé¢„çƒ­å®Œæˆ: {success_count}/{len(tasks)} è¯·æ±‚æˆåŠŸ")

# åº”ç”¨é…ç½®
def create_optimized_app():
    """åˆ›å»ºä¼˜åŒ–çš„FastAPIåº”ç”¨"""
    app = FastAPI(
        title="Perfect21 API",
        version="3.0.0",
        docs_url="/docs",
        redoc_url="/redoc"
    )

    # æ·»åŠ æ€§èƒ½ä¸­é—´ä»¶
    app.add_middleware(PerformanceMiddleware)

    # æ·»åŠ å‹ç¼©ä¸­é—´ä»¶
    app.add_middleware(GZipMiddleware, minimum_size=1000)

    # é…ç½®CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”è¯¥é™åˆ¶
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    return app

# å¯åŠ¨æ—¶é¢„çƒ­
async def startup_warmup():
    """å¯åŠ¨æ—¶æ‰§è¡Œé¢„çƒ­"""
    warmer = APIWarmer()
    await warmer.warmup()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    app = create_optimized_app()

    # æ·»åŠ å¯åŠ¨äº‹ä»¶
    @app.on_event("startup")
    async def startup_event():
        await startup_warmup()

    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=4,
        loop="uvloop",
        http="httptools"
    )
```

### 2. å¹¶å‘å¤„ç†ä¼˜åŒ–

```python
#!/usr/bin/env python3
"""
å¹¶å‘å¤„ç†ä¼˜åŒ–
"""

import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import List, Callable, Any
import functools
import time

class ConcurrencyManager:
    """å¹¶å‘ç®¡ç†å™¨"""

    def __init__(self, max_concurrent_tasks=100):
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        self.thread_pool = ThreadPoolExecutor(max_workers=20)
        self.process_pool = ProcessPoolExecutor(max_workers=4)

    async def run_with_semaphore(self, coro):
        """ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘"""
        async with self.semaphore:
            return await coro

    async def gather_with_limit(self, tasks, limit=50):
        """é™åˆ¶å¹¶å‘æ•°é‡çš„gather"""
        semaphore = asyncio.Semaphore(limit)

        async def run_task(task):
            async with semaphore:
                return await task

        limited_tasks = [run_task(task) for task in tasks]
        return await asyncio.gather(*limited_tasks, return_exceptions=True)

    def cpu_bound_task(self, func: Callable, *args, **kwargs):
        """CPUå¯†é›†å‹ä»»åŠ¡"""
        loop = asyncio.get_event_loop()
        return loop.run_in_executor(self.process_pool, func, *args, **kwargs)

    def io_bound_task(self, func: Callable, *args, **kwargs):
        """IOå¯†é›†å‹ä»»åŠ¡"""
        loop = asyncio.get_event_loop()
        return loop.run_in_executor(self.thread_pool, func, *args, **kwargs)

# æ™ºèƒ½è´Ÿè½½å‡è¡¡å™¨
class LoadBalancer:
    """æ™ºèƒ½è´Ÿè½½å‡è¡¡å™¨"""

    def __init__(self, servers: List[str]):
        self.servers = servers
        self.server_stats = {server: {"requests": 0, "errors": 0, "response_time": 0}
                           for server in servers}
        self.current_index = 0

    def get_best_server(self) -> str:
        """è·å–æœ€ä½³æœåŠ¡å™¨"""
        # ç®€å•çš„æƒé‡ç®—æ³•
        best_server = min(self.servers, key=lambda s: (
            self.server_stats[s]["errors"] * 1000 +
            self.server_stats[s]["response_time"] +
            self.server_stats[s]["requests"] * 0.1
        ))
        return best_server

    def record_request(self, server: str, response_time: float, is_error: bool = False):
        """è®°å½•è¯·æ±‚ç»Ÿè®¡"""
        stats = self.server_stats[server]
        stats["requests"] += 1
        stats["response_time"] = (stats["response_time"] * 0.9) + (response_time * 0.1)
        if is_error:
            stats["errors"] += 1

    async def make_request(self, path: str, **kwargs) -> Any:
        """å‘èµ·è´Ÿè½½å‡è¡¡çš„è¯·æ±‚"""
        server = self.get_best_server()
        url = f"{server}{path}"

        start_time = time.time()
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, **kwargs) as response:
                    result = await response.json()
                    response_time = time.time() - start_time
                    self.record_request(server, response_time, False)
                    return result
        except Exception as e:
            response_time = time.time() - start_time
            self.record_request(server, response_time, True)
            raise e

# è‡ªé€‚åº”çº¿ç¨‹æ± 
class AdaptiveThreadPool:
    """è‡ªé€‚åº”çº¿ç¨‹æ± """

    def __init__(self, min_workers=5, max_workers=50):
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.current_workers = min_workers
        self.pool = ThreadPoolExecutor(max_workers=min_workers)
        self.queue_size = 0
        self.completed_tasks = 0
        self.last_adjustment = time.time()

    def submit(self, fn, *args, **kwargs):
        """æäº¤ä»»åŠ¡"""
        self.queue_size += 1

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´çº¿ç¨‹æ± å¤§å°
        self._adjust_pool_size()

        future = self.pool.submit(self._wrapped_fn, fn, *args, **kwargs)
        return future

    def _wrapped_fn(self, fn, *args, **kwargs):
        """åŒ…è£…çš„å‡½æ•°æ‰§è¡Œ"""
        try:
            result = fn(*args, **kwargs)
            self.completed_tasks += 1
            return result
        finally:
            self.queue_size = max(0, self.queue_size - 1)

    def _adjust_pool_size(self):
        """è°ƒæ•´çº¿ç¨‹æ± å¤§å°"""
        now = time.time()
        if now - self.last_adjustment < 30:  # 30ç§’å†…ä¸é‡å¤è°ƒæ•´
            return

        # åŸºäºé˜Ÿåˆ—å¤§å°å’Œå®Œæˆä»»åŠ¡æ•°è°ƒæ•´
        if self.queue_size > self.current_workers * 2:  # é˜Ÿåˆ—ç§¯å‹ä¸¥é‡
            new_size = min(self.current_workers * 2, self.max_workers)
            if new_size > self.current_workers:
                self._resize_pool(new_size)
        elif self.queue_size < self.current_workers * 0.5:  # é˜Ÿåˆ—è¾ƒç©º
            new_size = max(self.current_workers // 2, self.min_workers)
            if new_size < self.current_workers:
                self._resize_pool(new_size)

        self.last_adjustment = now

    def _resize_pool(self, new_size: int):
        """è°ƒæ•´çº¿ç¨‹æ± å¤§å°"""
        old_pool = self.pool
        self.pool = ThreadPoolExecutor(max_workers=new_size)
        self.current_workers = new_size

        print(f"ğŸ“ˆ çº¿ç¨‹æ± å¤§å°è°ƒæ•´: {self.current_workers} -> {new_size}")

        # ä¼˜é›…å…³é—­æ—§çº¿ç¨‹æ± 
        old_pool.shutdown(wait=False)

# å…¨å±€å¹¶å‘ç®¡ç†å™¨
concurrency_manager = ConcurrencyManager()
adaptive_pool = AdaptiveThreadPool()
```

## ğŸ—„ï¸ æ•°æ®åº“æ€§èƒ½è°ƒä¼˜

### 1. è¿æ¥æ± ä¼˜åŒ–

```python
#!/usr/bin/env python3
"""
æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–
"""

import asyncpg
import asyncio
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import QueuePool
from typing import Optional
import time
import logging

class DatabaseManager:
    """æ•°æ®åº“ç®¡ç†å™¨"""

    def __init__(self, database_url: str):
        # è¿æ¥æ± é…ç½®
        self.engine = create_async_engine(
            database_url,
            # è¿æ¥æ± é…ç½®
            poolclass=QueuePool,
            pool_size=20,          # è¿æ¥æ± å¤§å°
            max_overflow=30,       # æœ€å¤§æº¢å‡ºè¿æ¥
            pool_timeout=30,       # è·å–è¿æ¥è¶…æ—¶
            pool_recycle=3600,     # è¿æ¥å›æ”¶æ—¶é—´(1å°æ—¶)
            pool_pre_ping=True,    # è¿æ¥å‰pingæ£€æŸ¥
            # æ€§èƒ½ä¼˜åŒ–
            echo=False,            # ç”Ÿäº§ç¯å¢ƒå…³é—­SQLæ—¥å¿—
            future=True,
            # PostgreSQLç‰¹å®šä¼˜åŒ–
            connect_args={
                "server_settings": {
                    "jit": "off",  # å…³é—­JITä¼˜åŒ–(å°æŸ¥è¯¢)
                    "application_name": "perfect21",
                },
                "command_timeout": 30,
            }
        )

        # ä¼šè¯å·¥å‚
        self.async_session = sessionmaker(
            bind=self.engine,
            class_=AsyncSession,
            expire_on_commit=False
        )

        # ç›‘æ§ç»Ÿè®¡
        self.query_stats = {
            "total_queries": 0,
            "slow_queries": 0,
            "total_time": 0
        }

    async def get_session(self) -> AsyncSession:
        """è·å–æ•°æ®åº“ä¼šè¯"""
        return self.async_session()

    async def execute_with_stats(self, session: AsyncSession, query, params=None):
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶ç»Ÿè®¡æ€§èƒ½"""
        start_time = time.time()

        try:
            if params:
                result = await session.execute(query, params)
            else:
                result = await session.execute(query)

            execution_time = time.time() - start_time

            # æ›´æ–°ç»Ÿè®¡
            self.query_stats["total_queries"] += 1
            self.query_stats["total_time"] += execution_time

            # æ…¢æŸ¥è¯¢å‘Šè­¦
            if execution_time > 0.5:  # 500ms
                self.query_stats["slow_queries"] += 1
                logging.warning(f"æ…¢æŸ¥è¯¢æ£€æµ‹: {execution_time:.3f}s - {str(query)[:100]}...")

            return result

        except Exception as e:
            logging.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            raise

    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        total = self.query_stats["total_queries"]
        if total > 0:
            avg_time = self.query_stats["total_time"] / total
            slow_ratio = self.query_stats["slow_queries"] / total * 100

            return {
                "total_queries": total,
                "average_time": f"{avg_time:.3f}s",
                "slow_queries": self.query_stats["slow_queries"],
                "slow_query_ratio": f"{slow_ratio:.1f}%"
            }
        return {"total_queries": 0}

# æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–å™¨
class QueryOptimizer:
    """æŸ¥è¯¢ä¼˜åŒ–å™¨"""

    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.query_cache = {}

    async def execute_with_cache(self, query, params=None, cache_key=None, ttl=300):
        """å¸¦ç¼“å­˜çš„æŸ¥è¯¢æ‰§è¡Œ"""
        if cache_key:
            cached_result = self._get_cached_result(cache_key)
            if cached_result:
                return cached_result

        async with self.db_manager.get_session() as session:
            result = await self.db_manager.execute_with_stats(session, query, params)

            if cache_key:
                self._cache_result(cache_key, result, ttl)

            return result

    def _get_cached_result(self, cache_key):
        """è·å–ç¼“å­˜ç»“æœ"""
        if cache_key in self.query_cache:
            result, timestamp = self.query_cache[cache_key]
            if time.time() - timestamp < 300:  # 5åˆ†é’Ÿç¼“å­˜
                return result
            else:
                del self.query_cache[cache_key]
        return None

    def _cache_result(self, cache_key, result, ttl):
        """ç¼“å­˜æŸ¥è¯¢ç»“æœ"""
        self.query_cache[cache_key] = (result, time.time())

# PostgreSQLç‰¹å®šä¼˜åŒ–
class PostgreSQLOptimizer:
    """PostgreSQLä¼˜åŒ–å™¨"""

    def __init__(self, connection_string: str):
        self.connection_string = connection_string

    async def optimize_database(self):
        """ä¼˜åŒ–æ•°æ®åº“é…ç½®"""
        conn = await asyncpg.connect(self.connection_string)

        try:
            # è®¾ç½®ä¼˜åŒ–å‚æ•°
            optimizations = [
                # å†…å­˜é…ç½®
                "SET shared_buffers = '256MB'",
                "SET effective_cache_size = '1GB'",
                "SET work_mem = '4MB'",
                "SET maintenance_work_mem = '64MB'",

                # æ£€æŸ¥ç‚¹é…ç½®
                "SET wal_buffers = '16MB'",
                "SET checkpoint_completion_target = 0.9",
                "SET checkpoint_segments = 32",

                # è¿æ¥é…ç½®
                "SET max_connections = 200",

                # æŸ¥è¯¢ä¼˜åŒ–
                "SET random_page_cost = 1.1",
                "SET effective_io_concurrency = 200",

                # ç»Ÿè®¡ä¿¡æ¯
                "SET default_statistics_target = 100",
            ]

            for sql in optimizations:
                try:
                    await conn.execute(sql)
                    print(f"âœ… åº”ç”¨ä¼˜åŒ–: {sql}")
                except Exception as e:
                    print(f"âš ï¸ ä¼˜åŒ–å¤±è´¥: {sql} - {e}")

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            await conn.execute("ANALYZE")
            print("âœ… æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯å·²æ›´æ–°")

        finally:
            await conn.close()

    async def get_slow_queries(self, limit=10):
        """è·å–æ…¢æŸ¥è¯¢"""
        conn = await asyncpg.connect(self.connection_string)

        try:
            # éœ€è¦å¯ç”¨pg_stat_statementsæ‰©å±•
            slow_queries = await conn.fetch("""
                SELECT
                    query,
                    calls,
                    total_time,
                    mean_time,
                    rows
                FROM pg_stat_statements
                ORDER BY mean_time DESC
                LIMIT $1
            """, limit)

            return [dict(record) for record in slow_queries]

        except Exception as e:
            print(f"è·å–æ…¢æŸ¥è¯¢å¤±è´¥: {e}")
            return []
        finally:
            await conn.close()

# ä½¿ç”¨ç¤ºä¾‹
async def setup_optimized_database():
    """è®¾ç½®ä¼˜åŒ–çš„æ•°æ®åº“"""
    database_url = "postgresql+asyncpg://user:password@localhost/perfect21"

    # åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨
    db_manager = DatabaseManager(database_url)

    # åˆ›å»ºæŸ¥è¯¢ä¼˜åŒ–å™¨
    query_optimizer = QueryOptimizer(db_manager)

    # PostgreSQLä¼˜åŒ–
    pg_optimizer = PostgreSQLOptimizer(database_url.replace("+asyncpg", ""))
    await pg_optimizer.optimize_database()

    return db_manager, query_optimizer
```

### 2. ç´¢å¼•ä¼˜åŒ–

```sql
-- Perfect21 æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–

-- ç”¨æˆ·è¡¨ç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_email
ON users(email) WHERE active = true;

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_username
ON users(username) WHERE active = true;

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_created_at
ON users(created_at);

-- è®¤è¯ä»¤ç‰Œè¡¨ç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_tokens_user_id
ON auth_tokens(user_id) WHERE active = true;

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_tokens_expires_at
ON auth_tokens(expires_at) WHERE active = true;

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_tokens_token_hash
ON auth_tokens USING hash(token_hash) WHERE active = true;

-- ä»»åŠ¡æ‰§è¡Œè®°å½•ç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_task_executions_user_id
ON task_executions(user_id);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_task_executions_created_at
ON task_executions(created_at);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_task_executions_status
ON task_executions(status) WHERE status IN ('running', 'pending');

-- å·¥ä½œç©ºé—´ç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_workspaces_user_id
ON workspaces(user_id) WHERE active = true;

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_workspaces_name
ON workspaces(name) WHERE active = true;

-- å¤åˆç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_task_executions_user_status
ON task_executions(user_id, status, created_at);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_tokens_user_type
ON auth_tokens(user_id, token_type) WHERE active = true;

-- åˆ†åŒºè¡¨ç¤ºä¾‹ï¼ˆå¤§é‡æ•°æ®æ—¶ä½¿ç”¨ï¼‰
CREATE TABLE task_execution_logs_partitioned (
    id BIGSERIAL,
    task_execution_id BIGINT,
    log_level VARCHAR(20),
    message TEXT,
    created_at TIMESTAMP DEFAULT NOW()
) PARTITION BY RANGE (created_at);

-- åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE task_execution_logs_202509
PARTITION OF task_execution_logs_partitioned
FOR VALUES FROM ('2025-09-01') TO ('2025-10-01');

CREATE TABLE task_execution_logs_202510
PARTITION OF task_execution_logs_partitioned
FOR VALUES FROM ('2025-10-01') TO ('2025-11-01');

-- åˆ†åŒºç´¢å¼•
CREATE INDEX ON task_execution_logs_202509(task_execution_id, created_at);
CREATE INDEX ON task_execution_logs_202510(task_execution_id, created_at);

-- æ¸…ç†æ—§åˆ†åŒºçš„å®šæ—¶ä»»åŠ¡
-- åœ¨cronä¸­è¿è¡Œ: 0 2 1 * * psql -d perfect21 -c "DROP TABLE IF EXISTS task_execution_logs_$(date -d '3 months ago' +%Y%m)"
```

## ğŸ”„ ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

### Redisç¼“å­˜ä¼˜åŒ–

```python
#!/usr/bin/env python3
"""
Redisç¼“å­˜ä¼˜åŒ–
"""

import redis.asyncio as aioredis
import json
import pickle
import time
import hashlib
from typing import Any, Optional, Union
import asyncio

class OptimizedRedisCache:
    """ä¼˜åŒ–çš„Redisç¼“å­˜"""

    def __init__(self, redis_url="redis://localhost:6379", db=0):
        # è¿æ¥æ± é…ç½®
        self.pool = aioredis.ConnectionPool.from_url(
            redis_url,
            db=db,
            max_connections=50,        # æœ€å¤§è¿æ¥æ•°
            retry_on_timeout=True,     # è¶…æ—¶é‡è¯•
            socket_keepalive=True,     # ä¿æŒè¿æ¥
            socket_keepalive_options={
                1: 1,  # TCP_KEEPIDLE
                2: 3,  # TCP_KEEPINTVL
                3: 5,  # TCP_KEEPCNT
            },
            health_check_interval=30   # å¥åº·æ£€æŸ¥é—´éš”
        )
        self.redis = aioredis.Redis(connection_pool=self.pool)

        # ç¼“å­˜ç»Ÿè®¡
        self.stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "deletes": 0
        }

    async def get(self, key: str, default=None) -> Any:
        """è·å–ç¼“å­˜å€¼"""
        try:
            value = await self.redis.get(key)
            if value is not None:
                self.stats["hits"] += 1
                return self._deserialize(value)
            else:
                self.stats["misses"] += 1
                return default
        except Exception as e:
            print(f"Redis GET error: {e}")
            return default

    async def set(self, key: str, value: Any, ttl: int = 3600) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        try:
            serialized_value = self._serialize(value)
            result = await self.redis.setex(key, ttl, serialized_value)
            if result:
                self.stats["sets"] += 1
            return result
        except Exception as e:
            print(f"Redis SET error: {e}")
            return False

    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        try:
            result = await self.redis.delete(key)
            if result:
                self.stats["deletes"] += 1
            return bool(result)
        except Exception as e:
            print(f"Redis DELETE error: {e}")
            return False

    async def get_or_set(self, key: str, func, ttl: int = 3600, *args, **kwargs) -> Any:
        """è·å–æˆ–è®¾ç½®ç¼“å­˜"""
        # å…ˆå°è¯•è·å–
        value = await self.get(key)
        if value is not None:
            return value

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå‡½æ•°
        if asyncio.iscoroutinefunction(func):
            value = await func(*args, **kwargs)
        else:
            value = func(*args, **kwargs)

        # è®¾ç½®ç¼“å­˜
        await self.set(key, value, ttl)
        return value

    async def batch_get(self, keys: list) -> dict:
        """æ‰¹é‡è·å–"""
        try:
            values = await self.redis.mget(keys)
            result = {}
            for i, key in enumerate(keys):
                if values[i] is not None:
                    result[key] = self._deserialize(values[i])
                    self.stats["hits"] += 1
                else:
                    self.stats["misses"] += 1
            return result
        except Exception as e:
            print(f"Redis BATCH GET error: {e}")
            return {}

    async def batch_set(self, items: dict, ttl: int = 3600) -> bool:
        """æ‰¹é‡è®¾ç½®"""
        try:
            pipe = self.redis.pipeline()
            for key, value in items.items():
                serialized_value = self._serialize(value)
                pipe.setex(key, ttl, serialized_value)

            results = await pipe.execute()
            success_count = sum(1 for r in results if r)
            self.stats["sets"] += success_count
            return success_count == len(items)
        except Exception as e:
            print(f"Redis BATCH SET error: {e}")
            return False

    async def clear_pattern(self, pattern: str) -> int:
        """æŒ‰æ¨¡å¼æ¸…ç†ç¼“å­˜"""
        try:
            keys = await self.redis.keys(pattern)
            if keys:
                deleted = await self.redis.delete(*keys)
                self.stats["deletes"] += deleted
                return deleted
            return 0
        except Exception as e:
            print(f"Redis CLEAR PATTERN error: {e}")
            return 0

    def _serialize(self, value: Any) -> bytes:
        """åºåˆ—åŒ–å€¼"""
        # å¯¹äºç®€å•ç±»å‹ä½¿ç”¨JSONï¼Œå¤æ‚ç±»å‹ä½¿ç”¨pickle
        try:
            return json.dumps(value).encode('utf-8')
        except (TypeError, ValueError):
            return pickle.dumps(value)

    def _deserialize(self, value: bytes) -> Any:
        """ååºåˆ—åŒ–å€¼"""
        try:
            return json.loads(value.decode('utf-8'))
        except (json.JSONDecodeError, UnicodeDecodeError):
            return pickle.loads(value)

    async def get_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        info = await self.redis.info("stats")

        hit_rate = 0
        if self.stats["hits"] + self.stats["misses"] > 0:
            hit_rate = self.stats["hits"] / (self.stats["hits"] + self.stats["misses"]) * 100

        return {
            "local_stats": self.stats,
            "hit_rate": f"{hit_rate:.2f}%",
            "redis_stats": {
                "keyspace_hits": info.get("keyspace_hits", 0),
                "keyspace_misses": info.get("keyspace_misses", 0),
                "used_memory": info.get("used_memory_human", "N/A"),
                "connected_clients": info.get("connected_clients", 0)
            }
        }

    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            await self.redis.ping()
            return True
        except Exception:
            return False

    async def close(self):
        """å…³é—­è¿æ¥"""
        await self.redis.close()

# æ™ºèƒ½ç¼“å­˜é”®ç”Ÿæˆå™¨
class CacheKeyGenerator:
    """ç¼“å­˜é”®ç”Ÿæˆå™¨"""

    @staticmethod
    def generate_key(prefix: str, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # æ„å»ºé”®çš„ç»„æˆéƒ¨åˆ†
        key_parts = [prefix]

        # æ·»åŠ ä½ç½®å‚æ•°
        for arg in args:
            if isinstance(arg, (str, int, float)):
                key_parts.append(str(arg))
            else:
                # å¯¹å¤æ‚å¯¹è±¡ä½¿ç”¨hash
                key_parts.append(hashlib.md5(str(arg).encode()).hexdigest()[:8])

        # æ·»åŠ å…³é”®å­—å‚æ•°
        if kwargs:
            sorted_kwargs = sorted(kwargs.items())
            kwargs_str = "&".join(f"{k}={v}" for k, v in sorted_kwargs)
            key_parts.append(hashlib.md5(kwargs_str.encode()).hexdigest()[:8])

        return ":".join(key_parts)

    @staticmethod
    def user_specific_key(user_id: str, prefix: str, *args) -> str:
        """ç”Ÿæˆç”¨æˆ·ç‰¹å®šçš„é”®"""
        return CacheKeyGenerator.generate_key(f"user:{user_id}:{prefix}", *args)

# ç¼“å­˜è£…é¥°å™¨
def cached(ttl: int = 3600, key_prefix: str = None):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            if key_prefix:
                cache_key = CacheKeyGenerator.generate_key(key_prefix, *args, **kwargs)
            else:
                cache_key = CacheKeyGenerator.generate_key(func.__name__, *args, **kwargs)

            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = await redis_cache.get(cache_key)
            if cached_result is not None:
                return cached_result

            # æ‰§è¡Œå‡½æ•°
            if asyncio.iscoroutinefunction(func):
                result = await func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)

            # è®¾ç½®ç¼“å­˜
            await redis_cache.set(cache_key, result, ttl)
            return result

        return wrapper
    return decorator

# å…¨å±€ç¼“å­˜å®ä¾‹
redis_cache = OptimizedRedisCache()

# ä½¿ç”¨ç¤ºä¾‹
@cached(ttl=1800, key_prefix="user_profile")
async def get_user_profile(user_id: str):
    """è·å–ç”¨æˆ·èµ„æ–™ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
    await asyncio.sleep(0.1)
    return {"user_id": user_id, "name": "Test User", "email": "test@example.com"}

# æ¸…ç†è¿‡æœŸç¼“å­˜çš„å®šæ—¶ä»»åŠ¡
async def cache_cleanup_task():
    """å®šæ—¶æ¸…ç†ç¼“å­˜ä»»åŠ¡"""
    while True:
        try:
            # æ¸…ç†è¿‡æœŸçš„ç”¨æˆ·ä¼šè¯
            await redis_cache.clear_pattern("session:*")

            # æ¸…ç†ä¸´æ—¶æ•°æ®
            await redis_cache.clear_pattern("temp:*")

            print("âœ… ç¼“å­˜æ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

        # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡
        await asyncio.sleep(3600)
```

## ğŸ“Š ç›‘æ§ä¸å‘Šè­¦

### æ€§èƒ½ç›‘æ§ç³»ç»Ÿ

```python
#!/usr/bin/env python3
"""
æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ
"""

import psutil
import time
import asyncio
import aiohttp
from datetime import datetime, timedelta
from typing import Dict, List, Any
import json

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.metrics_history = {}
        self.alert_thresholds = {
            "cpu_usage": 80,           # CPUä½¿ç”¨ç‡é˜ˆå€¼
            "memory_usage": 85,        # å†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼
            "disk_usage": 90,          # ç£ç›˜ä½¿ç”¨ç‡é˜ˆå€¼
            "api_response_time": 1.0,  # APIå“åº”æ—¶é—´é˜ˆå€¼(ç§’)
            "error_rate": 5.0,         # é”™è¯¯ç‡é˜ˆå€¼(%)
            "connection_count": 100    # æ•°æ®åº“è¿æ¥æ•°é˜ˆå€¼
        }
        self.alerts_sent = {}

    async def collect_system_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        # CPUä½¿ç”¨ç‡
        cpu_usage = psutil.cpu_percent(interval=1)

        # å†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        memory_usage = memory.percent

        # ç£ç›˜ä½¿ç”¨
        disk = psutil.disk_usage('/')
        disk_usage = (disk.used / disk.total) * 100

        # ç½‘ç»œIO
        net_io = psutil.net_io_counters()

        # è¿›ç¨‹ä¿¡æ¯
        process_count = len(psutil.pids())

        metrics = {
            "timestamp": datetime.now().isoformat(),
            "cpu_usage": cpu_usage,
            "memory_usage": memory_usage,
            "memory_available_gb": memory.available / (1024**3),
            "disk_usage": disk_usage,
            "disk_free_gb": disk.free / (1024**3),
            "network_bytes_sent": net_io.bytes_sent,
            "network_bytes_recv": net_io.bytes_recv,
            "process_count": process_count
        }

        return metrics

    async def collect_api_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†APIæŒ‡æ ‡"""
        try:
            start_time = time.time()
            async with aiohttp.ClientSession() as session:
                async with session.get('http://localhost:8000/health') as response:
                    response_time = time.time() - start_time
                    status_code = response.status

            # æ¨¡æ‹Ÿè·å–æ›´å¤šAPIæŒ‡æ ‡
            api_metrics = {
                "api_response_time": response_time,
                "api_status": "healthy" if status_code == 200 else "unhealthy",
                "requests_per_second": self._calculate_rps(),
                "error_rate": self._calculate_error_rate(),
                "active_connections": self._get_active_connections()
            }

        except Exception as e:
            api_metrics = {
                "api_response_time": float('inf'),
                "api_status": "down",
                "error": str(e)
            }

        return api_metrics

    async def collect_database_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†æ•°æ®åº“æŒ‡æ ‡"""
        # è¿™é‡Œåº”è¯¥è¿æ¥å®é™…çš„æ•°æ®åº“è·å–æŒ‡æ ‡
        # ä¸ºæ¼”ç¤ºç›®çš„ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        db_metrics = {
            "active_connections": 15,
            "slow_query_count": 2,
            "cache_hit_rate": 94.5,
            "database_size_mb": 256.7,
            "longest_query_time": 0.234
        }

        return db_metrics

    def _calculate_rps(self) -> float:
        """è®¡ç®—æ¯ç§’è¯·æ±‚æ•°"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥åŸºäºå®é™…è¯·æ±‚æ—¥å¿—
        return 25.3

    def _calculate_error_rate(self) -> float:
        """è®¡ç®—é”™è¯¯ç‡"""
        # ç®€åŒ–å®ç°
        return 0.8

    def _get_active_connections(self) -> int:
        """è·å–æ´»è·ƒè¿æ¥æ•°"""
        # ç®€åŒ–å®ç°
        return 42

    async def check_alerts(self, metrics: Dict[str, Any]):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        alerts = []

        # æ£€æŸ¥å„é¡¹æŒ‡æ ‡
        for metric_name, threshold in self.alert_thresholds.items():
            if metric_name in metrics:
                value = metrics[metric_name]

                if isinstance(value, (int, float)) and value > threshold:
                    # é¿å…é‡å¤å‘Šè­¦ï¼ˆ5åˆ†é’Ÿå†…ä¸é‡å¤å‘é€ç›¸åŒå‘Šè­¦ï¼‰
                    alert_key = f"{metric_name}_{threshold}"
                    last_alert = self.alerts_sent.get(alert_key, 0)

                    if time.time() - last_alert > 300:  # 5åˆ†é’Ÿ
                        alerts.append({
                            "metric": metric_name,
                            "current_value": value,
                            "threshold": threshold,
                            "severity": "warning" if value < threshold * 1.2 else "critical",
                            "timestamp": datetime.now().isoformat()
                        })
                        self.alerts_sent[alert_key] = time.time()

        if alerts:
            await self.send_alerts(alerts)

        return alerts

    async def send_alerts(self, alerts: List[Dict[str, Any]]):
        """å‘é€å‘Šè­¦"""
        for alert in alerts:
            print(f"ğŸš¨ ALERT: {alert['metric']} = {alert['current_value']}, "
                  f"threshold = {alert['threshold']} ({alert['severity']})")

            # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„å‘Šè­¦ç³»ç»Ÿ
            # å¦‚Slackã€é‚®ä»¶ã€é’‰é’‰ç­‰
            await self._send_to_webhook(alert)

    async def _send_to_webhook(self, alert: Dict[str, Any]):
        """å‘é€åˆ°Webhook"""
        webhook_url = "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"

        try:
            payload = {
                "text": f"Perfect21 Alert: {alert['metric']} exceeded threshold",
                "attachments": [{
                    "color": "danger" if alert['severity'] == "critical" else "warning",
                    "fields": [
                        {"title": "Metric", "value": alert['metric'], "short": True},
                        {"title": "Current Value", "value": str(alert['current_value']), "short": True},
                        {"title": "Threshold", "value": str(alert['threshold']), "short": True},
                        {"title": "Severity", "value": alert['severity'], "short": True}
                    ]
                }]
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(webhook_url, json=payload) as response:
                    if response.status == 200:
                        print("âœ… å‘Šè­¦å·²å‘é€åˆ°Slack")
                    else:
                        print(f"âŒ å‘Šè­¦å‘é€å¤±è´¥: {response.status}")

        except Exception as e:
            print(f"âŒ å‘Šè­¦å‘é€å¼‚å¸¸: {e}")

    def store_metrics(self, metrics: Dict[str, Any]):
        """å­˜å‚¨å†å²æŒ‡æ ‡"""
        timestamp = metrics.get('timestamp', datetime.now().isoformat())

        # ä¿å­˜åˆ°å†…å­˜ï¼ˆå®é™…åº”è¯¥ä¿å­˜åˆ°æ—¶åºæ•°æ®åº“ï¼‰
        for key, value in metrics.items():
            if key not in self.metrics_history:
                self.metrics_history[key] = []

            self.metrics_history[key].append({
                "timestamp": timestamp,
                "value": value
            })

            # åªä¿ç•™æœ€è¿‘1å°æ—¶çš„æ•°æ®
            cutoff_time = datetime.now() - timedelta(hours=1)
            self.metrics_history[key] = [
                item for item in self.metrics_history[key]
                if datetime.fromisoformat(item["timestamp"].replace('Z', '')) > cutoff_time
            ]

    def get_metrics_summary(self, hours=1) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        summary = {}
        cutoff_time = datetime.now() - timedelta(hours=hours)

        for metric_name, history in self.metrics_history.items():
            recent_data = [
                item for item in history
                if datetime.fromisoformat(item["timestamp"].replace('Z', '')) > cutoff_time
            ]

            if recent_data and all(isinstance(item["value"], (int, float)) for item in recent_data):
                values = [item["value"] for item in recent_data]
                summary[metric_name] = {
                    "current": values[-1] if values else 0,
                    "average": sum(values) / len(values),
                    "max": max(values),
                    "min": min(values),
                    "count": len(values)
                }

        return summary

    async def run_monitoring_loop(self, interval=30):
        """è¿è¡Œç›‘æ§å¾ªç¯"""
        print("ğŸ” å¯åŠ¨æ€§èƒ½ç›‘æ§...")

        while True:
            try:
                # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
                system_metrics = await self.collect_system_metrics()
                api_metrics = await self.collect_api_metrics()
                db_metrics = await self.collect_database_metrics()

                # åˆå¹¶æŒ‡æ ‡
                all_metrics = {**system_metrics, **api_metrics, **db_metrics}

                # å­˜å‚¨æŒ‡æ ‡
                self.store_metrics(all_metrics)

                # æ£€æŸ¥å‘Šè­¦
                await self.check_alerts(all_metrics)

                # æ‰“å°æ‘˜è¦
                if int(time.time()) % 300 == 0:  # æ¯5åˆ†é’Ÿæ‰“å°ä¸€æ¬¡
                    summary = self.get_metrics_summary()
                    print(f"ğŸ“Š ç³»ç»ŸçŠ¶æ€æ‘˜è¦: CPU={summary.get('cpu_usage', {}).get('current', 0):.1f}%, "
                          f"å†…å­˜={summary.get('memory_usage', {}).get('current', 0):.1f}%, "
                          f"APIå“åº”={summary.get('api_response_time', {}).get('current', 0):.3f}s")

            except Exception as e:
                print(f"âŒ ç›‘æ§å¼‚å¸¸: {e}")

            await asyncio.sleep(interval)

# æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå™¨
class PerformanceReporter:
    """æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, monitor: PerformanceMonitor):
        self.monitor = monitor

    def generate_daily_report(self) -> str:
        """ç”Ÿæˆæ—¥æŠ¥"""
        summary = self.monitor.get_metrics_summary(hours=24)

        report = []
        report.append("ğŸ“Š Perfect21 æ€§èƒ½æ—¥æŠ¥")
        report.append("=" * 40)
        report.append(f"æŠ¥å‘Šæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        # ç³»ç»ŸæŒ‡æ ‡
        report.append("ğŸ–¥ï¸ ç³»ç»ŸæŒ‡æ ‡:")
        if 'cpu_usage' in summary:
            cpu = summary['cpu_usage']
            report.append(f"  CPUä½¿ç”¨ç‡: å¹³å‡{cpu['average']:.1f}%, å³°å€¼{cpu['max']:.1f}%")

        if 'memory_usage' in summary:
            memory = summary['memory_usage']
            report.append(f"  å†…å­˜ä½¿ç”¨ç‡: å¹³å‡{memory['average']:.1f}%, å³°å€¼{memory['max']:.1f}%")

        # APIæŒ‡æ ‡
        report.append("")
        report.append("ğŸš€ APIæŒ‡æ ‡:")
        if 'api_response_time' in summary:
            api = summary['api_response_time']
            report.append(f"  å“åº”æ—¶é—´: å¹³å‡{api['average']:.3f}s, æœ€å¤§{api['max']:.3f}s")

        if 'error_rate' in summary:
            error = summary['error_rate']
            report.append(f"  é”™è¯¯ç‡: å¹³å‡{error['average']:.2f}%")

        # å»ºè®®
        report.append("")
        report.append("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        recommendations = self._generate_recommendations(summary)
        for rec in recommendations:
            report.append(f"  - {rec}")

        return "\n".join(report)

    def _generate_recommendations(self, summary: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # CPUä½¿ç”¨ç‡å»ºè®®
        if 'cpu_usage' in summary:
            cpu_avg = summary['cpu_usage']['average']
            if cpu_avg > 70:
                recommendations.append("CPUä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®å¢åŠ workeræ•°é‡æˆ–ä¼˜åŒ–ç®—æ³•")
            elif cpu_avg < 30:
                recommendations.append("CPUä½¿ç”¨ç‡è¾ƒä½ï¼Œå¯ä»¥è€ƒè™‘å‡å°‘èµ„æºåˆ†é…")

        # å†…å­˜ä½¿ç”¨å»ºè®®
        if 'memory_usage' in summary:
            memory_avg = summary['memory_usage']['average']
            if memory_avg > 80:
                recommendations.append("å†…å­˜ä½¿ç”¨ç‡é«˜ï¼Œå»ºè®®å¯ç”¨å†…å­˜æ¸…ç†æˆ–å¢åŠ å†…å­˜")

        # APIå“åº”æ—¶é—´å»ºè®®
        if 'api_response_time' in summary:
            api_avg = summary['api_response_time']['average']
            if api_avg > 0.5:
                recommendations.append("APIå“åº”æ—¶é—´è¾ƒæ…¢ï¼Œå»ºè®®å¯ç”¨ç¼“å­˜æˆ–ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢")

        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–")

        return recommendations

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•°"""
    monitor = PerformanceMonitor()
    reporter = PerformanceReporter(monitor)

    # å¯åŠ¨ç›‘æ§ï¼ˆåœ¨åå°è¿è¡Œï¼‰
    monitor_task = asyncio.create_task(monitor.run_monitoring_loop(interval=30))

    # å®šæœŸç”ŸæˆæŠ¥å‘Š
    async def report_task():
        while True:
            await asyncio.sleep(3600 * 24)  # æ¯å¤©ç”Ÿæˆä¸€æ¬¡æŠ¥å‘Š
            report = reporter.generate_daily_report()
            print(report)

            # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
            with open(f"performance_report_{datetime.now().strftime('%Y%m%d')}.txt", "w") as f:
                f.write(report)

    report_task_obj = asyncio.create_task(report_task())

    # ç­‰å¾…ä»»åŠ¡å®Œæˆ
    await asyncio.gather(monitor_task, report_task_obj)

if __name__ == "__main__":
    asyncio.run(main())
```

---

> âš¡ **æ€»ç»“**: Perfect21 v3.0.0 æ€§èƒ½è°ƒä¼˜æ¶µç›–äº†ä»ç³»ç»Ÿèµ„æºåˆ°åº”ç”¨å±‚é¢çš„å…¨æ–¹ä½ä¼˜åŒ–ã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„è°ƒä¼˜å’Œç›‘æ§ï¼ŒPerfect21 èƒ½å¤Ÿåœ¨å„ç§è´Ÿè½½ä¸‹ä¿æŒé«˜æ€§èƒ½è¿è¡Œï¼Œä¸ºç”¨æˆ·æä¾›å¿«é€Ÿã€ç¨³å®šçš„æ™ºèƒ½å¼€å‘ä½“éªŒã€‚
>
> **è®°ä½**: æ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œéœ€è¦æ ¹æ®å®é™…ä½¿ç”¨æƒ…å†µä¸æ–­è°ƒæ•´å’Œæ”¹è¿›ã€‚