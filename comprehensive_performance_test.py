#!/usr/bin/env python3
"""
ğŸš€ Claude Enhancer Comprehensive Performance Test Suite
==========================================

æ‰§è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…æ‹¬ï¼š
1. è´Ÿè½½æµ‹è¯•ï¼ˆ1000å¹¶å‘ç”¨æˆ·ï¼‰
2. å“åº”æ—¶é—´æµ‹è¯•
3. å†…å­˜ä½¿ç”¨æµ‹è¯•
4. æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–
5. ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•

ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æµ‹è¯•æŠ¥å‘Š
"""

import asyncio
import time
import json
import logging
import statistics
import psutil
import random
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import aiohttp
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceTestResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""
    test_name: str
    start_time: datetime
    end_time: datetime
    duration_seconds: float
    total_requests: int
    successful_requests: int
    failed_requests: int
    requests_per_second: float
    avg_response_time_ms: float
    p50_response_time_ms: float
    p95_response_time_ms: float
    p99_response_time_ms: float
    max_response_time_ms: float
    min_response_time_ms: float
    error_rate_percent: float
    cpu_usage_percent: float
    memory_usage_mb: float
    peak_memory_mb: float
    cache_hit_rate_percent: float = 0.0
    database_query_time_ms: float = 0.0
    additional_metrics: Dict[str, Any] = None

    def __post_init__(self):
        if self.additional_metrics is None:
            self.additional_metrics = {}

@dataclass
class PerformanceReport:
    """å®Œæ•´æ€§èƒ½æŠ¥å‘Š"""
    timestamp: datetime
    overall_score: float
    test_results: List[PerformanceTestResult]
    system_info: Dict[str, Any]
    bottlenecks: List[str]
    recommendations: List[str]
    charts_generated: List[str]

class ComprehensivePerformanceTester:
    """ç»¼åˆæ€§èƒ½æµ‹è¯•å™¨"""

    def __init__(self, base_url: str = "http://localhost:8080"):
        self.base_url = base_url
        self.test_results = []
        self.session = None

        # ç›‘æ§æ•°æ®
        self.cpu_samples = []
        self.memory_samples = []
        self.response_times = []

        # æµ‹è¯•é…ç½®
        self.test_configs = {
            'load_test': {
                'concurrent_users': 1000,
                'duration_seconds': 300,  # 5åˆ†é’Ÿ
                'ramp_up_seconds': 60     # 1åˆ†é’ŸåŠ å‹
            },
            'stress_test': {
                'concurrent_users': 2000,
                'duration_seconds': 180,  # 3åˆ†é’Ÿ
                'ramp_up_seconds': 30     # 30ç§’åŠ å‹
            },
            'spike_test': {
                'concurrent_users': 5000,
                'duration_seconds': 60,   # 1åˆ†é’Ÿ
                'ramp_up_seconds': 10     # 10ç§’åŠ å‹
            }
        }

    async def initialize(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ”§ åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•ç¯å¢ƒ...")

        # åˆ›å»ºHTTPä¼šè¯
        connector = aiohttp.TCPConnector(
            limit=2000,  # æ€»è¿æ¥æ± å¤§å°
            limit_per_host=500,  # æ¯ä¸ªä¸»æœºçš„è¿æ¥æ•°
            ttl_dns_cache=300,
            use_dns_cache=True,
        )

        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )

        # æ£€æŸ¥ç›®æ ‡æœåŠ¡å¯ç”¨æ€§
        await self._check_service_availability()

        logger.info("âœ… æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")

    async def _check_service_availability(self):
        """æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§"""
        try:
            async with self.session.get(f"{self.base_url}/health") as response:
                if response.status == 200:
                    logger.info("âœ… ç›®æ ‡æœåŠ¡å¯ç”¨")
                else:
                    logger.warning(f"âš ï¸ æœåŠ¡è¿”å›çŠ¶æ€ç : {response.status}")
        except Exception as e:
            logger.error(f"âŒ æ— æ³•è¿æ¥åˆ°ç›®æ ‡æœåŠ¡: {e}")
            raise

    async def run_comprehensive_tests(self) -> PerformanceReport:
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹ç»¼åˆæ€§èƒ½æµ‹è¯•")
        start_time = datetime.now()

        try:
            pass  # Auto-fixed empty block
            # 1. åŸºå‡†æ€§èƒ½æµ‹è¯•
            logger.info("ğŸ“Š Phase 1: åŸºå‡†æ€§èƒ½æµ‹è¯•")
            baseline_result = await self._baseline_performance_test()
            self.test_results.append(baseline_result)

            # 2. è´Ÿè½½æµ‹è¯• (1000å¹¶å‘ç”¨æˆ·)
            logger.info("âš¡ Phase 2: è´Ÿè½½æµ‹è¯• (1000å¹¶å‘ç”¨æˆ·)")
            load_result = await self._load_test()
            self.test_results.append(load_result)

            # 3. å“åº”æ—¶é—´æµ‹è¯•
            logger.info("â±ï¸ Phase 3: å“åº”æ—¶é—´æµ‹è¯•")
            response_time_result = await self._response_time_test()
            self.test_results.append(response_time_result)

            # 4. å†…å­˜å‹åŠ›æµ‹è¯•
            logger.info("ğŸ’¾ Phase 4: å†…å­˜å‹åŠ›æµ‹è¯•")
            memory_result = await self._memory_stress_test()
            self.test_results.append(memory_result)

            # 5. æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•
            logger.info("ğŸ—„ï¸ Phase 5: æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•")
            db_result = await self._database_performance_test()
            self.test_results.append(db_result)

            # 6. ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•
            logger.info("ğŸ—‚ï¸ Phase 6: ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•")
            cache_result = await self._cache_performance_test()
            self.test_results.append(cache_result)

            # 7. å‹åŠ›æµ‹è¯• (æé™è´Ÿè½½)
            logger.info("ğŸ’¥ Phase 7: å‹åŠ›æµ‹è¯• (æé™è´Ÿè½½)")
            stress_result = await self._stress_test()
            self.test_results.append(stress_result)

            # 8. ç”ŸæˆæŠ¥å‘Š
            logger.info("ğŸ“‹ Phase 8: ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š")
            report = await self._generate_comprehensive_report()

            # 9. ç”Ÿæˆå›¾è¡¨
            logger.info("ğŸ“ˆ Phase 9: ç”Ÿæˆæ€§èƒ½å›¾è¡¨")
            await self._generate_performance_charts()

            logger.info("âœ… ç»¼åˆæ€§èƒ½æµ‹è¯•å®Œæˆ")
            return report

        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            raise
        finally:
            if self.session:
                await self.session.close()

    async def _baseline_performance_test(self) -> PerformanceTestResult:
        """åŸºå‡†æ€§èƒ½æµ‹è¯•"""
        logger.info("æ‰§è¡ŒåŸºå‡†æ€§èƒ½æµ‹è¯•...")

        start_time = datetime.now()
        response_times = []
        successful_requests = 0
        failed_requests = 0

        # ç›‘æ§å¼€å§‹
        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            pass  # Auto-fixed empty block
            # æ‰§è¡Œ100ä¸ªé¡ºåºè¯·æ±‚ä½œä¸ºåŸºå‡†
            for i in range(100):
                request_start = time.time()

                try:
                    async with self.session.get(f"{self.base_url}/api/status") as response:
                        if response.status == 200:
                            successful_requests += 1
                        else:
                            failed_requests += 1

                        duration_ms = (time.time() - request_start) * 1000
                        response_times.append(duration_ms)

                except Exception as e:
                    failed_requests += 1
                    duration_ms = (time.time() - request_start) * 1000
                    response_times.append(duration_ms)

                await asyncio.sleep(0.1)  # 100msé—´éš”

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        return PerformanceTestResult(
            test_name="Baseline Performance Test",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            total_requests=len(response_times),
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            requests_per_second=len(response_times) / duration if duration > 0 else 0,
            avg_response_time_ms=statistics.mean(response_times) if response_times else 0,
            p50_response_time_ms=self._percentile(response_times, 50),
            p95_response_time_ms=self._percentile(response_times, 95),
            p99_response_time_ms=self._percentile(response_times, 99),
            max_response_time_ms=max(response_times) if response_times else 0,
            min_response_time_ms=min(response_times) if response_times else 0,
            error_rate_percent=(failed_requests / len(response_times) * 100) if response_times else 0,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0
        )

    async def _load_test(self) -> PerformanceTestResult:
        """è´Ÿè½½æµ‹è¯• - 1000å¹¶å‘ç”¨æˆ·"""
        logger.info("æ‰§è¡Œè´Ÿè½½æµ‹è¯• (1000å¹¶å‘ç”¨æˆ·)...")

        config = self.test_configs['load_test']
        start_time = datetime.now()

        # æ¸…ç†ä¹‹å‰çš„ç›‘æ§æ•°æ®
        self.cpu_samples.clear()
        self.memory_samples.clear()

        # å¯åŠ¨ç³»ç»Ÿç›‘æ§
        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            pass  # Auto-fixed empty block
            # æ‰§è¡Œè´Ÿè½½æµ‹è¯•
            results = await self._execute_concurrent_test(
                concurrent_users=config['concurrent_users'],
                duration_seconds=config['duration_seconds'],
                ramp_up_seconds=config['ramp_up_seconds']
            )

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        return PerformanceTestResult(
            test_name=f"Load Test ({config['concurrent_users']} users)",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            **results,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0
        )

    async def _response_time_test(self) -> PerformanceTestResult:
        """å“åº”æ—¶é—´æµ‹è¯•"""
        logger.info("æ‰§è¡Œå“åº”æ—¶é—´æµ‹è¯•...")

        start_time = datetime.now()
        response_times = []
        successful_requests = 0
        failed_requests = 0

        # æµ‹è¯•ä¸åŒç±»å‹çš„ç«¯ç‚¹
        endpoints = [
            "/api/health",
            "/api/status",
            "/api/metrics",
            "/api/users",
            "/api/auth/login"
        ]

        # æ¸…ç†ç›‘æ§æ•°æ®
        self.cpu_samples.clear()
        self.memory_samples.clear()

        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            pass  # Auto-fixed empty block
            # å¯¹æ¯ä¸ªç«¯ç‚¹æ‰§è¡Œå¤šæ¬¡è¯·æ±‚
            for endpoint in endpoints:
                for _ in range(50):  # æ¯ä¸ªç«¯ç‚¹50æ¬¡è¯·æ±‚
                    request_start = time.time()

                    try:
                        async with self.session.get(f"{self.base_url}{endpoint}") as response:
                            duration_ms = (time.time() - request_start) * 1000
                            response_times.append(duration_ms)

                            if response.status in [200, 201, 202]:
                                successful_requests += 1
                            else:
                                failed_requests += 1

                    except Exception as e:
                        duration_ms = (time.time() - request_start) * 1000
                        response_times.append(duration_ms)
                        failed_requests += 1

                    await asyncio.sleep(0.02)  # 20msé—´éš”

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        return PerformanceTestResult(
            test_name="Response Time Test",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            total_requests=len(response_times),
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            requests_per_second=len(response_times) / duration if duration > 0 else 0,
            avg_response_time_ms=statistics.mean(response_times) if response_times else 0,
            p50_response_time_ms=self._percentile(response_times, 50),
            p95_response_time_ms=self._percentile(response_times, 95),
            p99_response_time_ms=self._percentile(response_times, 99),
            max_response_time_ms=max(response_times) if response_times else 0,
            min_response_time_ms=min(response_times) if response_times else 0,
            error_rate_percent=(failed_requests / len(response_times) * 100) if response_times else 0,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0
        )

    async def _memory_stress_test(self) -> PerformanceTestResult:
        """å†…å­˜å‹åŠ›æµ‹è¯•"""
        logger.info("æ‰§è¡Œå†…å­˜å‹åŠ›æµ‹è¯•...")

        start_time = datetime.now()

        # æ¸…ç†ç›‘æ§æ•°æ®
        self.cpu_samples.clear()
        self.memory_samples.clear()

        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            pass  # Auto-fixed empty block
            # åˆ›å»ºå†…å­˜å‹åŠ› - å¤§é‡å¹¶å‘è¿æ¥
            results = await self._execute_concurrent_test(
                concurrent_users=500,
                duration_seconds=120,  # 2åˆ†é’Ÿ
                ramp_up_seconds=20,
                endpoint="/api/data/large"  # å‡è®¾æœ‰å¤§æ•°æ®ç«¯ç‚¹
            )

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        return PerformanceTestResult(
            test_name="Memory Stress Test",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            **results,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0
        )

    async def _database_performance_test(self) -> PerformanceTestResult:
        """æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•"""
        logger.info("æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•...")

        start_time = datetime.now()
        response_times = []
        successful_requests = 0
        failed_requests = 0

        # æ•°æ®åº“ç›¸å…³ç«¯ç‚¹
        db_endpoints = [
            "/api/users",
            "/api/users/search",
            "/api/analytics",
            "/api/reports"
        ]

        self.cpu_samples.clear()
        self.memory_samples.clear()

        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            pass  # Auto-fixed empty block
            # æµ‹è¯•æ•°æ®åº“å¯†é›†å‹æ“ä½œ
            for endpoint in db_endpoints:
                for _ in range(25):  # æ¯ä¸ªç«¯ç‚¹25æ¬¡è¯·æ±‚
                    request_start = time.time()

                    try:
                        pass  # Auto-fixed empty block
                        # æ·»åŠ æŸ¥è¯¢å‚æ•°ä»¥è§¦å‘æ•°æ®åº“æ“ä½œ
                        params = {"limit": 100, "sort": "created_at", "filter": "active"}
                        async with self.session.get(f"{self.base_url}{endpoint}", params=params) as response:
                            duration_ms = (time.time() - request_start) * 1000
                            response_times.append(duration_ms)

                            if response.status in [200, 201]:
                                successful_requests += 1
                            else:
                                failed_requests += 1

                    except Exception as e:
                        duration_ms = (time.time() - request_start) * 1000
                        response_times.append(duration_ms)
                        failed_requests += 1

                    await asyncio.sleep(0.05)  # 50msé—´éš”

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # è®¡ç®—æ•°æ®åº“æŸ¥è¯¢æ—¶é—´ (å‡è®¾å“åº”æ—¶é—´ä¸»è¦æ¥è‡ªæ•°æ®åº“)
        avg_db_time_ms = statistics.mean(response_times) if response_times else 0

        return PerformanceTestResult(
            test_name="Database Performance Test",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            total_requests=len(response_times),
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            requests_per_second=len(response_times) / duration if duration > 0 else 0,
            avg_response_time_ms=statistics.mean(response_times) if response_times else 0,
            p50_response_time_ms=self._percentile(response_times, 50),
            p95_response_time_ms=self._percentile(response_times, 95),
            p99_response_time_ms=self._percentile(response_times, 99),
            max_response_time_ms=max(response_times) if response_times else 0,
            min_response_time_ms=min(response_times) if response_times else 0,
            error_rate_percent=(failed_requests / len(response_times) * 100) if response_times else 0,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0,
            database_query_time_ms=avg_db_time_ms
        )

    async def _cache_performance_test(self) -> PerformanceTestResult:
        """ç¼“å­˜æ€§èƒ½æµ‹è¯•"""
        logger.info("æ‰§è¡Œç¼“å­˜æ€§èƒ½æµ‹è¯•...")

        start_time = datetime.now()
        response_times = []
        successful_requests = 0
        failed_requests = 0
        cache_hits = 0
        cache_misses = 0

        self.cpu_samples.clear()
        self.memory_samples.clear()

        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            pass  # Auto-fixed empty block
            # ç¬¬ä¸€è½®ï¼šé¢„çƒ­ç¼“å­˜
            warmup_endpoints = ["/api/users/1", "/api/users/2", "/api/users/3", "/api/config"]

            for endpoint in warmup_endpoints:
                for _ in range(5):  # é¢„çƒ­5æ¬¡
                    try:
                        async with self.session.get(f"{self.base_url}{endpoint}") as response:
                            if response.status == 200:
                                cache_misses += 1
                    except:
                        pass
                    await asyncio.sleep(0.01)

            # ç¬¬äºŒè½®ï¼šæµ‹è¯•ç¼“å­˜å‘½ä¸­
            for endpoint in warmup_endpoints:
                for _ in range(20):  # æ¯ä¸ªç«¯ç‚¹20æ¬¡è¯·æ±‚
                    request_start = time.time()

                    try:
                        async with self.session.get(f"{self.base_url}{endpoint}") as response:
                            duration_ms = (time.time() - request_start) * 1000
                            response_times.append(duration_ms)

                            if response.status == 200:
                                successful_requests += 1
                                # å‡è®¾å“åº”æ—¶é—´<50msä¸ºç¼“å­˜å‘½ä¸­
                                if duration_ms < 50:
                                    cache_hits += 1
                                else:
                                    cache_misses += 1
                            else:
                                failed_requests += 1

                    except Exception as e:
                        duration_ms = (time.time() - request_start) * 1000
                        response_times.append(duration_ms)
                        failed_requests += 1
                        cache_misses += 1

                    await asyncio.sleep(0.01)  # 10msé—´éš”

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        total_cache_requests = cache_hits + cache_misses
        cache_hit_rate = (cache_hits / total_cache_requests * 100) if total_cache_requests > 0 else 0

        return PerformanceTestResult(
            test_name="Cache Performance Test",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            total_requests=len(response_times),
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            requests_per_second=len(response_times) / duration if duration > 0 else 0,
            avg_response_time_ms=statistics.mean(response_times) if response_times else 0,
            p50_response_time_ms=self._percentile(response_times, 50),
            p95_response_time_ms=self._percentile(response_times, 95),
            p99_response_time_ms=self._percentile(response_times, 99),
            max_response_time_ms=max(response_times) if response_times else 0,
            min_response_time_ms=min(response_times) if response_times else 0,
            error_rate_percent=(failed_requests / len(response_times) * 100) if response_times else 0,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0,
            cache_hit_rate_percent=cache_hit_rate,
            additional_metrics={
                "cache_hits": cache_hits,
                "cache_misses": cache_misses,
                "total_cache_requests": total_cache_requests
            }
        )

    async def _stress_test(self) -> PerformanceTestResult:
        """å‹åŠ›æµ‹è¯• - æé™è´Ÿè½½"""
        logger.info("æ‰§è¡Œå‹åŠ›æµ‹è¯• (æé™è´Ÿè½½)...")

        config = self.test_configs['stress_test']
        start_time = datetime.now()

        self.cpu_samples.clear()
        self.memory_samples.clear()

        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            results = await self._execute_concurrent_test(
                concurrent_users=config['concurrent_users'],
                duration_seconds=config['duration_seconds'],
                ramp_up_seconds=config['ramp_up_seconds']
            )

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        return PerformanceTestResult(
            test_name=f"Stress Test ({config['concurrent_users']} users)",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            **results,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0
        )

    async def _execute_concurrent_test(self, concurrent_users: int, duration_seconds: int,
                                     ramp_up_seconds: int, endpoint: str = "/api/health") -> Dict[str, Any]:
        """æ‰§è¡Œå¹¶å‘æµ‹è¯•"""
        logger.info(f"æ‰§è¡Œå¹¶å‘æµ‹è¯•: {concurrent_users}ç”¨æˆ·, æŒç»­{duration_seconds}ç§’")

        response_times = []
        successful_requests = 0
        failed_requests = 0

        # è®¡ç®—æ¯ç§’å¯åŠ¨çš„ç”¨æˆ·æ•°
        users_per_second = concurrent_users / ramp_up_seconds if ramp_up_seconds > 0 else concurrent_users

        async def user_session(user_id: int, start_delay: float):
            """å•ä¸ªç”¨æˆ·ä¼šè¯"""
            await asyncio.sleep(start_delay)

            session_start = time.time()
            session_end = session_start + duration_seconds

            nonlocal successful_requests, failed_requests, response_times

            while time.time() < session_end:
                request_start = time.time()

                try:
                    async with self.session.get(f"{self.base_url}{endpoint}") as response:
                        duration_ms = (time.time() - request_start) * 1000
                        response_times.append(duration_ms)

                        if response.status in [200, 201, 202]:
                            successful_requests += 1
                        else:
                            failed_requests += 1

                except Exception as e:
                    duration_ms = (time.time() - request_start) * 1000
                    response_times.append(duration_ms)
                    failed_requests += 1

                # éšæœºç­‰å¾…æ—¶é—´ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
                await asyncio.sleep(random.uniform(0.1, 1.0))

        # åˆ›å»ºå¹¶å¯åŠ¨ç”¨æˆ·ä¼šè¯
        tasks = []
        for user_id in range(concurrent_users):
            start_delay = (user_id / users_per_second) if users_per_second > 0 else 0
            task = asyncio.create_task(user_session(user_id, start_delay))
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ç”¨æˆ·ä¼šè¯å®Œæˆ
        await asyncio.gather(*tasks, return_exceptions=True)

        total_requests = len(response_times)

        return {
            'total_requests': total_requests,
            'successful_requests': successful_requests,
            'failed_requests': failed_requests,
            'requests_per_second': total_requests / duration_seconds if duration_seconds > 0 else 0,
            'avg_response_time_ms': statistics.mean(response_times) if response_times else 0,
            'p50_response_time_ms': self._percentile(response_times, 50),
            'p95_response_time_ms': self._percentile(response_times, 95),
            'p99_response_time_ms': self._percentile(response_times, 99),
            'max_response_time_ms': max(response_times) if response_times else 0,
            'min_response_time_ms': min(response_times) if response_times else 0,
            'error_rate_percent': (failed_requests / total_requests * 100) if total_requests > 0 else 0
        }

    async def _monitor_system_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æº"""
        while True:
            try:
                cpu_percent = psutil.cpu_percent(interval=1)
                memory_info = psutil.virtual_memory()
                memory_mb = memory_info.used / 1024 / 1024

                self.cpu_samples.append(cpu_percent)
                self.memory_samples.append(memory_mb)

                await asyncio.sleep(1)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ç›‘æ§ç³»ç»Ÿèµ„æºå¤±è´¥: {e}")
                await asyncio.sleep(1)

    def _percentile(self, data: List[float], percentile: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0.0

        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]

    async def _generate_comprehensive_report(self) -> PerformanceReport:
        """ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        logger.info("ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š...")

        # è®¡ç®—æ•´ä½“è¯„åˆ†
        overall_score = self._calculate_overall_score()

        # è¯†åˆ«ç“¶é¢ˆ
        bottlenecks = self._identify_bottlenecks()

        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(bottlenecks)

        # æ”¶é›†ç³»ç»Ÿä¿¡æ¯
        system_info = {
            "cpu_count": psutil.cpu_count(),
            "total_memory_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024,
            "platform": psutil.platform.platform(),
            "python_version": psutil.sys.version,
            "test_duration_minutes": sum(r.duration_seconds for r in self.test_results) / 60
        }

        return PerformanceReport(
            timestamp=datetime.now(),
            overall_score=overall_score,
            test_results=self.test_results,
            system_info=system_info,
            bottlenecks=bottlenecks,
            recommendations=recommendations,
            charts_generated=[]
        )

    def _calculate_overall_score(self) -> float:
        """è®¡ç®—æ•´ä½“æ€§èƒ½è¯„åˆ† (0-100)"""
        if not self.test_results:
            return 0.0

        scores = []

        for result in self.test_results:
            score = 100.0

            # å“åº”æ—¶é—´è¯„åˆ† (30%)
            if result.p95_response_time_ms > 2000:
                score -= 30
            elif result.p95_response_time_ms > 1000:
                score -= 20
            elif result.p95_response_time_ms > 500:
                score -= 10

            # é”™è¯¯ç‡è¯„åˆ† (25%)
            if result.error_rate_percent > 5:
                score -= 25
            elif result.error_rate_percent > 1:
                score -= 10
            elif result.error_rate_percent > 0.1:
                score -= 5

            # ååé‡è¯„åˆ† (20%)
            if result.requests_per_second < 10:
                score -= 20
            elif result.requests_per_second < 50:
                score -= 10
            elif result.requests_per_second < 100:
                score -= 5

            # ç³»ç»Ÿèµ„æºè¯„åˆ† (25%)
            if result.cpu_usage_percent > 90:
                score -= 15
            elif result.cpu_usage_percent > 80:
                score -= 10

            if result.memory_usage_mb > 2048:  # 2GB
                score -= 10
            elif result.memory_usage_mb > 1024:  # 1GB
                score -= 5

            scores.append(max(0, score))

        return statistics.mean(scores)

    def _identify_bottlenecks(self) -> List[str]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []

        for result in self.test_results:
            pass  # Auto-fixed empty block
            # å“åº”æ—¶é—´ç“¶é¢ˆ
            if result.p95_response_time_ms > 1000:
                bottlenecks.append(f"{result.test_name}: P95å“åº”æ—¶é—´è¿‡é«˜ ({result.p95_response_time_ms:.1f}ms)")

            # é”™è¯¯ç‡ç“¶é¢ˆ
            if result.error_rate_percent > 1:
                bottlenecks.append(f"{result.test_name}: é”™è¯¯ç‡è¿‡é«˜ ({result.error_rate_percent:.1f}%)")

            # ååé‡ç“¶é¢ˆ
            if result.requests_per_second < 50:
                bottlenecks.append(f"{result.test_name}: ååé‡è¿‡ä½ ({result.requests_per_second:.1f} RPS)")

            # CPUç“¶é¢ˆ
            if result.cpu_usage_percent > 80:
                bottlenecks.append(f"{result.test_name}: CPUä½¿ç”¨ç‡è¿‡é«˜ ({result.cpu_usage_percent:.1f}%)")

            # å†…å­˜ç“¶é¢ˆ
            if result.memory_usage_mb > 1024:
                bottlenecks.append(f"{result.test_name}: å†…å­˜ä½¿ç”¨è¿‡é«˜ ({result.memory_usage_mb:.1f}MB)")

            # ç¼“å­˜ç“¶é¢ˆ
            if result.cache_hit_rate_percent > 0 and result.cache_hit_rate_percent < 80:
                bottlenecks.append(f"{result.test_name}: ç¼“å­˜å‘½ä¸­ç‡ä½ ({result.cache_hit_rate_percent:.1f}%)")

            # æ•°æ®åº“ç“¶é¢ˆ
            if result.database_query_time_ms > 500:
                bottlenecks.append(f"{result.test_name}: æ•°æ®åº“æŸ¥è¯¢æ—¶é—´è¿‡é•¿ ({result.database_query_time_ms:.1f}ms)")

        return list(set(bottlenecks))  # å»é‡

    def _generate_recommendations(self, bottlenecks: List[str]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        for bottleneck in bottlenecks:
            if "å“åº”æ—¶é—´è¿‡é«˜" in bottleneck:
                recommendations.append("å»ºè®®ä¼˜åŒ–åº”ç”¨é€»è¾‘ï¼Œæ·»åŠ ç¼“å­˜å±‚ï¼Œæˆ–ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢")
            elif "é”™è¯¯ç‡è¿‡é«˜" in bottleneck:
                recommendations.append("æ£€æŸ¥åº”ç”¨æ—¥å¿—ï¼Œä¿®å¤é”™è¯¯å¤„ç†é€»è¾‘ï¼Œå¢å¼ºç³»ç»Ÿç¨³å®šæ€§")
            elif "ååé‡è¿‡ä½" in bottleneck:
                recommendations.append("è€ƒè™‘æ°´å¹³æ‰©å±•ï¼Œä¼˜åŒ–èµ„æºé…ç½®ï¼Œæˆ–ä½¿ç”¨è´Ÿè½½å‡è¡¡")
            elif "CPUä½¿ç”¨ç‡è¿‡é«˜" in bottleneck:
                recommendations.append("ä¼˜åŒ–CPUå¯†é›†å‹æ“ä½œï¼Œè€ƒè™‘å‚ç›´æ‰©å±•æˆ–åˆ†å¸ƒå¼å¤„ç†")
            elif "å†…å­˜ä½¿ç”¨è¿‡é«˜" in bottleneck:
                recommendations.append("ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œæ£€æŸ¥å†…å­˜æ³„æ¼ï¼Œè€ƒè™‘å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–ç®—æ³•")
            elif "ç¼“å­˜å‘½ä¸­ç‡ä½" in bottleneck:
                recommendations.append("ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ï¼Œå¢åŠ ç¼“å­˜é¢„çƒ­ï¼Œè°ƒæ•´ç¼“å­˜TTL")
            elif "æ•°æ®åº“æŸ¥è¯¢æ—¶é—´è¿‡é•¿" in bottleneck:
                recommendations.append("ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•ï¼Œé‡æ„æŸ¥è¯¢è¯­å¥ï¼Œè€ƒè™‘æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–")

        # æ·»åŠ é€šç”¨å»ºè®®
        if not recommendations:
            recommendations.append("ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œå»ºè®®å®šæœŸç›‘æ§å¹¶æŒç»­ä¼˜åŒ–")

        return list(set(recommendations))  # å»é‡

    async def _generate_performance_charts(self):
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨"""
        logger.info("ç”Ÿæˆæ€§èƒ½å›¾è¡¨...")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        charts_dir = Path("performance_charts")
        charts_dir.mkdir(exist_ok=True)

        # 1. å“åº”æ—¶é—´å¯¹æ¯”å›¾
        self._create_response_time_chart(charts_dir)

        # 2. ååé‡å¯¹æ¯”å›¾
        self._create_throughput_chart(charts_dir)

        # 3. ç³»ç»Ÿèµ„æºä½¿ç”¨å›¾
        self._create_resource_usage_chart(charts_dir)

        # 4. é”™è¯¯ç‡å›¾
        self._create_error_rate_chart(charts_dir)

        logger.info(f"å›¾è¡¨å·²ç”Ÿæˆåˆ° {charts_dir} ç›®å½•")

    def _create_response_time_chart(self, charts_dir: Path):
        """åˆ›å»ºå“åº”æ—¶é—´å¯¹æ¯”å›¾"""
        try:
            test_names = [r.test_name for r in self.test_results]
            avg_times = [r.avg_response_time_ms for r in self.test_results]
            p95_times = [r.p95_response_time_ms for r in self.test_results]
            p99_times = [r.p99_response_time_ms for r in self.test_results]

            plt.figure(figsize=(12, 6))
            x = range(len(test_names))

            plt.bar([i - 0.25 for i in x], avg_times, 0.25, label='Average', alpha=0.8)
            plt.bar(x, p95_times, 0.25, label='P95', alpha=0.8)
            plt.bar([i + 0.25 for i in x], p99_times, 0.25, label='P99', alpha=0.8)

            plt.xlabel('Test Cases')
            plt.ylabel('Response Time (ms)')
            plt.title('Response Time Comparison')
            plt.xticks(x, test_names, rotation=45, ha='right')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()

            plt.savefig(charts_dir / "response_time_comparison.png", dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            logger.error(f"åˆ›å»ºå“åº”æ—¶é—´å›¾è¡¨å¤±è´¥: {e}")

    def _create_throughput_chart(self, charts_dir: Path):
        """åˆ›å»ºååé‡å¯¹æ¯”å›¾"""
        try:
            test_names = [r.test_name for r in self.test_results]
            throughputs = [r.requests_per_second for r in self.test_results]

            plt.figure(figsize=(10, 6))
            bars = plt.bar(test_names, throughputs, alpha=0.7, color='skyblue')

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, throughputs):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                        f'{value:.1f}', ha='center', va='bottom')

            plt.xlabel('Test Cases')
            plt.ylabel('Requests per Second')
            plt.title('Throughput Comparison')
            plt.xticks(rotation=45, ha='right')
            plt.grid(True, alpha=0.3, axis='y')
            plt.tight_layout()

            plt.savefig(charts_dir / "throughput_comparison.png", dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            logger.error(f"åˆ›å»ºååé‡å›¾è¡¨å¤±è´¥: {e}")

    def _create_resource_usage_chart(self, charts_dir: Path):
        """åˆ›å»ºç³»ç»Ÿèµ„æºä½¿ç”¨å›¾"""
        try:
            test_names = [r.test_name for r in self.test_results]
            cpu_usage = [r.cpu_usage_percent for r in self.test_results]
            memory_usage = [r.memory_usage_mb for r in self.test_results]

            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

            # CPUä½¿ç”¨ç‡
            ax1.bar(test_names, cpu_usage, alpha=0.7, color='orange')
            ax1.set_ylabel('CPU Usage (%)')
            ax1.set_title('CPU Usage by Test')
            ax1.set_xticklabels(test_names, rotation=45, ha='right')
            ax1.grid(True, alpha=0.3)

            # å†…å­˜ä½¿ç”¨é‡
            ax2.bar(test_names, memory_usage, alpha=0.7, color='green')
            ax2.set_ylabel('Memory Usage (MB)')
            ax2.set_title('Memory Usage by Test')
            ax2.set_xticklabels(test_names, rotation=45, ha='right')
            ax2.grid(True, alpha=0.3)

            plt.tight_layout()
            plt.savefig(charts_dir / "resource_usage.png", dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            logger.error(f"åˆ›å»ºèµ„æºä½¿ç”¨å›¾è¡¨å¤±è´¥: {e}")

    def _create_error_rate_chart(self, charts_dir: Path):
        """åˆ›å»ºé”™è¯¯ç‡å›¾"""
        try:
            test_names = [r.test_name for r in self.test_results]
            error_rates = [r.error_rate_percent for r in self.test_results]

            plt.figure(figsize=(10, 6))
            colors = ['red' if rate > 1 else 'yellow' if rate > 0.1 else 'green' for rate in error_rates]
            bars = plt.bar(test_names, error_rates, alpha=0.7, color=colors)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, error_rates):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{value:.2f}%', ha='center', va='bottom')

            plt.xlabel('Test Cases')
            plt.ylabel('Error Rate (%)')
            plt.title('Error Rate by Test')
            plt.xticks(rotation=45, ha='right')
            plt.grid(True, alpha=0.3, axis='y')
            plt.tight_layout()

            plt.savefig(charts_dir / "error_rate.png", dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            logger.error(f"åˆ›å»ºé”™è¯¯ç‡å›¾è¡¨å¤±è´¥: {e}")

    async def save_report_to_file(self, report: PerformanceReport, filename: str = None):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if filename is None:
            filename = f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        report_data = {
            "timestamp": report.timestamp.isoformat(),
            "overall_score": report.overall_score,
            "system_info": report.system_info,
            "bottlenecks": report.bottlenecks,
            "recommendations": report.recommendations,
            "charts_generated": report.charts_generated,
            "test_results": [asdict(result) for result in report.test_results]
        }

        # å¤„ç†datetimeå­—æ®µ
        for test_result in report_data["test_results"]:
            test_result["start_time"] = test_result["start_time"].isoformat()
            test_result["end_time"] = test_result["end_time"].isoformat()

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        logger.info(f"æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")

async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨Claude Enhancerç»¼åˆæ€§èƒ½æµ‹è¯•")

    # é…ç½®æµ‹è¯•ç›®æ ‡ - å¯ä»¥æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    target_url = "http://localhost:8080"  # é»˜è®¤æµ‹è¯•æœ¬åœ°æœåŠ¡

    # æ£€æŸ¥æ˜¯å¦æœ‰æ€§èƒ½æ¼”ç¤ºæœåŠ¡è¿è¡Œ
    try:
        import subprocess
        import sys

        # å¯åŠ¨æ€§èƒ½æ¼”ç¤ºæœåŠ¡
        logger.info("å¯åŠ¨æ€§èƒ½æ¼”ç¤ºæœåŠ¡...")
        demo_process = subprocess.Popen([
            sys.executable, "run_performance_demo.py"
        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

        # ç­‰å¾…æœåŠ¡å¯åŠ¨
        await asyncio.sleep(10)

    except Exception as e:
        logger.warning(f"æ— æ³•å¯åŠ¨æ¼”ç¤ºæœåŠ¡: {e}")
        logger.info("å°†æµ‹è¯•æ¨¡æ‹Ÿç«¯ç‚¹...")

    try:
        pass  # Auto-fixed empty block
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = ComprehensivePerformanceTester(target_url)

        # åˆå§‹åŒ–
        await tester.initialize()

        # è¿è¡Œæµ‹è¯•
        report = await tester.run_comprehensive_tests()

        # ä¿å­˜æŠ¥å‘Š
        await tester.save_report_to_file(report)

        # æ‰“å°æ‘˜è¦
    print("\n" + "="*80)
    print("ğŸ¯ PERFECT21 æ€§èƒ½æµ‹è¯•æŠ¥å‘Šæ‘˜è¦")
    print("="*80)
    print(f"æ•´ä½“æ€§èƒ½è¯„åˆ†: {report.overall_score:.1f}/100")
    print(f"æµ‹è¯•æ—¶é—´: {report.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æµ‹è¯•é¡¹ç›®æ•°: {len(report.test_results)}")

        if report.bottlenecks:
        print(f"\nâš ï¸ å‘ç°çš„ç“¶é¢ˆ ({len(report.bottlenecks)}ä¸ª):")
            for bottleneck in report.bottlenecks:
                print(f"  - {bottleneck}")

        if report.recommendations:
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®® ({len(report.recommendations)}ä¸ª):")
            for recommendation in report.recommendations:
            print(f"  - {recommendation}")

    print("\nğŸ“Š è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for result in report.test_results:
            print(f"\n{result.test_name}:")
            print(f"  RPS: {result.requests_per_second:.1f}")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {result.avg_response_time_ms:.1f}ms")
            print(f"  P95å“åº”æ—¶é—´: {result.p95_response_time_ms:.1f}ms")
            print(f"  é”™è¯¯ç‡: {result.error_rate_percent:.2f}%")
            print(f"  CPUä½¿ç”¨: {result.cpu_usage_percent:.1f}%")
            print(f"  å†…å­˜ä½¿ç”¨: {result.memory_usage_mb:.1f}MB")
            if result.cache_hit_rate_percent > 0:
                print(f"  ç¼“å­˜å‘½ä¸­ç‡: {result.cache_hit_rate_percent:.1f}%")

    print(f"\nğŸ“ æŠ¥å‘Šæ–‡ä»¶: performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    print("ğŸ“ˆ å›¾è¡¨æ–‡ä»¶: performance_charts/ ç›®å½•")
    # print("="*80)

        return 0

    except Exception as e:
        logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return 1

    finally:
        pass  # Auto-fixed empty block
        # æ¸…ç†
        try:
            if 'demo_process' in locals():
                demo_process.terminate()
        except:
            pass

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)