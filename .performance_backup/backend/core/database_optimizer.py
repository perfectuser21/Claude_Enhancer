"""
Performance Optimization: Database Query Optimizer
æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–å™¨ - ä¼ä¸šçº§æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿ
"""

import asyncio
import asyncpg
import logging
import time
from typing import Any, Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from contextlib import asynccontextmanager
from datetime import datetime, timedelta
from functools import wraps
import json
import hashlib
from enum import Enum

logger = logging.getLogger(__name__)


class QueryType(Enum):
    """æŸ¥è¯¢ç±»å‹"""

    SELECT = "SELECT"
    INSERT = "INSERT"
    UPDATE = "UPDATE"
    DELETE = "DELETE"
    AGGREGATE = "AGGREGATE"


@dataclass
class QueryStats:
    """æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯"""

    query_hash: str
    query_type: QueryType
    avg_duration: float = 0.0
    max_duration: float = 0.0
    min_duration: float = float("inf")
    total_executions: int = 0
    total_duration: float = 0.0
    last_executed: Optional[datetime] = None
    slow_threshold_exceeded: int = 0
    error_count: int = 0
    cached_hits: int = 0


@dataclass
class DatabaseConfig:
    """æ•°æ®åº“é…ç½®"""

    url: str
    pool_size: int = 20
    max_overflow: int = 30
    pool_timeout: int = 30
    pool_recycle: int = 3600
    slow_query_threshold: float = 1.0  # 1ç§’
    enable_query_cache: bool = True
    query_cache_ttl: int = 300  # 5åˆ†é’Ÿ
    enable_prepared_statements: bool = True
    statement_cache_size: int = 100


class DatabaseOptimizer:
    """æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self, config: DatabaseConfig):
        self.config = config
        self.pool = None
        self.query_stats: Dict[str, QueryStats] = {}
        self.query_cache: Dict[str, Tuple[Any, datetime]] = {}
        self.prepared_statements: Dict[str, str] = {}
        self._lock = asyncio.Lock()

    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
        try:
            self.pool = await asyncpg.create_pool(
                self.config.url,
                min_size=1,
                max_size=self.config.pool_size,
                command_timeout=self.config.pool_timeout,
                server_settings={
                    "application_name": "claude-enhancer_optimizer",
                    "tcp_keepalives_idle": "300",
                    "tcp_keepalives_interval": "30",
                    "tcp_keepalives_count": "3",
                },
            )

            # é¢„çƒ­è¿æ¥æ± 
            async with self.pool.acquire() as conn:
                await conn.execute("SELECT 1")

            logger.info(f"âœ… æ•°æ®åº“ä¼˜åŒ–å™¨åˆå§‹åŒ–æˆåŠŸ - è¿æ¥æ± å¤§å°: {self.config.pool_size}")

            # å¯åŠ¨åå°ä»»åŠ¡
            asyncio.create_task(self._periodic_optimization())
            asyncio.create_task(self._cache_cleanup())
            asyncio.create_task(self._stats_aggregation())

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise

    def _hash_query(self, query: str, params: tuple = None) -> str:
        """ç”ŸæˆæŸ¥è¯¢å“ˆå¸Œ"""
        query_normalized = " ".join(query.split())  # æ ‡å‡†åŒ–ç©ºæ ¼
        if params:
            query_with_params = f"{query_normalized}:{str(params)}"
        else:
            query_with_params = query_normalized
        return hashlib.md5(query_with_params.encode()).hexdigest()[:16]

    def _detect_query_type(self, query: str) -> QueryType:
        """æ£€æµ‹æŸ¥è¯¢ç±»å‹"""
        query_upper = query.strip().upper()
        if query_upper.startswith("SELECT"):
            if any(
                agg in query_upper
                for agg in ["COUNT(", "SUM(", "AVG(", "MAX(", "MIN(", "GROUP BY"]
            ):
                return QueryType.AGGREGATE
            return QueryType.SELECT
        elif query_upper.startswith("INSERT"):
            return QueryType.INSERT
        elif query_upper.startswith("UPDATE"):
            return QueryType.UPDATE
        elif query_upper.startswith("DELETE"):
            return QueryType.DELETE
        else:
            return QueryType.SELECT

    async def execute_optimized(
        self, query: str, *args, fetch_type: str = "all"
    ) -> Any:
        """æ‰§è¡Œä¼˜åŒ–çš„æŸ¥è¯¢"""
        start_time = time.time()
        query_hash = self._hash_query(query, args)
        query_type = self._detect_query_type(query)

        # æ£€æŸ¥æŸ¥è¯¢ç¼“å­˜ï¼ˆä»…å¯¹SELECTæŸ¥è¯¢ï¼‰
        if (
            self.config.enable_query_cache
            and query_type in [QueryType.SELECT, QueryType.AGGREGATE]
            and query_hash in self.query_cache
        ):
            cached_result, cache_time = self.query_cache[query_hash]
            if datetime.now() - cache_time < timedelta(
                seconds=self.config.query_cache_ttl
            ):
                # æ›´æ–°ç»Ÿè®¡
                await self._update_stats(query_hash, query_type, 0.001, cached=True)
                logger.debug(f"ğŸ¯ æŸ¥è¯¢ç¼“å­˜å‘½ä¸­: {query_hash}")
                return cached_result
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del self.query_cache[query_hash]

        try:
            # æ‰§è¡ŒæŸ¥è¯¢
            async with self.pool.acquire() as conn:
                if fetch_type == "all":
                    result = await conn.fetch(query, *args)
                elif fetch_type == "one":
                    result = await conn.fetchrow(query, *args)
                elif fetch_type == "val":
                    result = await conn.fetchval(query, *args)
                elif fetch_type == "execute":
                    result = await conn.execute(query, *args)
                else:
                    result = await conn.fetch(query, *args)

            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            duration = time.time() - start_time

            # ç¼“å­˜SELECTæŸ¥è¯¢ç»“æœ
            if (
                self.config.enable_query_cache
                and query_type in [QueryType.SELECT, QueryType.AGGREGATE]
                and duration < self.config.slow_query_threshold
            ):  # åªç¼“å­˜å¿«é€ŸæŸ¥è¯¢
                self.query_cache[query_hash] = (result, datetime.now())

            # æ›´æ–°ç»Ÿè®¡
            await self._update_stats(query_hash, query_type, duration)

            # æ…¢æŸ¥è¯¢æ—¥å¿—
            if duration > self.config.slow_query_threshold:
                logger.warning(
                    f"ğŸŒ æ…¢æŸ¥è¯¢æ£€æµ‹ - è€—æ—¶: {duration:.3f}s, "
                    f"ç±»å‹: {query_type.value}, "
                    f"æŸ¥è¯¢: {query[:100]}..."
                )

            return result

        except Exception as e:
            duration = time.time() - start_time
            await self._update_stats(query_hash, query_type, duration, error=True)
            logger.error(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}, æŸ¥è¯¢: {query[:100]}...")
            raise

    async def _update_stats(
        self,
        query_hash: str,
        query_type: QueryType,
        duration: float,
        cached: bool = False,
        error: bool = False,
    ):
        """æ›´æ–°æŸ¥è¯¢ç»Ÿè®¡"""
        async with self._lock:
            if query_hash not in self.query_stats:
                self.query_stats[query_hash] = QueryStats(
                    query_hash=query_hash, query_type=query_type
                )

            stats = self.query_stats[query_hash]

            if error:
                stats.error_count += 1
                return

            if cached:
                stats.cached_hits += 1
                return

            stats.total_executions += 1
            stats.total_duration += duration
            stats.avg_duration = stats.total_duration / stats.total_executions
            stats.max_duration = max(stats.max_duration, duration)
            stats.min_duration = min(stats.min_duration, duration)
            stats.last_executed = datetime.now()

            if duration > self.config.slow_query_threshold:
                stats.slow_threshold_exceeded += 1

    async def execute_batch_optimized(
        self, query: str, params_list: List[tuple]
    ) -> List[Any]:
        """æ‰¹é‡æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢"""
        if not params_list:
            return []

        start_time = time.time()
        query_type = self._detect_query_type(query)

        try:
            # ä½¿ç”¨executemanyä¼˜åŒ–æ‰¹é‡æ“ä½œ
            async with self.pool.acquire() as conn:
                async with conn.transaction():
                    if query_type == QueryType.INSERT:
                        # æ‰¹é‡æ’å…¥ä¼˜åŒ–
                        results = await conn.executemany(query, params_list)
                    else:
                        # å…¶ä»–ç±»å‹æŸ¥è¯¢
                        results = []
                        for params in params_list:
                            result = await conn.fetch(query, *params)
                            results.append(result)

            duration = time.time() - start_time

            logger.info(
                f"ğŸ“¦ æ‰¹é‡æŸ¥è¯¢å®Œæˆ - "
                f"ç±»å‹: {query_type.value}, "
                f"æ•°é‡: {len(params_list)}, "
                f"è€—æ—¶: {duration:.3f}s"
            )

            return results

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æŸ¥è¯¢å¤±è´¥: {e}")
            raise

    async def execute_with_connection_optimization(
        self, queries: List[Tuple[str, tuple]]
    ) -> List[Any]:
        """åœ¨å•ä¸ªè¿æ¥ä¸­æ‰§è¡Œå¤šä¸ªæŸ¥è¯¢ï¼ˆå‡å°‘è¿æ¥å¼€é”€ï¼‰"""
        start_time = time.time()
        results = []

        try:
            async with self.pool.acquire() as conn:
                async with conn.transaction():
                    for query, params in queries:
                        result = await conn.fetch(query, *params)
                        results.append(result)

            duration = time.time() - start_time

            logger.debug(
                f"ğŸ”— è¿æ¥ä¼˜åŒ–æŸ¥è¯¢å®Œæˆ - " f"æŸ¥è¯¢æ•°: {len(queries)}, " f"è€—æ—¶: {duration:.3f}s"
            )

            return results

        except Exception as e:
            logger.error(f"âŒ è¿æ¥ä¼˜åŒ–æŸ¥è¯¢å¤±è´¥: {e}")
            raise

    async def analyze_table_performance(self, table_name: str) -> Dict[str, Any]:
        """åˆ†æè¡¨æ€§èƒ½"""
        analysis = {}

        try:
            async with self.pool.acquire() as conn:
                # è¡¨å¤§å°åˆ†æ
                size_query = """
                    SELECT
                        pg_size_pretty(pg_total_relation_size($1)) as total_size,
                        pg_size_pretty(pg_relation_size($1)) as table_size,
                        pg_size_pretty(pg_indexes_size($1)) as indexes_size
                """
                size_result = await conn.fetchrow(size_query, table_name)
                analysis["size"] = dict(size_result) if size_result else {}

                # ç´¢å¼•ä½¿ç”¨åˆ†æ
                index_query = """
                    SELECT
                        indexname,
                        idx_scan,
                        idx_tup_read,
                        idx_tup_fetch
                    FROM pg_stat_user_indexes
                    WHERE relname = $1
                """
                index_results = await conn.fetch(index_query, table_name)
                analysis["indexes"] = [dict(row) for row in index_results]

                # è¡¨ç»Ÿè®¡ä¿¡æ¯
                stats_query = """
                    SELECT
                        seq_scan,
                        seq_tup_read,
                        idx_scan,
                        idx_tup_fetch,
                        n_tup_ins,
                        n_tup_upd,
                        n_tup_del,
                        n_tup_hot_upd,
                        n_live_tup,
                        n_dead_tup
                    FROM pg_stat_user_tables
                    WHERE relname = $1
                """
                stats_result = await conn.fetchrow(stats_query, table_name)
                analysis["stats"] = dict(stats_result) if stats_result else {}

                # å»ºè®®ä¼˜åŒ–
                suggestions = []
                if stats_result:
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦VACUUM
                    if stats_result["n_dead_tup"] > stats_result["n_live_tup"] * 0.1:
                        suggestions.append("å»ºè®®æ‰§è¡ŒVACUUMæ¸…ç†æ­»å…ƒç»„")

                    # æ£€æŸ¥ç´¢å¼•æ•ˆç‡
                    if stats_result["seq_scan"] > stats_result["idx_scan"] * 2:
                        suggestions.append("è€ƒè™‘æ·»åŠ ç´¢å¼•ä»¥å‡å°‘é¡ºåºæ‰«æ")

                analysis["suggestions"] = suggestions

        except Exception as e:
            logger.error(f"âŒ è¡¨æ€§èƒ½åˆ†æå¤±è´¥: {e}")
            analysis["error"] = str(e)

        return analysis

    async def get_slow_queries(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æ…¢æŸ¥è¯¢åˆ—è¡¨"""
        slow_queries = []

        for query_hash, stats in self.query_stats.items():
            if stats.slow_threshold_exceeded > 0:
                slow_queries.append(
                    {
                        "query_hash": query_hash,
                        "query_type": stats.query_type.value,
                        "avg_duration": stats.avg_duration,
                        "max_duration": stats.max_duration,
                        "total_executions": stats.total_executions,
                        "slow_count": stats.slow_threshold_exceeded,
                        "error_count": stats.error_count,
                        "last_executed": stats.last_executed.isoformat()
                        if stats.last_executed
                        else None,
                    }
                )

        # æŒ‰å¹³å‡æ‰§è¡Œæ—¶é—´æ’åº
        slow_queries.sort(key=lambda x: x["avg_duration"], reverse=True)
        return slow_queries[:limit]

    async def get_database_stats(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_queries": len(self.query_stats),
            "cache_hit_rate": 0.0,
            "avg_query_time": 0.0,
            "slow_query_count": 0,
            "error_rate": 0.0,
            "cache_size": len(self.query_cache),
        }

        if self.query_stats:
            total_executions = sum(
                qs.total_executions for qs in self.query_stats.values()
            )
            total_cached_hits = sum(qs.cached_hits for qs in self.query_stats.values())
            total_duration = sum(qs.total_duration for qs in self.query_stats.values())
            total_errors = sum(qs.error_count for qs in self.query_stats.values())
            slow_queries = sum(
                1 for qs in self.query_stats.values() if qs.slow_threshold_exceeded > 0
            )

            if total_executions > 0:
                stats["cache_hit_rate"] = (
                    total_cached_hits / (total_executions + total_cached_hits)
                ) * 100
                stats["avg_query_time"] = total_duration / total_executions
                stats["error_rate"] = (total_errors / total_executions) * 100

            stats["slow_query_count"] = slow_queries

        return stats

    async def _periodic_optimization(self):
        """å®šæœŸä¼˜åŒ–ä»»åŠ¡"""
        while True:
            try:
                await asyncio.sleep(3600)  # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡

                # åˆ†ææ•°æ®åº“æ€§èƒ½
                await self._analyze_and_optimize()

                # æ¸…ç†ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿ç•™æœ€è¿‘24å°æ—¶ï¼‰
                await self._cleanup_old_stats()

            except Exception as e:
                logger.error(f"âŒ å®šæœŸä¼˜åŒ–ä»»åŠ¡å¤±è´¥: {e}")

    async def _analyze_and_optimize(self):
        """åˆ†æå¹¶ä¼˜åŒ–æ•°æ®åº“"""
        try:
            async with self.pool.acquire() as conn:
                # è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
                db_stats = await conn.fetch(
                    """
                    SELECT
                        datname,
                        numbackends,
                        xact_commit,
                        xact_rollback,
                        blks_read,
                        blks_hit,
                        tup_returned,
                        tup_fetched,
                        tup_inserted,
                        tup_updated,
                        tup_deleted
                    FROM pg_stat_database
                    WHERE datname = current_database()
                """
                )

                if db_stats:
                    stats = dict(db_stats[0])
                    buffer_hit_ratio = (
                        (stats["blks_hit"] / (stats["blks_hit"] + stats["blks_read"]))
                        * 100
                        if (stats["blks_hit"] + stats["blks_read"]) > 0
                        else 0
                    )

                    logger.info(
                        f"ğŸ“Š æ•°æ®åº“æ€§èƒ½åˆ†æ - "
                        f"ç¼“å†²åŒºå‘½ä¸­ç‡: {buffer_hit_ratio:.2f}%, "
                        f"æ´»è·ƒè¿æ¥: {stats['numbackends']}, "
                        f"æäº¤äº‹åŠ¡: {stats['xact_commit']}, "
                        f"å›æ»šäº‹åŠ¡: {stats['xact_rollback']}"
                    )

                    # å¦‚æœç¼“å†²åŒºå‘½ä¸­ç‡ä½äº95%ï¼Œå»ºè®®å¢åŠ shared_buffers
                    if buffer_hit_ratio < 95:
                        logger.warning("âš ï¸ ç¼“å†²åŒºå‘½ä¸­ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ shared_buffersé…ç½®")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆ†æå¤±è´¥: {e}")

    async def _cache_cleanup(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        while True:
            try:
                await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡

                now = datetime.now()
                expired_keys = []

                for key, (_, cache_time) in self.query_cache.items():
                    if now - cache_time > timedelta(
                        seconds=self.config.query_cache_ttl
                    ):
                        expired_keys.append(key)

                for key in expired_keys:
                    del self.query_cache[key]

                if expired_keys:
                    logger.debug(f"ğŸ§¹ æ¸…ç†è¿‡æœŸæŸ¥è¯¢ç¼“å­˜: {len(expired_keys)}ä¸ª")

            except Exception as e:
                logger.error(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

    async def _cleanup_old_stats(self):
        """æ¸…ç†æ—§ç»Ÿè®¡æ•°æ®"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=24)
            old_stats = []

            for query_hash, stats in self.query_stats.items():
                if stats.last_executed and stats.last_executed < cutoff_time:
                    old_stats.append(query_hash)

            for query_hash in old_stats:
                del self.query_stats[query_hash]

            if old_stats:
                logger.debug(f"ğŸ§¹ æ¸…ç†æ—§æŸ¥è¯¢ç»Ÿè®¡: {len(old_stats)}ä¸ª")

        except Exception as e:
            logger.error(f"âŒ ç»Ÿè®¡æ¸…ç†å¤±è´¥: {e}")

    async def _stats_aggregation(self):
        """ç»Ÿè®¡ä¿¡æ¯èšåˆ"""
        while True:
            try:
                await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿèšåˆä¸€æ¬¡

                stats = await self.get_database_stats()

                if stats["total_queries"] > 0:
                    logger.info(
                        f"ğŸ“ˆ æ•°æ®åº“æ€§èƒ½æ‘˜è¦ - "
                        f"æŸ¥è¯¢æ€»æ•°: {stats['total_queries']}, "
                        f"ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.2f}%, "
                        f"å¹³å‡æŸ¥è¯¢æ—¶é—´: {stats['avg_query_time']:.3f}s, "
                        f"æ…¢æŸ¥è¯¢æ•°: {stats['slow_query_count']}, "
                        f"é”™è¯¯ç‡: {stats['error_rate']:.2f}%"
                    )

            except Exception as e:
                logger.error(f"âŒ ç»Ÿè®¡èšåˆå¤±è´¥: {e}")

    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            async with self.pool.acquire() as conn:
                await conn.execute("SELECT 1")
                return True
        except:
            return False

    async def close(self):
        """å…³é—­è¿æ¥æ± """
        if self.pool:
            await self.pool.close()
            logger.info("ğŸ“´ æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")


def optimized_query(cache_ttl: int = 300, batch_size: Optional[int] = None):
    """æŸ¥è¯¢ä¼˜åŒ–è£…é¥°å™¨"""

    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # è·å–æ•°æ®åº“ä¼˜åŒ–å™¨å®ä¾‹
            optimizer = getattr(wrapper, "_optimizer", None)
            if not optimizer:
                return await func(*args, **kwargs)

            # æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢
            return await func(*args, **kwargs)

        return wrapper

    return decorator


# ä½¿ç”¨ç¤ºä¾‹
@optimized_query(cache_ttl=600)
async def get_user_by_id(user_id: int, optimizer: DatabaseOptimizer):
    """è·å–ç”¨æˆ·ä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    query = "SELECT * FROM users WHERE id = $1"
    return await optimizer.execute_optimized(query, user_id, fetch_type="one")


@optimized_query(batch_size=100)
async def bulk_insert_logs(logs: List[dict], optimizer: DatabaseOptimizer):
    """æ‰¹é‡æ’å…¥æ—¥å¿—ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    query = "INSERT INTO logs (level, message, timestamp) VALUES ($1, $2, $3)"
    params_list = [(log["level"], log["message"], log["timestamp"]) for log in logs]
    return await optimizer.execute_batch_optimized(query, params_list)
