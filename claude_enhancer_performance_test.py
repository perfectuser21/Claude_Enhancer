#!/usr/bin/env python3
"""
Claude Enhancer ç³»ç»Ÿæ€§èƒ½åˆ†æå·¥å…·
å®Œæ•´æµ‹è¯•æ‰€æœ‰performanceç›¸å…³è„šæœ¬çš„æ‰§è¡Œæ—¶é—´å’Œæ•ˆç‡
"""

import os
import time
import subprocess
import json
import sys
from pathlib import Path
from datetime import datetime
import statistics


class PerformanceAnalyzer:
    def __init__(self, claude_dir="/home/xx/dev/Perfect21/.claude"):
        self.claude_dir = Path(claude_dir)
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "system_info": self.get_system_info(),
            "test_results": {},
            "summary": {},
            "recommendations": [],
        }

    def get_system_info(self):
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        try:
            with open("/proc/cpuinfo", "r") as f:
                cpu_info = f.read()
            cpu_count = cpu_info.count("processor")

            with open("/proc/meminfo", "r") as f:
                mem_info = f.read()
            mem_total = (
                int(
                    [line for line in mem_info.split("\n") if "MemTotal" in line][
                        0
                    ].split()[1]
                )
                // 1024
            )

            return {
                "cpu_cores": cpu_count,
                "memory_mb": mem_total,
                "python_version": sys.version,
                "os": os.uname().sysname,
            }
        except Exception as e:
            return {"error": str(e)}

    def measure_execution_time(self, command, description="", runs=3):
        """æµ‹é‡å‘½ä»¤æ‰§è¡Œæ—¶é—´ï¼ˆå¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼ï¼‰"""
        times = []
        success_count = 0

        for i in range(runs):
            try:
                start_time = time.perf_counter()
                result = subprocess.run(
                    command,
                    shell=True,
                    capture_output=True,
                    text=True,
                    timeout=60,
                    cwd=self.claude_dir.parent,
                )
                end_time = time.perf_counter()

                execution_time = end_time - start_time
                times.append(execution_time)

                if result.returncode == 0:
                    success_count += 1

            except subprocess.TimeoutExpired:
                times.append(60.0)  # è¶…æ—¶è®°ä¸º60ç§’
            except Exception as e:
                print(f"æ‰§è¡Œé”™è¯¯ {command}: {e}")
                times.append(float("inf"))

        if times:
            avg_time = statistics.mean([t for t in times if t != float("inf")])
            min_time = (
                min([t for t in times if t != float("inf")])
                if any(t != float("inf") for t in times)
                else 0
            )
            max_time = (
                max([t for t in times if t != float("inf")])
                if any(t != float("inf") for t in times)
                else 0
            )
        else:
            avg_time = min_time = max_time = 0

        return {
            "description": description,
            "command": command,
            "runs": runs,
            "success_rate": success_count / runs,
            "avg_time": round(avg_time, 4),
            "min_time": round(min_time, 4),
            "max_time": round(max_time, 4),
            "all_times": [round(t, 4) for t in times],
        }

    def test_cleanup_scripts(self):
        """æµ‹è¯•æ‰€æœ‰cleanupè„šæœ¬ç‰ˆæœ¬"""
        cleanup_scripts = [
            (".claude/scripts/cleanup.sh", "æ ‡å‡†æ¸…ç†è„šæœ¬"),
            (".claude/scripts/performance_optimized_cleanup.sh", "æ€§èƒ½ä¼˜åŒ–æ¸…ç†è„šæœ¬"),
            (".claude/scripts/ultra_optimized_cleanup.sh", "è¶…çº§ä¼˜åŒ–æ¸…ç†è„šæœ¬"),
            (".claude/scripts/safe_cleanup.sh", "å®‰å…¨æ¸…ç†è„šæœ¬"),
        ]

        results = {}
        for script_path, description in cleanup_scripts:
            full_path = self.claude_dir.parent / script_path
            if full_path.exists():
                # ç¡®ä¿è„šæœ¬å¯æ‰§è¡Œ
                os.chmod(full_path, 0o755)
                command = f"bash {full_path}"
                results[script_path] = self.measure_execution_time(command, description)
            else:
                results[script_path] = {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}

        return results

    def test_performance_scripts(self):
        """æµ‹è¯•æ€§èƒ½ç›¸å…³è„šæœ¬"""
        performance_scripts = [
            (".claude/scripts/performance_benchmark.sh", "æ€§èƒ½åŸºå‡†æµ‹è¯•"),
            (".claude/scripts/ultra_performance_benchmark.sh", "è¶…çº§æ€§èƒ½åŸºå‡†æµ‹è¯•"),
            (".claude/scripts/quick_performance_test.sh", "å¿«é€Ÿæ€§èƒ½æµ‹è¯•"),
            (".claude/scripts/performance_comparison.sh", "æ€§èƒ½å¯¹æ¯”æµ‹è¯•"),
            (".claude/scripts/performance_monitor.sh", "æ€§èƒ½ç›‘æ§è„šæœ¬"),
        ]

        results = {}
        for script_path, description in performance_scripts:
            full_path = self.claude_dir.parent / script_path
            if full_path.exists():
                os.chmod(full_path, 0o755)
                command = f"bash {full_path}"
                results[script_path] = self.measure_execution_time(command, description)
            else:
                results[script_path] = {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}

        return results

    def test_python_scripts(self):
        """æµ‹è¯•Pythonæ€§èƒ½è„šæœ¬"""
        python_scripts = [
            (".claude/hooks/parallel_execution_optimizer.py", "å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–å™¨"),
            (".claude/hooks/performance_test.py", "æ€§èƒ½æµ‹è¯•è„šæœ¬"),
            (".claude/scripts/smart_document_loader.py", "æ™ºèƒ½æ–‡æ¡£åŠ è½½å™¨"),
        ]

        results = {}
        for script_path, description in python_scripts:
            full_path = self.claude_dir.parent / script_path
            if full_path.exists():
                command = f"python3 {full_path} --test"
                results[script_path] = self.measure_execution_time(command, description)
            else:
                results[script_path] = {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}

        return results

    def test_hook_system(self):
        """æµ‹è¯•Hookç³»ç»Ÿå“åº”æ—¶é—´"""
        hook_scripts = [
            (".claude/hooks/smart_agent_selector.sh", "æ™ºèƒ½Agenté€‰æ‹©å™¨"),
            (".claude/hooks/ultra_smart_agent_selector.sh", "è¶…çº§æ™ºèƒ½Agenté€‰æ‹©å™¨"),
            (".claude/hooks/smart_dispatcher.py", "æ™ºèƒ½è°ƒåº¦å™¨"),
            (".claude/hooks/enforcer.sh", "å¼ºåˆ¶æ‰§è¡Œå™¨"),
        ]

        results = {}
        for script_path, description in hook_scripts:
            full_path = self.claude_dir.parent / script_path
            if full_path.exists():
                if script_path.endswith(".py"):
                    command = f"python3 {full_path} --dry-run"
                else:
                    os.chmod(full_path, 0o755)
                    command = f"bash {full_path} --test"
                results[script_path] = self.measure_execution_time(
                    command, description, runs=5
                )
            else:
                results[script_path] = {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}

        return results

    def test_system_startup(self):
        """æµ‹è¯•ç³»ç»Ÿå¯åŠ¨æ—¶é—´"""
        startup_commands = [
            ("source .claude/scripts/load_config.sh", "é…ç½®åŠ è½½"),
            ("bash .claude/install.sh --dry-run", "å®‰è£…è„šæœ¬"),
            ("python3 .claude/scripts/config_validator.py", "é…ç½®éªŒè¯"),
        ]

        results = {}
        for command, description in startup_commands:
            results[command] = self.measure_execution_time(command, description)

        return results

    def analyze_file_sizes(self):
        """åˆ†ææ–‡ä»¶å¤§å°å’Œæ•°é‡"""
        file_stats = {
            "total_files": 0,
            "total_size_mb": 0,
            "largest_files": [],
            "file_types": {},
        }

        try:
            for root, dirs, files in os.walk(self.claude_dir):
                for file in files:
                    file_path = Path(root) / file
                    try:
                        size = file_path.stat().st_size
                        file_stats["total_files"] += 1
                        file_stats["total_size_mb"] += size / (1024 * 1024)

                        # è®°å½•æœ€å¤§çš„æ–‡ä»¶
                        file_stats["largest_files"].append((str(file_path), size))

                        # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
                        ext = file_path.suffix.lower()
                        if ext in file_stats["file_types"]:
                            file_stats["file_types"][ext] += 1
                        else:
                            file_stats["file_types"][ext] = 1

                    except OSError:
                        continue

            # æ’åºæœ€å¤§æ–‡ä»¶
            file_stats["largest_files"].sort(key=lambda x: x[1], reverse=True)
            file_stats["largest_files"] = file_stats["largest_files"][:10]
            file_stats["total_size_mb"] = round(file_stats["total_size_mb"], 2)

        except Exception as e:
            file_stats["error"] = str(e)

        return file_stats

    def generate_recommendations(self):
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åˆ†æcleanupè„šæœ¬æ€§èƒ½
        cleanup_results = self.results["test_results"].get("cleanup_scripts", {})
        cleanup_times = {}
        for script, result in cleanup_results.items():
            if isinstance(result, dict) and "avg_time" in result:
                cleanup_times[script] = result["avg_time"]

        if cleanup_times:
            fastest = min(cleanup_times, key=cleanup_times.get)
            slowest = max(cleanup_times, key=cleanup_times.get)
            recommendations.append(
                {
                    "category": "Cleanup Scripts",
                    "issue": f"æœ€æ…¢çš„cleanupè„šæœ¬: {slowest} ({cleanup_times[slowest]:.4f}s)",
                    "recommendation": f"å»ºè®®ä½¿ç”¨æœ€å¿«çš„è„šæœ¬: {fastest} ({cleanup_times[fastest]:.4f}s)",
                    "priority": "medium",
                }
            )

        # åˆ†æHookå“åº”æ—¶é—´
        hook_results = self.results["test_results"].get("hook_system", {})
        slow_hooks = []
        for script, result in hook_results.items():
            if (
                isinstance(result, dict)
                and "avg_time" in result
                and result["avg_time"] > 1.0
            ):
                slow_hooks.append((script, result["avg_time"]))

        if slow_hooks:
            recommendations.append(
                {
                    "category": "Hook System",
                    "issue": f"å‘ç°{len(slow_hooks)}ä¸ªæ…¢é€ŸHook (>1s)",
                    "recommendation": "ä¼˜åŒ–è¿™äº›Hookçš„æ‰§è¡Œé€»è¾‘ï¼Œè€ƒè™‘å¼‚æ­¥æ‰§è¡Œ",
                    "details": slow_hooks,
                    "priority": "high",
                }
            )

        # åˆ†ææ–‡ä»¶ç³»ç»Ÿ
        file_stats = self.results["test_results"].get("file_analysis", {})
        if file_stats.get("total_size_mb", 0) > 50:
            recommendations.append(
                {
                    "category": "File System",
                    "issue": f"Claudeç›®å½•å¤§å°: {file_stats['total_size_mb']}MB",
                    "recommendation": "æ¸…ç†ä¸å¿…è¦çš„æ–‡ä»¶ï¼Œå‹ç¼©å¤§æ–‡ä»¶",
                    "priority": "low",
                }
            )

        return recommendations

    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½åˆ†æ"""
        print("ğŸš€ å¼€å§‹Claude Enhanceræ€§èƒ½åˆ†æ...")
        print("=" * 60)

        # 1. æµ‹è¯•cleanupè„šæœ¬
        print("ğŸ“ æµ‹è¯•Cleanupè„šæœ¬...")
        self.results["test_results"]["cleanup_scripts"] = self.test_cleanup_scripts()

        # 2. æµ‹è¯•æ€§èƒ½è„šæœ¬
        print("âš¡ æµ‹è¯•Performanceè„šæœ¬...")
        self.results["test_results"][
            "performance_scripts"
        ] = self.test_performance_scripts()

        # 3. æµ‹è¯•Pythonè„šæœ¬
        print("ğŸ æµ‹è¯•Pythonè„šæœ¬...")
        self.results["test_results"]["python_scripts"] = self.test_python_scripts()

        # 4. æµ‹è¯•Hookç³»ç»Ÿ
        print("ğŸ”— æµ‹è¯•Hookç³»ç»Ÿ...")
        self.results["test_results"]["hook_system"] = self.test_hook_system()

        # 5. æµ‹è¯•ç³»ç»Ÿå¯åŠ¨
        print("ğŸš€ æµ‹è¯•ç³»ç»Ÿå¯åŠ¨...")
        self.results["test_results"]["system_startup"] = self.test_system_startup()

        # 6. åˆ†ææ–‡ä»¶ç³»ç»Ÿ
        print("ğŸ“Š åˆ†ææ–‡ä»¶ç³»ç»Ÿ...")
        self.results["test_results"]["file_analysis"] = self.analyze_file_sizes()

        # 7. ç”Ÿæˆå»ºè®®
        print("ğŸ’¡ ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
        self.results["recommendations"] = self.generate_recommendations()

        # 8. è®¡ç®—æ€»ç»“
        self.calculate_summary()

        print("âœ… æ€§èƒ½åˆ†æå®Œæˆ!")
        return self.results

    def calculate_summary(self):
        """è®¡ç®—æ€»ç»“ç»Ÿè®¡"""
        all_times = []
        script_count = 0
        success_count = 0

        for category, tests in self.results["test_results"].items():
            if category == "file_analysis":
                continue

            if isinstance(tests, dict):
                for test_name, result in tests.items():
                    if isinstance(result, dict) and "avg_time" in result:
                        all_times.append(result["avg_time"])
                        script_count += 1
                        if result.get("success_rate", 0) > 0.5:
                            success_count += 1

        if all_times:
            self.results["summary"] = {
                "total_scripts_tested": script_count,
                "successful_scripts": success_count,
                "success_rate": round(success_count / script_count, 2),
                "avg_execution_time": round(statistics.mean(all_times), 4),
                "fastest_script": round(min(all_times), 4),
                "slowest_script": round(max(all_times), 4),
                "total_test_duration": round(sum(all_times), 2),
            }

    def save_results(self, filename="claude_enhancer_performance_report.json"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        report_path = self.claude_dir.parent / filename
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        return report_path


def main():
    analyzer = PerformanceAnalyzer()
    results = analyzer.run_full_analysis()
    report_path = analyzer.save_results()

    # æ‰“å°ç®€è¦æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æ€§èƒ½åˆ†ææ€»ç»“")
    print("=" * 60)

    summary = results.get("summary", {})
    if summary:
        print(f"æµ‹è¯•è„šæœ¬æ€»æ•°: {summary.get('total_scripts_tested', 0)}")
        print(f"æˆåŠŸç‡: {summary.get('success_rate', 0) * 100:.1f}%")
        print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {summary.get('avg_execution_time', 0):.4f}s")
        print(f"æœ€å¿«è„šæœ¬: {summary.get('fastest_script', 0):.4f}s")
        print(f"æœ€æ…¢è„šæœ¬: {summary.get('slowest_script', 0):.4f}s")

    print(f"\nğŸ’¡ å‘ç° {len(results.get('recommendations', []))} ä¸ªä¼˜åŒ–å»ºè®®")

    return report_path


if __name__ == "__main__":
    main()
