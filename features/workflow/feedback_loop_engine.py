#!/usr/bin/env python3
"""
Perfect21 åé¦ˆå¾ªç¯å¼•æ“
===================

è§£å†³å·¥ä½œæµæ‰§è¡Œä¸­çš„å…³é”®é—®é¢˜ï¼š
1. æµ‹è¯•å¤±è´¥æ—¶ä¸åº”ç»§ç»­æäº¤ï¼Œè€Œæ˜¯è¿”å›ä¿®å¤
2. åŒä¸€ä¸ªagentè´Ÿè´£ä¿®å¤è‡ªå·±ç¼–å†™çš„ä»£ç 
3. æ™ºèƒ½é‡è¯•æœºåˆ¶å’ŒçŠ¶æ€ç®¡ç†
4. ä¸ç°æœ‰è´¨é‡é—¨å’ŒåŒæ­¥ç‚¹ç³»ç»Ÿé›†æˆ
"""

import json
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path

logger = logging.getLogger("FeedbackLoopEngine")


class FeedbackAction(Enum):
    """åé¦ˆåŠ¨ä½œç±»å‹"""
    RETRY = "retry"
    ESCALATE = "escalate"
    ABORT = "abort"
    CONTINUE = "continue"
    ROLLBACK = "rollback"


class ValidationStage(Enum):
    """éªŒè¯é˜¶æ®µ"""
    IMPLEMENTATION = "implementation"
    TESTING = "testing"
    INTEGRATION = "integration"
    DEPLOYMENT = "deployment"
    QUALITY_GATE = "quality_gate"


class FeedbackSeverity(Enum):
    """åé¦ˆä¸¥é‡æ€§"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class FeedbackContext:
    """åé¦ˆä¸Šä¸‹æ–‡"""
    workflow_id: str
    stage: ValidationStage
    agent_name: str
    task_id: str
    original_prompt: str
    validation_result: Dict[str, Any]
    failure_reason: str
    retry_count: int = 0
    max_retries: int = 3
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class RetryStrategy:
    """é‡è¯•ç­–ç•¥"""
    max_attempts: int = 3
    backoff_factor: float = 1.5
    timeout_multiplier: float = 1.2
    escalation_threshold: int = 2
    abort_conditions: List[str] = field(default_factory=list)
    custom_fixes: Dict[str, str] = field(default_factory=dict)


@dataclass
class FeedbackDecision:
    """åé¦ˆå†³ç­–"""
    action: FeedbackAction
    target_agent: str
    enhanced_prompt: str
    retry_strategy: RetryStrategy
    validation_requirements: Dict[str, Any]
    success_criteria: Dict[str, Any]
    reasoning: str
    confidence: float
    estimated_fix_time: int  # ä¼°è®¡ä¿®å¤æ—¶é—´ï¼ˆç§’ï¼‰


class FeedbackLoopEngine:
    """åé¦ˆå¾ªç¯å¼•æ“"""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.feedback_history: Dict[str, List[FeedbackContext]] = {}
        self.active_feedback_loops: Dict[str, FeedbackContext] = {}
        self.retry_strategies: Dict[str, RetryStrategy] = {}

        # çŠ¶æ€æŒä¹…åŒ–æ–‡ä»¶
        self.state_file = self.project_root / ".perfect21" / "feedback_state.json"
        self.state_file.parent.mkdir(exist_ok=True)

        # åŠ è½½å†å²çŠ¶æ€
        self._load_state()

        # åˆå§‹åŒ–é»˜è®¤é‡è¯•ç­–ç•¥
        self._init_default_strategies()

    def _init_default_strategies(self):
        """åˆå§‹åŒ–é»˜è®¤é‡è¯•ç­–ç•¥"""
        # ä»£ç å®ç°é˜¶æ®µçš„é‡è¯•ç­–ç•¥
        self.retry_strategies["implementation"] = RetryStrategy(
            max_attempts=3,
            backoff_factor=1.0,  # ç«‹å³é‡è¯•
            timeout_multiplier=1.5,
            escalation_threshold=2,
            abort_conditions=["syntax_error_repeated", "invalid_imports"],
            custom_fixes={
                "import_error": "è¯·æ£€æŸ¥å¯¼å…¥è·¯å¾„å’Œæ¨¡å—æ˜¯å¦å­˜åœ¨",
                "syntax_error": "è¯·ä»”ç»†æ£€æŸ¥ä»£ç è¯­æ³•ï¼Œç‰¹åˆ«æ˜¯æ‹¬å·ã€å¼•å·åŒ¹é…",
                "type_error": "è¯·æ£€æŸ¥å˜é‡ç±»å‹å’Œå‡½æ•°ç­¾å"
            }
        )

        # æµ‹è¯•é˜¶æ®µçš„é‡è¯•ç­–ç•¥
        self.retry_strategies["testing"] = RetryStrategy(
            max_attempts=4,  # æµ‹è¯•å…è®¸æ›´å¤šé‡è¯•
            backoff_factor=1.2,
            timeout_multiplier=1.3,
            escalation_threshold=3,
            abort_conditions=["test_framework_error", "dependency_missing"],
            custom_fixes={
                "assertion_error": "è¯·æ ¹æ®æµ‹è¯•å¤±è´¥ä¿¡æ¯ä¿®æ­£å®ç°é€»è¾‘",
                "test_timeout": "è¯·ä¼˜åŒ–ä»£ç æ€§èƒ½æˆ–è°ƒæ•´æµ‹è¯•è¶…æ—¶æ—¶é—´",
                "missing_test_case": "è¯·è¡¥å……ç¼ºå¤±çš„æµ‹è¯•ç”¨ä¾‹"
            }
        )

        # è´¨é‡é—¨é˜¶æ®µçš„é‡è¯•ç­–ç•¥
        self.retry_strategies["quality_gate"] = RetryStrategy(
            max_attempts=2,  # è´¨é‡é—¨é‡è¯•æ¬¡æ•°è¾ƒå°‘
            backoff_factor=2.0,
            timeout_multiplier=1.1,
            escalation_threshold=1,
            abort_conditions=["security_vulnerability", "performance_regression"],
            custom_fixes={
                "code_quality": "è¯·æŒ‰ç…§ä»£ç è§„èŒƒä¿®æ­£è´¨é‡é—®é¢˜",
                "coverage_low": "è¯·å¢åŠ æµ‹è¯•è¦†ç›–ç‡åˆ°è¦æ±‚çš„é˜ˆå€¼",
                "security_issue": "è¯·ä¿®å¤å®‰å…¨æ¼æ´ï¼Œè¿™æ˜¯å¼ºåˆ¶è¦æ±‚"
            }
        )

    def analyze_failure(self, context: FeedbackContext) -> FeedbackDecision:
        """
        åˆ†æå¤±è´¥åŸå› å¹¶å†³å®šåé¦ˆç­–ç•¥

        è¿™æ˜¯åé¦ˆå¾ªç¯çš„æ ¸å¿ƒå†³ç­–ç‚¹
        """
        logger.info(f"åˆ†æå¤±è´¥: {context.task_id} - {context.failure_reason}")

        # è·å–é€‚ç”¨çš„é‡è¯•ç­–ç•¥
        strategy = self.retry_strategies.get(
            context.stage.value,
            self.retry_strategies["implementation"]
        )

        # åˆ†æå¤±è´¥ä¸¥é‡æ€§
        severity = self._assess_failure_severity(context)

        # åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸­æ­¢
        if self._should_abort(context, strategy):
            return FeedbackDecision(
                action=FeedbackAction.ABORT,
                target_agent=context.agent_name,
                enhanced_prompt="",
                retry_strategy=strategy,
                validation_requirements={},
                success_criteria={},
                reasoning=f"è¾¾åˆ°ä¸­æ­¢æ¡ä»¶: {context.failure_reason}",
                confidence=0.9,
                estimated_fix_time=0
            )

        # åˆ¤æ–­æ˜¯å¦éœ€è¦å‡çº§å¤„ç†
        if context.retry_count >= strategy.escalation_threshold:
            return self._create_escalation_decision(context, strategy, severity)

        # åˆ›å»ºé‡è¯•å†³ç­–
        return self._create_retry_decision(context, strategy, severity)

    def _assess_failure_severity(self, context: FeedbackContext) -> FeedbackSeverity:
        """è¯„ä¼°å¤±è´¥ä¸¥é‡æ€§"""
        failure_reason = context.failure_reason.lower()
        validation_result = context.validation_result

        # æ£€æŸ¥å…³é”®è¯ç¡®å®šä¸¥é‡æ€§
        critical_keywords = ["security", "vulnerability", "data_loss", "corruption"]
        high_keywords = ["crash", "exception", "error", "failure", "timeout"]
        medium_keywords = ["warning", "deprecated", "slow", "performance"]

        if any(keyword in failure_reason for keyword in critical_keywords):
            return FeedbackSeverity.CRITICAL
        elif any(keyword in failure_reason for keyword in high_keywords):
            return FeedbackSeverity.HIGH
        elif any(keyword in failure_reason for keyword in medium_keywords):
            return FeedbackSeverity.MEDIUM
        else:
            return FeedbackSeverity.LOW

    def _should_abort(self, context: FeedbackContext, strategy: RetryStrategy) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸­æ­¢é‡è¯•"""
        # è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°
        if context.retry_count >= strategy.max_attempts:
            return True

        # å‘½ä¸­ä¸­æ­¢æ¡ä»¶
        for condition in strategy.abort_conditions:
            if condition in context.failure_reason.lower():
                return True

        # æ—¶é—´çª—å£æ£€æŸ¥ï¼ˆè¶…è¿‡1å°æ—¶çš„åå¤å¤±è´¥ï¼‰
        elapsed = datetime.now() - context.created_at
        if elapsed > timedelta(hours=1):
            return True

        return False

    def _create_retry_decision(self, context: FeedbackContext,
                             strategy: RetryStrategy,
                             severity: FeedbackSeverity) -> FeedbackDecision:
        """åˆ›å»ºé‡è¯•å†³ç­–"""

        # å¢å¼ºæç¤ºè¯ï¼ŒåŒ…å«å¤±è´¥ä¿¡æ¯å’Œä¿®å¤æŒ‡å¯¼
        enhanced_prompt = self._create_enhanced_prompt(context, strategy)

        # å®šä¹‰æˆåŠŸæ ‡å‡†
        success_criteria = self._define_success_criteria(context)

        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºé‡è¯•æ¬¡æ•°é€’å‡ï¼‰
        confidence = max(0.3, 0.9 - (context.retry_count * 0.2))

        # ä¼°è®¡ä¿®å¤æ—¶é—´
        base_time = 300  # 5åˆ†é’ŸåŸºç¡€æ—¶é—´
        time_multiplier = 1 + (context.retry_count * 0.5)
        estimated_time = int(base_time * time_multiplier)

        return FeedbackDecision(
            action=FeedbackAction.RETRY,
            target_agent=context.agent_name,  # åŒä¸€ä¸ªagentä¿®å¤
            enhanced_prompt=enhanced_prompt,
            retry_strategy=strategy,
            validation_requirements=self._create_validation_requirements(context),
            success_criteria=success_criteria,
            reasoning=f"ç¬¬{context.retry_count + 1}æ¬¡é‡è¯•ï¼Œå¤±è´¥åŸå› : {context.failure_reason}",
            confidence=confidence,
            estimated_fix_time=estimated_time
        )

    def _create_escalation_decision(self, context: FeedbackContext,
                                  strategy: RetryStrategy,
                                  severity: FeedbackSeverity) -> FeedbackDecision:
        """åˆ›å»ºå‡çº§å†³ç­–"""

        # é€‰æ‹©å‡çº§ç›®æ ‡agent
        escalation_agent = self._select_escalation_agent(context)

        # åˆ›å»ºå‡çº§æç¤ºè¯
        escalation_prompt = self._create_escalation_prompt(context, escalation_agent)

        return FeedbackDecision(
            action=FeedbackAction.ESCALATE,
            target_agent=escalation_agent,
            enhanced_prompt=escalation_prompt,
            retry_strategy=strategy,
            validation_requirements=self._create_validation_requirements(context),
            success_criteria=self._define_success_criteria(context),
            reasoning=f"å‡çº§å¤„ç†ï¼ŒåŸagent: {context.agent_name}, å¤±è´¥{context.retry_count}æ¬¡",
            confidence=0.7,
            estimated_fix_time=600  # å‡çº§éœ€è¦æ›´å¤šæ—¶é—´
        )

    def _create_enhanced_prompt(self, context: FeedbackContext,
                              strategy: RetryStrategy) -> str:
        """åˆ›å»ºå¢å¼ºçš„æç¤ºè¯"""

        # åŸºç¡€æç¤ºè¯
        base_prompt = context.original_prompt

        # å¤±è´¥åˆ†æ
        failure_analysis = f"""
## ğŸ”´ å‰æ¬¡æ‰§è¡Œå¤±è´¥åˆ†æ

**å¤±è´¥åŸå› **: {context.failure_reason}
**é‡è¯•æ¬¡æ•°**: {context.retry_count + 1}/{strategy.max_attempts}
**éªŒè¯ç»“æœ**: {json.dumps(context.validation_result, indent=2, ensure_ascii=False)}

## ğŸ”§ ä¿®å¤æŒ‡å¯¼

**é˜¶æ®µ**: {context.stage.value}
**é‡ç‚¹å…³æ³¨**:
"""

        # æ·»åŠ é’ˆå¯¹æ€§ä¿®å¤å»ºè®®
        stage_guidance = {
            ValidationStage.IMPLEMENTATION: [
                "è¯·ä»”ç»†æ£€æŸ¥ä»£ç è¯­æ³•å’Œé€»è¾‘é”™è¯¯",
                "ç¡®ä¿æ‰€æœ‰å¯¼å…¥çš„æ¨¡å—å’Œä¾èµ–éƒ½æ­£ç¡®",
                "éªŒè¯å‡½æ•°ç­¾åå’Œè¿”å›å€¼ç±»å‹",
                "æ³¨æ„å˜é‡ä½œç”¨åŸŸå’Œå‘½åè§„èŒƒ"
            ],
            ValidationStage.TESTING: [
                "åˆ†ææµ‹è¯•å¤±è´¥çš„å…·ä½“åŸå› ",
                "æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹çš„æœŸæœ›å€¼æ˜¯å¦æ­£ç¡®",
                "ç¡®ä¿æµ‹è¯•ç¯å¢ƒå’Œæ•°æ®å‡†å¤‡å……åˆ†",
                "éªŒè¯æµ‹è¯•è¦†ç›–äº†æ‰€æœ‰å…³é”®è·¯å¾„"
            ],
            ValidationStage.QUALITY_GATE: [
                "æŒ‰ç…§è´¨é‡æ ‡å‡†ä¿®æ­£ä»£ç è§„èŒƒé—®é¢˜",
                "ä¼˜åŒ–æ€§èƒ½ç“¶é¢ˆå’Œèµ„æºä½¿ç”¨",
                "ä¿®å¤å®‰å…¨æ¼æ´å’Œé£é™©ç‚¹",
                "ç¡®ä¿æ–‡æ¡£å’Œæ³¨é‡Šå®Œæ•´"
            ]
        }

        guidance_list = stage_guidance.get(context.stage, stage_guidance[ValidationStage.IMPLEMENTATION])
        for guidance in guidance_list:
            failure_analysis += f"\n- {guidance}"

        # æ·»åŠ è‡ªå®šä¹‰ä¿®å¤å»ºè®®
        if strategy.custom_fixes:
            failure_analysis += "\n\n**å…·ä½“ä¿®å¤å»ºè®®**:\n"
            for error_type, fix_suggestion in strategy.custom_fixes.items():
                if error_type.lower() in context.failure_reason.lower():
                    failure_analysis += f"- {fix_suggestion}\n"

        # æ·»åŠ éªŒè¯è¦æ±‚
        failure_analysis += f"""

## âœ… éªŒè¯è¦æ±‚

è¯·ç¡®ä¿ä¿®å¤åçš„ä»£ç èƒ½å¤Ÿé€šè¿‡ä»¥ä¸‹éªŒè¯:
1. **åŸºæœ¬åŠŸèƒ½**: æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸å·¥ä½œ
2. **é”™è¯¯å¤„ç†**: å¦¥å–„å¤„ç†å¼‚å¸¸æƒ…å†µ
3. **æ€§èƒ½è¦æ±‚**: æ»¡è¶³æ€§èƒ½æŒ‡æ ‡
4. **å®‰å…¨æ ‡å‡†**: ç¬¦åˆå®‰å…¨è§„èŒƒ
5. **æµ‹è¯•è¦†ç›–**: æœ‰å……åˆ†çš„æµ‹è¯•éªŒè¯

## ğŸ¯ æˆåŠŸæ ‡å‡†

{self._format_success_criteria(context)}

---

## ğŸ“ åŸå§‹ä»»åŠ¡

{base_prompt}
"""

        return failure_analysis

    def _create_escalation_prompt(self, context: FeedbackContext,
                                escalation_agent: str) -> str:
        """åˆ›å»ºå‡çº§æç¤ºè¯"""

        escalation_prompt = f"""
## ğŸš¨ ä»»åŠ¡å‡çº§å¤„ç†

**åŸå§‹è´Ÿè´£Agent**: {context.agent_name}
**å‡çº§åŸå› **: ç»è¿‡{context.retry_count}æ¬¡é‡è¯•ä»æœªè§£å†³
**å½“å‰å¤„ç†Agent**: {escalation_agent}

## ğŸ” é—®é¢˜è¯¦æƒ…

**å¤±è´¥é˜¶æ®µ**: {context.stage.value}
**æ ¸å¿ƒé—®é¢˜**: {context.failure_reason}
**éªŒè¯ç»“æœ**:
{json.dumps(context.validation_result, indent=2, ensure_ascii=False)}

## ğŸ¯ å‡çº§å¤„ç†è¦æ±‚

ä½œä¸ºå‡çº§å¤„ç†çš„ä¸“å®¶ï¼Œè¯·ä½ :

1. **æ·±åº¦åˆ†æ**: ä»ä¸åŒè§’åº¦åˆ†æé—®é¢˜æ ¹å› 
2. **æ¶æ„å®¡æŸ¥**: æ£€æŸ¥æ˜¯å¦æœ‰æ¶æ„æˆ–è®¾è®¡é—®é¢˜
3. **å…¨é¢ä¿®å¤**: ä¸ä»…ä¿®å¤è¡¨é¢é—®é¢˜ï¼Œè¿˜è¦ç¡®ä¿å¥å£®æ€§
4. **çŸ¥è¯†ä¼ é€’**: åœ¨ä»£ç ä¸­æ·»åŠ æ³¨é‡Šè¯´æ˜ä¿®å¤æ€è·¯

## ğŸ“‹ åŸå§‹ä»»åŠ¡

{context.original_prompt}

## ğŸ”§ ä¸“å®¶çº§ä¿®å¤æŒ‡å¯¼

è¯·ä»¥{escalation_agent}çš„ä¸“ä¸šè§†è§’ï¼Œæä¾›é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆã€‚
é‡ç‚¹å…³æ³¨ä»£ç è´¨é‡ã€å¯ç»´æŠ¤æ€§å’Œå¥å£®æ€§ã€‚
"""

        return escalation_prompt

    def _select_escalation_agent(self, context: FeedbackContext) -> str:
        """é€‰æ‹©å‡çº§å¤„ç†çš„agent"""

        # åŸºäºå½“å‰é˜¶æ®µå’Œå¤±è´¥ç±»å‹é€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶
        escalation_map = {
            ValidationStage.IMPLEMENTATION: {
                "syntax_error": "python-pro",
                "import_error": "backend-architect",
                "logic_error": "fullstack-engineer",
                "type_error": "typescript-pro",
                "default": "code-reviewer"
            },
            ValidationStage.TESTING: {
                "test_failure": "test-engineer",
                "coverage": "e2e-test-specialist",
                "performance": "performance-tester",
                "default": "test-engineer"
            },
            ValidationStage.QUALITY_GATE: {
                "security": "security-auditor",
                "performance": "performance-engineer",
                "architecture": "backend-architect",
                "default": "code-reviewer"
            }
        }

        stage_map = escalation_map.get(context.stage, escalation_map[ValidationStage.IMPLEMENTATION])

        # æŸ¥æ‰¾åŒ¹é…çš„é”™è¯¯ç±»å‹
        for error_type, agent in stage_map.items():
            if error_type != "default" and error_type in context.failure_reason.lower():
                return agent

        # è¿”å›é»˜è®¤agent
        return stage_map["default"]

    def _define_success_criteria(self, context: FeedbackContext) -> Dict[str, Any]:
        """å®šä¹‰æˆåŠŸæ ‡å‡†"""

        base_criteria = {
            "execution_success": True,
            "no_critical_errors": True,
            "validation_passed": True
        }

        # æ ¹æ®é˜¶æ®µæ·»åŠ ç‰¹å®šæ ‡å‡†
        stage_criteria = {
            ValidationStage.IMPLEMENTATION: {
                "syntax_valid": True,
                "imports_resolved": True,
                "no_runtime_errors": True
            },
            ValidationStage.TESTING: {
                "all_tests_pass": True,
                "coverage_threshold": ">= 80%",
                "no_test_timeouts": True
            },
            ValidationStage.QUALITY_GATE: {
                "quality_score": ">= 8.0",
                "security_issues": "== 0",
                "performance_regression": False
            }
        }

        specific_criteria = stage_criteria.get(context.stage, {})
        base_criteria.update(specific_criteria)

        return base_criteria

    def _create_validation_requirements(self, context: FeedbackContext) -> Dict[str, Any]:
        """åˆ›å»ºéªŒè¯è¦æ±‚"""

        return {
            "stage": context.stage.value,
            "retry_count": context.retry_count + 1,
            "previous_failures": [context.failure_reason],
            "success_criteria": self._define_success_criteria(context),
            "timeout": 300 * (1.2 ** context.retry_count),  # é€’å¢è¶…æ—¶
            "validation_type": "enhanced",
            "failure_sensitive": True
        }

    def _format_success_criteria(self, context: FeedbackContext) -> str:
        """æ ¼å¼åŒ–æˆåŠŸæ ‡å‡†æ˜¾ç¤º"""
        criteria = self._define_success_criteria(context)
        formatted = []

        for criterion, requirement in criteria.items():
            formatted.append(f"- {criterion}: {requirement}")

        return "\n".join(formatted)

    def register_feedback_loop(self, workflow_id: str, stage: ValidationStage,
                             agent_name: str, task_id: str,
                             original_prompt: str) -> str:
        """æ³¨å†Œåé¦ˆå¾ªç¯"""

        feedback_id = f"{workflow_id}_{stage.value}_{task_id}"

        context = FeedbackContext(
            workflow_id=workflow_id,
            stage=stage,
            agent_name=agent_name,
            task_id=task_id,
            original_prompt=original_prompt,
            validation_result={},
            failure_reason=""
        )

        self.active_feedback_loops[feedback_id] = context

        # åˆå§‹åŒ–å†å²è®°å½•
        if workflow_id not in self.feedback_history:
            self.feedback_history[workflow_id] = []

        logger.info(f"æ³¨å†Œåé¦ˆå¾ªç¯: {feedback_id}")
        return feedback_id

    def process_validation_failure(self, feedback_id: str,
                                 validation_result: Dict[str, Any],
                                 failure_reason: str) -> FeedbackDecision:
        """å¤„ç†éªŒè¯å¤±è´¥"""

        if feedback_id not in self.active_feedback_loops:
            raise ValueError(f"åé¦ˆå¾ªç¯ä¸å­˜åœ¨: {feedback_id}")

        context = self.active_feedback_loops[feedback_id]
        context.validation_result = validation_result
        context.failure_reason = failure_reason
        context.retry_count += 1
        context.updated_at = datetime.now()

        # æ·»åŠ åˆ°å†å²è®°å½•
        self.feedback_history[context.workflow_id].append(context)

        # åˆ†æå¹¶ç”Ÿæˆå†³ç­–
        decision = self.analyze_failure(context)

        # ä¿å­˜çŠ¶æ€
        self._save_state()

        logger.info(f"å¤„ç†éªŒè¯å¤±è´¥: {feedback_id} -> {decision.action.value}")
        return decision

    def process_validation_success(self, feedback_id: str,
                                 validation_result: Dict[str, Any]) -> bool:
        """å¤„ç†éªŒè¯æˆåŠŸ"""

        if feedback_id not in self.active_feedback_loops:
            return False

        context = self.active_feedback_loops[feedback_id]
        context.validation_result = validation_result
        context.updated_at = datetime.now()

        # ç§»é™¤æ´»è·ƒå¾ªç¯
        del self.active_feedback_loops[feedback_id]

        # ä¿å­˜åˆ°å†å²
        self.feedback_history[context.workflow_id].append(context)

        logger.info(f"éªŒè¯æˆåŠŸï¼Œå…³é—­åé¦ˆå¾ªç¯: {feedback_id}")
        self._save_state()
        return True

    def get_retry_instruction(self, decision: FeedbackDecision) -> str:
        """è·å–é‡è¯•æŒ‡ä»¤"""

        if decision.action != FeedbackAction.RETRY:
            return ""

        instruction = f"""
## ğŸ”„ Perfect21 åé¦ˆå¾ªç¯é‡è¯•æŒ‡ä»¤

**ç›®æ ‡Agent**: {decision.target_agent}
**ä¿®å¤ä»»åŠ¡**: åŸºäºéªŒè¯å¤±è´¥è¿›è¡Œä»£ç ä¿®å¤
**ç½®ä¿¡åº¦**: {decision.confidence:.2f}
**é¢„ä¼°æ—¶é—´**: {decision.estimated_fix_time}ç§’

### æ‰§è¡ŒæŒ‡ä»¤:
```xml
<function_calls>
  <invoke name="Task">
    <parameter name="subagent_type">{decision.target_agent}</parameter>
    <parameter name="prompt">{decision.enhanced_prompt}</parameter>
  </invoke>
</function_calls>
```

### éªŒè¯è¦æ±‚:
{json.dumps(decision.validation_requirements, indent=2, ensure_ascii=False)}

### æˆåŠŸæ ‡å‡†:
{json.dumps(decision.success_criteria, indent=2, ensure_ascii=False)}

âš ï¸ **é‡è¦**: æ‰§è¡Œå®Œæˆåå¿…é¡»é‡æ–°è¿è¡Œç›¸åŒçš„éªŒè¯æµç¨‹
"""

        return instruction

    def get_escalation_instruction(self, decision: FeedbackDecision) -> str:
        """è·å–å‡çº§æŒ‡ä»¤"""

        if decision.action != FeedbackAction.ESCALATE:
            return ""

        instruction = f"""
## ğŸš¨ Perfect21 åé¦ˆå¾ªç¯å‡çº§æŒ‡ä»¤

**å‡çº§åˆ°**: {decision.target_agent}
**å‡çº§åŸå› **: {decision.reasoning}
**ç½®ä¿¡åº¦**: {decision.confidence:.2f}
**é¢„ä¼°æ—¶é—´**: {decision.estimated_fix_time}ç§’

### æ‰§è¡ŒæŒ‡ä»¤:
```xml
<function_calls>
  <invoke name="Task">
    <parameter name="subagent_type">{decision.target_agent}</parameter>
    <parameter name="prompt">{decision.enhanced_prompt}</parameter>
  </invoke>
</function_calls>
```

### ä¸“å®¶çº§è¦æ±‚:
- æ·±åº¦åˆ†æé—®é¢˜æ ¹å› 
- æä¾›æ¶æ„çº§è§£å†³æ–¹æ¡ˆ
- ç¡®ä¿ä»£ç å¥å£®æ€§å’Œå¯ç»´æŠ¤æ€§
- æ·»åŠ è¯¦ç»†çš„ä¿®å¤è¯´æ˜

âš ï¸ **é‡è¦**: å‡çº§å¤„ç†å®Œæˆåå¿…é¡»è¿›è¡Œå…¨é¢éªŒè¯
"""

        return instruction

    def get_workflow_feedback_status(self, workflow_id: str) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµåé¦ˆçŠ¶æ€"""

        active_loops = {fid: ctx for fid, ctx in self.active_feedback_loops.items()
                       if ctx.workflow_id == workflow_id}

        history = self.feedback_history.get(workflow_id, [])

        # ç»Ÿè®¡ä¿¡æ¯
        total_failures = len(history)
        total_retries = sum(ctx.retry_count for ctx in history)
        success_rate = 0

        if total_failures > 0:
            successful_fixes = len([ctx for ctx in history if ctx.validation_result.get('success', False)])
            success_rate = successful_fixes / total_failures

        return {
            "workflow_id": workflow_id,
            "active_feedback_loops": len(active_loops),
            "total_failures": total_failures,
            "total_retries": total_retries,
            "success_rate": success_rate,
            "active_loops": {fid: {
                "stage": ctx.stage.value,
                "agent": ctx.agent_name,
                "retry_count": ctx.retry_count,
                "failure_reason": ctx.failure_reason
            } for fid, ctx in active_loops.items()},
            "recent_history": [
                {
                    "stage": ctx.stage.value,
                    "agent": ctx.agent_name,
                    "retry_count": ctx.retry_count,
                    "failure_reason": ctx.failure_reason,
                    "timestamp": ctx.updated_at.isoformat()
                }
                for ctx in history[-5:]  # æœ€è¿‘5æ¡
            ]
        }

    def cleanup_expired_loops(self, max_age_hours: int = 24):
        """æ¸…ç†è¿‡æœŸçš„åé¦ˆå¾ªç¯"""

        cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
        expired_ids = []

        for feedback_id, context in self.active_feedback_loops.items():
            if context.created_at < cutoff_time:
                expired_ids.append(feedback_id)

        for feedback_id in expired_ids:
            logger.warning(f"æ¸…ç†è¿‡æœŸåé¦ˆå¾ªç¯: {feedback_id}")
            del self.active_feedback_loops[feedback_id]

        if expired_ids:
            self._save_state()

        return len(expired_ids)

    def _save_state(self):
        """ä¿å­˜çŠ¶æ€åˆ°æ–‡ä»¶"""
        try:
            state = {
                "active_feedback_loops": {
                    fid: {
                        "workflow_id": ctx.workflow_id,
                        "stage": ctx.stage.value,
                        "agent_name": ctx.agent_name,
                        "task_id": ctx.task_id,
                        "original_prompt": ctx.original_prompt,
                        "validation_result": ctx.validation_result,
                        "failure_reason": ctx.failure_reason,
                        "retry_count": ctx.retry_count,
                        "max_retries": ctx.max_retries,
                        "created_at": ctx.created_at.isoformat(),
                        "updated_at": ctx.updated_at.isoformat(),
                        "metadata": ctx.metadata
                    }
                    for fid, ctx in self.active_feedback_loops.items()
                },
                "feedback_history": {
                    wid: [
                        {
                            "workflow_id": ctx.workflow_id,
                            "stage": ctx.stage.value,
                            "agent_name": ctx.agent_name,
                            "task_id": ctx.task_id,
                            "original_prompt": ctx.original_prompt,
                            "validation_result": ctx.validation_result,
                            "failure_reason": ctx.failure_reason,
                            "retry_count": ctx.retry_count,
                            "max_retries": ctx.max_retries,
                            "created_at": ctx.created_at.isoformat(),
                            "updated_at": ctx.updated_at.isoformat(),
                            "metadata": ctx.metadata
                        }
                        for ctx in history
                    ]
                    for wid, history in self.feedback_history.items()
                }
            }

            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logger.error(f"ä¿å­˜åé¦ˆå¾ªç¯çŠ¶æ€å¤±è´¥: {e}")

    def _load_state(self):
        """ä»æ–‡ä»¶åŠ è½½çŠ¶æ€"""
        try:
            if not self.state_file.exists():
                return

            with open(self.state_file, 'r', encoding='utf-8') as f:
                state = json.load(f)

            # æ¢å¤æ´»è·ƒå¾ªç¯
            for fid, ctx_data in state.get("active_feedback_loops", {}).items():
                context = FeedbackContext(
                    workflow_id=ctx_data["workflow_id"],
                    stage=ValidationStage(ctx_data["stage"]),
                    agent_name=ctx_data["agent_name"],
                    task_id=ctx_data["task_id"],
                    original_prompt=ctx_data["original_prompt"],
                    validation_result=ctx_data["validation_result"],
                    failure_reason=ctx_data["failure_reason"],
                    retry_count=ctx_data["retry_count"],
                    max_retries=ctx_data["max_retries"],
                    created_at=datetime.fromisoformat(ctx_data["created_at"]),
                    updated_at=datetime.fromisoformat(ctx_data["updated_at"]),
                    metadata=ctx_data["metadata"]
                )
                self.active_feedback_loops[fid] = context

            # æ¢å¤å†å²è®°å½•
            for wid, history_data in state.get("feedback_history", {}).items():
                history = []
                for ctx_data in history_data:
                    context = FeedbackContext(
                        workflow_id=ctx_data["workflow_id"],
                        stage=ValidationStage(ctx_data["stage"]),
                        agent_name=ctx_data["agent_name"],
                        task_id=ctx_data["task_id"],
                        original_prompt=ctx_data["original_prompt"],
                        validation_result=ctx_data["validation_result"],
                        failure_reason=ctx_data["failure_reason"],
                        retry_count=ctx_data["retry_count"],
                        max_retries=ctx_data["max_retries"],
                        created_at=datetime.fromisoformat(ctx_data["created_at"]),
                        updated_at=datetime.fromisoformat(ctx_data["updated_at"]),
                        metadata=ctx_data["metadata"]
                    )
                    history.append(context)
                self.feedback_history[wid] = history

        except Exception as e:
            logger.error(f"åŠ è½½åé¦ˆå¾ªç¯çŠ¶æ€å¤±è´¥: {e}")


# å…¨å±€å®ä¾‹
_feedback_engine = None

def get_feedback_engine(project_root: str = "/home/xx/dev/Perfect21") -> FeedbackLoopEngine:
    """è·å–å…¨å±€åé¦ˆå¾ªç¯å¼•æ“å®ä¾‹"""
    global _feedback_engine
    if _feedback_engine is None:
        _feedback_engine = FeedbackLoopEngine(project_root)
    return _feedback_engine