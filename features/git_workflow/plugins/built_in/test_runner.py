#!/usr/bin/env python3
"""
Perfect21 Git Hooks - Test Runner Plugin
æµ‹è¯•è¿è¡Œæ’ä»¶ï¼Œæ”¯æŒå¤šç§æµ‹è¯•æ¡†æ¶
"""

import os
import json
import subprocess
from typing import Dict, Any, List, Optional, Tuple

try:
    from ..base_plugin import (
        TestPlugin, PluginResult, PluginStatus, PluginMetadata, PluginPriority
    )
except ImportError:
    from features.git_workflow.plugins.base_plugin import (
        TestPlugin, PluginResult, PluginStatus, PluginMetadata, PluginPriority
    )


class TestRunnerPlugin(TestPlugin):
    """æµ‹è¯•è¿è¡Œæ’ä»¶"""

    def _get_metadata(self) -> PluginMetadata:
        return PluginMetadata(
            name="test_runner",
            version="2.1.0",
            description="å¤šæµ‹è¯•æ¡†æ¶æ”¯æŒçš„æµ‹è¯•è¿è¡Œå™¨",
            author="Perfect21 Team",
            category="testing",
            priority=PluginPriority.HIGH,
            dependencies=["python:subprocess"],
            supports_parallel=True,
            timeout=300
        )

    def execute(self, context: Dict[str, Any]) -> PluginResult:
        """æ‰§è¡Œæµ‹è¯•"""
        staged_files = self.get_staged_files()

        # æ£€æµ‹é¡¹ç›®ç±»å‹å’Œæµ‹è¯•æ¡†æ¶
        test_framework = self._detect_test_framework()

        if not test_framework:
            return PluginResult(
                status=PluginStatus.SKIPPED,
                message="æœªæ£€æµ‹åˆ°æ”¯æŒçš„æµ‹è¯•æ¡†æ¶"
            )

        # æ ¹æ®é…ç½®å†³å®šè¿è¡Œå“ªäº›æµ‹è¯•
        test_scope = self._determine_test_scope(staged_files, context)

        if not test_scope['should_run']:
            return PluginResult(
                status=PluginStatus.SKIPPED,
                message=test_scope['reason']
            )

        # è¿è¡Œæµ‹è¯•
        test_results = []

        for test_command in test_scope['commands']:
            result = self._run_test_command(test_command, test_framework)
            test_results.append(result)

        # åˆ†æç»“æœ
        overall_result = self._analyze_test_results(test_results)

        return overall_result

    def _detect_test_framework(self) -> Optional[Dict[str, Any]]:
        """æ£€æµ‹æµ‹è¯•æ¡†æ¶"""
        frameworks = [
            {
                'name': 'pytest',
                'command': 'pytest',
                'config_files': ['pytest.ini', 'pyproject.toml', 'setup.cfg'],
                'test_patterns': ['test_*.py', '*_test.py'],
                'check_command': 'python -m pytest --version'
            },
            {
                'name': 'unittest',
                'command': 'python -m unittest',
                'config_files': [],
                'test_patterns': ['test_*.py', '*_test.py'],
                'check_command': 'python -c "import unittest"'
            },
            {
                'name': 'jest',
                'command': 'npm test',
                'config_files': ['jest.config.js', 'package.json'],
                'test_patterns': ['*.test.js', '*.spec.js'],
                'check_command': 'npm list jest'
            },
            {
                'name': 'mocha',
                'command': 'npx mocha',
                'config_files': ['.mocharc.json', 'mocha.opts'],
                'test_patterns': ['*.test.js', '*.spec.js'],
                'check_command': 'npm list mocha'
            },
            {
                'name': 'cargo_test',
                'command': 'cargo test',
                'config_files': ['Cargo.toml'],
                'test_patterns': ['**/tests/*.rs'],
                'check_command': 'cargo --version'
            },
            {
                'name': 'go_test',
                'command': 'go test',
                'config_files': ['go.mod'],
                'test_patterns': ['*_test.go'],
                'check_command': 'go version'
            }
        ]

        for framework in frameworks:
            if self._check_framework_available(framework):
                return framework

        return None

    def _check_framework_available(self, framework: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æµ‹è¯•æ¡†æ¶æ˜¯å¦å¯ç”¨"""
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        for config_file in framework['config_files']:
            if os.path.exists(config_file):
                try:
                    # å°è¯•è¿è¡Œæ£€æŸ¥å‘½ä»¤
                    subprocess.run(
                        framework['check_command'].split(),
                        capture_output=True,
                        timeout=10,
                        check=True
                    )
                    return True
                except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):
                    continue

        # æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•æ–‡ä»¶
        import glob
        for pattern in framework['test_patterns']:
            if glob.glob(pattern, recursive=True):
                try:
                    subprocess.run(
                        framework['check_command'].split(),
                        capture_output=True,
                        timeout=10,
                        check=True
                    )
                    return True
                except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):
                    continue

        return False

    def _determine_test_scope(self, staged_files: List[str], context: Dict[str, Any]) -> Dict[str, Any]:
        """ç¡®å®šæµ‹è¯•èŒƒå›´"""
        test_mode = self.get_config_value('test_mode', 'auto')
        coverage_threshold = self.get_config_value('coverage_threshold', 80)
        parallel_jobs = self.get_config_value('parallel_jobs', 4)

        # æ£€æŸ¥æ˜¯å¦æœ‰ä»£ç æ›´æ”¹
        code_files = [f for f in staged_files if self._is_code_file(f)]

        if not code_files and test_mode == 'auto':
            return {
                'should_run': False,
                'reason': 'æ²¡æœ‰ä»£ç æ–‡ä»¶å˜æ›´ï¼Œè·³è¿‡æµ‹è¯•',
                'commands': []
            }

        # æ ¹æ®æ–‡ä»¶å˜æ›´ç¡®å®šæµ‹è¯•å‘½ä»¤
        commands = []

        if test_mode == 'all':
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•
            base_command = self.get_config_value('test_command', 'python -m pytest')
            commands.append(f"{base_command} -n {parallel_jobs}")

        elif test_mode == 'affected':
            # åªè¿è¡Œç›¸å…³æµ‹è¯•
            affected_tests = self._find_affected_tests(code_files)
            if affected_tests:
                base_command = self.get_config_value('test_command', 'python -m pytest')
                for test_file in affected_tests:
                    commands.append(f"{base_command} {test_file}")
            else:
                return {
                    'should_run': False,
                    'reason': 'æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æµ‹è¯•æ–‡ä»¶',
                    'commands': []
                }

        else:  # autoæ¨¡å¼
            # æ™ºèƒ½ç¡®å®šæµ‹è¯•èŒƒå›´
            if len(code_files) > 10:
                # å¤§é‡æ–‡ä»¶å˜æ›´ï¼Œè¿è¡Œæ‰€æœ‰æµ‹è¯•
                base_command = self.get_config_value('test_command', 'python -m pytest')
                commands.append(f"{base_command} -n {parallel_jobs}")
            else:
                # å°‘é‡æ–‡ä»¶å˜æ›´ï¼Œè¿è¡Œç›¸å…³æµ‹è¯•
                affected_tests = self._find_affected_tests(code_files)
                if affected_tests:
                    base_command = self.get_config_value('test_command', 'python -m pytest')
                    commands.extend(f"{base_command} {test_file}" for test_file in affected_tests)
                else:
                    # æ²¡æœ‰ç›¸å…³æµ‹è¯•ï¼Œè¿è¡Œå¿«é€Ÿæµ‹è¯•
                    base_command = self.get_config_value('test_command', 'python -m pytest')
                    commands.append(f"{base_command} -x --ff")

        return {
            'should_run': len(commands) > 0,
            'reason': f'å°†è¿è¡Œ {len(commands)} ä¸ªæµ‹è¯•å‘½ä»¤',
            'commands': commands
        }

    def _is_code_file(self, file_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä»£ç æ–‡ä»¶"""
        code_extensions = {'.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.h', '.rs', '.go'}
        return any(file_path.endswith(ext) for ext in code_extensions)

    def _find_affected_tests(self, code_files: List[str]) -> List[str]:
        """æŸ¥æ‰¾å—å½±å“çš„æµ‹è¯•æ–‡ä»¶"""
        affected_tests = []

        for code_file in code_files:
            # åŸºäºæ–‡ä»¶åæ¨æ–­æµ‹è¯•æ–‡ä»¶
            test_candidates = self._generate_test_candidates(code_file)

            for candidate in test_candidates:
                if os.path.exists(candidate) and candidate not in affected_tests:
                    affected_tests.append(candidate)

        return affected_tests

    def _generate_test_candidates(self, code_file: str) -> List[str]:
        """ç”Ÿæˆå¯èƒ½çš„æµ‹è¯•æ–‡ä»¶è·¯å¾„"""
        candidates = []

        file_dir = os.path.dirname(code_file)
        file_name = os.path.basename(code_file)
        name_without_ext = os.path.splitext(file_name)[0]

        # å¸¸è§çš„æµ‹è¯•æ–‡ä»¶å‘½åæ¨¡å¼
        patterns = [
            f"test_{file_name}",
            f"{name_without_ext}_test.py",
            f"test_{name_without_ext}.py",
        ]

        # å¸¸è§çš„æµ‹è¯•ç›®å½•
        test_dirs = [
            "tests",
            "test",
            "__tests__",
            file_dir,
            os.path.join(file_dir, "tests"),
            os.path.join(file_dir, "test"),
        ]

        for test_dir in test_dirs:
            for pattern in patterns:
                candidate = os.path.join(test_dir, pattern)
                candidates.append(candidate)

        return candidates

    def _run_test_command(self, command: str, framework: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å‘½ä»¤"""
        import time

        start_time = time.time()

        try:
            result = subprocess.run(
                command.split(),
                capture_output=True,
                text=True,
                timeout=self.metadata.timeout
            )

            execution_time = time.time() - start_time

            # è§£ææµ‹è¯•ç»“æœ
            parsed_result = self._parse_test_output(
                result.stdout, result.stderr, framework['name']
            )

            return {
                "command": command,
                "exit_code": result.returncode,
                "execution_time": execution_time,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "success": result.returncode == 0,
                "parsed": parsed_result
            }

        except subprocess.TimeoutExpired:
            return {
                "command": command,
                "exit_code": -1,
                "execution_time": time.time() - start_time,
                "stdout": "",
                "stderr": f"æµ‹è¯•è¶…æ—¶ ({self.metadata.timeout}s)",
                "success": False,
                "parsed": {"tests_run": 0, "failures": 1, "errors": 0}
            }

        except Exception as e:
            return {
                "command": command,
                "exit_code": -1,
                "execution_time": time.time() - start_time,
                "stdout": "",
                "stderr": f"æµ‹è¯•æ‰§è¡Œé”™è¯¯: {str(e)}",
                "success": False,
                "parsed": {"tests_run": 0, "failures": 1, "errors": 0}
            }

    def _parse_test_output(self, stdout: str, stderr: str, framework: str) -> Dict[str, Any]:
        """è§£ææµ‹è¯•è¾“å‡º"""
        if framework == 'pytest':
            return self._parse_pytest_output(stdout + stderr)
        elif framework == 'unittest':
            return self._parse_unittest_output(stdout + stderr)
        elif framework in ['jest', 'mocha']:
            return self._parse_js_test_output(stdout + stderr)
        else:
            return self._parse_generic_output(stdout + stderr)

    def _parse_pytest_output(self, output: str) -> Dict[str, Any]:
        """è§£æpytestè¾“å‡º"""
        result = {"tests_run": 0, "failures": 0, "errors": 0, "passed": 0, "skipped": 0}

        # æŸ¥æ‰¾æµ‹è¯•ç»“æœæ±‡æ€»è¡Œ
        import re

        # pytestæ ¼å¼: "= 5 failed, 10 passed, 2 skipped in 1.23s ="
        summary_pattern = r'(\d+)\s+(failed|passed|skipped|error)'
        matches = re.findall(summary_pattern, output, re.IGNORECASE)

        for count, status in matches:
            count = int(count)
            status = status.lower()

            if status == 'passed':
                result['passed'] = count
            elif status == 'failed':
                result['failures'] = count
            elif status == 'error':
                result['errors'] = count
            elif status == 'skipped':
                result['skipped'] = count

        result['tests_run'] = result['passed'] + result['failures'] + result['errors'] + result['skipped']

        return result

    def _parse_unittest_output(self, output: str) -> Dict[str, Any]:
        """è§£æunittestè¾“å‡º"""
        result = {"tests_run": 0, "failures": 0, "errors": 0}

        import re

        # unittestæ ¼å¼: "Ran 10 tests in 1.234s"
        run_match = re.search(r'Ran (\d+) tests? in', output)
        if run_match:
            result['tests_run'] = int(run_match.group(1))

        # æŸ¥æ‰¾å¤±è´¥å’Œé”™è¯¯
        if 'FAILED (failures=' in output:
            failures_match = re.search(r'failures=(\d+)', output)
            if failures_match:
                result['failures'] = int(failures_match.group(1))

        if 'errors=' in output:
            errors_match = re.search(r'errors=(\d+)', output)
            if errors_match:
                result['errors'] = int(errors_match.group(1))

        return result

    def _parse_js_test_output(self, output: str) -> Dict[str, Any]:
        """è§£æJavaScriptæµ‹è¯•è¾“å‡º"""
        result = {"tests_run": 0, "failures": 0, "errors": 0, "passed": 0}

        import re

        # Jest/Mochaå¸¸è§æ ¼å¼
        patterns = [
            r'(\d+) passing',
            r'(\d+) failing',
            r'(\d+) tests?.*passed',
            r'(\d+) tests?.*failed',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, output, re.IGNORECASE)
            for match in matches:
                count = int(match)
                if 'pass' in pattern:
                    result['passed'] = count
                elif 'fail' in pattern:
                    result['failures'] = count

        result['tests_run'] = result['passed'] + result['failures']

        return result

    def _parse_generic_output(self, output: str) -> Dict[str, Any]:
        """é€šç”¨è¾“å‡ºè§£æ"""
        # ç®€å•çš„å¯å‘å¼è§£æ
        result = {"tests_run": 0, "failures": 0, "errors": 0}

        # ç»Ÿè®¡å…³é”®è¯å‡ºç°æ¬¡æ•°
        output_lower = output.lower()
        result['failures'] = output_lower.count('fail')
        result['errors'] = output_lower.count('error')

        return result

    def _analyze_test_results(self, test_results: List[Dict[str, Any]]) -> PluginResult:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        total_tests = sum(r['parsed']['tests_run'] for r in test_results)
        total_failures = sum(r['parsed']['failures'] for r in test_results)
        total_errors = sum(r['parsed']['errors'] for r in test_results)
        total_time = sum(r['execution_time'] for r in test_results)

        successful_commands = sum(1 for r in test_results if r['success'])
        total_commands = len(test_results)

        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_test_report(test_results)

        # åˆ¤æ–­ç»“æœ
        if total_failures > 0 or total_errors > 0:
            status = PluginStatus.FAILURE
            message = f"æµ‹è¯•å¤±è´¥: {total_failures} ä¸ªå¤±è´¥, {total_errors} ä¸ªé”™è¯¯"
        elif successful_commands < total_commands:
            status = PluginStatus.FAILURE
            message = f"æµ‹è¯•å‘½ä»¤æ‰§è¡Œå¤±è´¥: {successful_commands}/{total_commands}"
        elif total_tests == 0:
            status = PluginStatus.WARNING
            message = "æ²¡æœ‰è¿è¡Œä»»ä½•æµ‹è¯•"
        else:
            status = PluginStatus.SUCCESS
            message = f"æ‰€æœ‰æµ‹è¯•é€šè¿‡: {total_tests} ä¸ªæµ‹è¯•"

        return PluginResult(
            status=status,
            message=message,
            execution_time=total_time,
            details={
                "total_tests": total_tests,
                "total_failures": total_failures,
                "total_errors": total_errors,
                "successful_commands": successful_commands,
                "total_commands": total_commands,
                "execution_time": total_time,
                "report": report,
                "results": test_results
            }
        )

    def _generate_test_report(self, test_results: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_tests = sum(r['parsed']['tests_run'] for r in test_results)
        total_failures = sum(r['parsed']['failures'] for r in test_results)
        total_errors = sum(r['parsed']['errors'] for r in test_results)
        passed_tests = total_tests - total_failures - total_errors
        total_time = sum(r['execution_time'] for r in test_results)

        success_rate = (passed_tests / max(total_tests, 1)) * 100

        report = f"""
ğŸ§ª æµ‹è¯•æ‰§è¡ŒæŠ¥å‘Š
==============
æ‰§è¡Œå‘½ä»¤: {len(test_results)}
æ€»æµ‹è¯•æ•°: {total_tests}
é€šè¿‡: {passed_tests} | å¤±è´¥: {total_failures} | é”™è¯¯: {total_errors}
æˆåŠŸç‡: {success_rate:.1f}%
æ‰§è¡Œæ—¶é—´: {total_time:.2f}s

ğŸ“Š è¯¦ç»†ç»“æœ:
"""

        for i, result in enumerate(test_results, 1):
            status_icon = "âœ…" if result['success'] else "âŒ"
            parsed = result['parsed']

            report += f"""
{status_icon} å‘½ä»¤ {i}: {result['command'][:50]}...
  æµ‹è¯•: {parsed.get('tests_run', 0)} | å¤±è´¥: {parsed.get('failures', 0)} | é”™è¯¯: {parsed.get('errors', 0)}
  æ—¶é—´: {result['execution_time']:.2f}s | é€€å‡ºç : {result['exit_code']}
"""

            # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            if not result['success'] and result['stderr']:
                error_lines = result['stderr'].split('\n')[:3]
                for line in error_lines:
                    if line.strip():
                        report += f"  ğŸ” {line.strip()}\n"

        return report.strip()