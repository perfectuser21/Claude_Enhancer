#!/usr/bin/env python3
"""
Smart Classifier - æ™ºèƒ½å†…å®¹åˆ†ç±»å™¨

åŸºäºè¯­ä¹‰ç†è§£å’Œæ¨¡æ¿åŒ¹é…çš„æ™ºèƒ½å†…å®¹åˆ†ç±»ç³»ç»Ÿï¼Œ
èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«æ–‡æ¡£ä¸­çš„å›ºå®šã€åŠ¨æ€ã€æ˜“å˜å†…å®¹ã€‚
"""

import os
import re
import json
import hashlib
from typing import Dict, List, Tuple, Optional, Set
from datetime import datetime, timedelta
from dataclasses import dataclass
from difflib import SequenceMatcher

@dataclass
class ContentSegment:
    """å†…å®¹ç‰‡æ®µ"""
    id: str
    content: str
    type: str  # 'fixed', 'dynamic', 'volatile'
    confidence: float
    start_line: int
    end_line: int
    fingerprint: str
    last_seen: str
    change_frequency: int
    semantic_tags: List[str]

@dataclass
class ClassificationRule:
    """åˆ†ç±»è§„åˆ™"""
    name: str
    pattern: str
    content_type: str
    confidence: float
    semantic_indicators: List[str]
    anti_patterns: List[str] = None

class SmartClassifier:
    """æ™ºèƒ½å†…å®¹åˆ†ç±»å™¨"""

    def __init__(self, project_root: str = None):
        # æ™ºèƒ½æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•
        if project_root is None:
            current_file = os.path.abspath(__file__)
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_file)))

        self.project_root = project_root
        self.memory_file = os.path.join(project_root, 'features', 'claude_md_manager', 'classification_memory.json')

        # åŠ è½½åˆ†ç±»è®°å¿†
        self.memory = self._load_memory()

        # å®šä¹‰åˆ†ç±»è§„åˆ™
        self.rules = self._define_classification_rules()

    def _define_classification_rules(self) -> List[ClassificationRule]:
        """å®šä¹‰åˆ†ç±»è§„åˆ™"""
        return [
            # å›ºå®šå†…å®¹è§„åˆ™
            ClassificationRule(
                name="project_identity",
                pattern=r"é¡¹ç›®èº«ä»½|æ ¸å¿ƒåŸåˆ™|æ™ºèƒ½ç¼–æ’å™¨",
                content_type="fixed",
                confidence=0.95,
                semantic_indicators=["é¡¹ç›®", "å®šä¹‰", "æœ¬è´¨", "åŸåˆ™"]
            ),
            ClassificationRule(
                name="core_concepts",
                pattern=r"é¡¹ç›®æœ¬è´¨|æ ¸å¿ƒç†å¿µ|ä¸å˜.*ç†å¿µ",
                content_type="fixed",
                confidence=0.9,
                semantic_indicators=["æœ¬è´¨", "ç†å¿µ", "ä¸å˜", "æ°¸è¿œ", "æ ¸å¿ƒ"]
            ),
            ClassificationRule(
                name="basic_usage",
                pattern=r"åŸºæœ¬ä½¿ç”¨|python3 main/cli\.py",
                content_type="fixed",
                confidence=0.85,
                semantic_indicators=["ä½¿ç”¨", "å‘½ä»¤", "python3", "cli.py"]
            ),
            ClassificationRule(
                name="architecture",
                pattern=r"æ¶æ„|ç›®å½•ç»“æ„|Perfect21/",
                content_type="fixed",
                confidence=0.8,
                semantic_indicators=["æ¶æ„", "ç»“æ„", "ç›®å½•", "ä»£ç ", "æ¨¡å—"]
            ),
            ClassificationRule(
                name="extension_rules",
                pattern=r"æ‰©å±•è§„åˆ™|æ–°åŠŸèƒ½.*features",
                content_type="fixed",
                confidence=0.85,
                semantic_indicators=["æ‰©å±•", "è§„åˆ™", "features", "æ–°åŠŸèƒ½"]
            ),

            # åŠ¨æ€å†…å®¹è§„åˆ™
            ClassificationRule(
                name="version_info",
                pattern=r"å½“å‰ç‰ˆæœ¬.*v\d+\.\d+\.\d+|æœ€åæ›´æ–°.*\d{4}-\d{2}-\d{2}",
                content_type="dynamic",
                confidence=0.95,
                semantic_indicators=["ç‰ˆæœ¬", "æ›´æ–°", "æ—¥æœŸ", "æ—¶é—´"],
                anti_patterns=[r"ç‰ˆæœ¬æ§åˆ¶", r"ç‰ˆæœ¬ç®¡ç†"]
            ),
            ClassificationRule(
                name="module_status",
                pattern=r"æ¨¡å—çŠ¶æ€|âœ…\s*æ­£å¸¸|âŒ\s*å¼‚å¸¸",
                content_type="dynamic",
                confidence=0.9,
                semantic_indicators=["çŠ¶æ€", "æ¨¡å—", "æ­£å¸¸", "å¼‚å¸¸", "è¿è¡Œ"]
            ),
            ClassificationRule(
                name="current_status",
                pattern=r"å½“å‰çŠ¶æ€|ç³»ç»ŸçŠ¶æ€|ç”Ÿäº§å°±ç»ª",
                content_type="dynamic",
                confidence=0.85,
                semantic_indicators=["å½“å‰", "çŠ¶æ€", "ç³»ç»Ÿ", "ç”Ÿäº§"]
            ),

            # æ˜“å˜å†…å®¹è§„åˆ™
            ClassificationRule(
                name="temp_info",
                pattern=r"TODO|FIXME|ä¸´æ—¶|æµ‹è¯•ä¸­",
                content_type="volatile",
                confidence=0.9,
                semantic_indicators=["TODO", "ä¸´æ—¶", "æµ‹è¯•", "ä¿®å¤", "å¾…åŠ"]
            ),
            ClassificationRule(
                name="recent_updates",
                pattern=r"è¿‘æœŸ.*æ›´æ–°|æœ€æ–°.*å˜æ›´|åˆšåˆš.*",
                content_type="volatile",
                confidence=0.85,
                semantic_indicators=["è¿‘æœŸ", "æœ€æ–°", "åˆšåˆš", "æ–°å¢", "å˜æ›´"]
            ),
            ClassificationRule(
                name="doc_meta",
                pattern=r"æ–‡æ¡£è¯´æ˜|è¯·å‹¿.*ä¿®æ”¹|å¯éšæ—¶æ›´æ–°",
                content_type="volatile",
                confidence=0.8,
                semantic_indicators=["æ–‡æ¡£", "è¯´æ˜", "ä¿®æ”¹", "æ›´æ–°"]
            )
        ]

    def classify_content(self, content: str) -> List[ContentSegment]:
        """åˆ†ç±»å†…å®¹"""
        segments = []
        lines = content.split('\n')

        # æŒ‰ç« èŠ‚åˆ†å‰²
        sections = self._split_into_sections(lines)

        for section in sections:
            segment = self._classify_section(section)
            if segment:
                segments.append(segment)

        # æ›´æ–°åˆ†ç±»è®°å¿†
        self._update_memory(segments)

        return segments

    def _split_into_sections(self, lines: List[str]) -> List[Dict]:
        """æŒ‰ç« èŠ‚åˆ†å‰²å†…å®¹"""
        sections = []
        current_section = None

        for i, line in enumerate(lines):
            # æ ‡é¢˜è¡Œ
            if line.startswith('#') and not line.startswith('<!--'):
                if current_section:
                    sections.append(current_section)

                # ç¡®å®šæ ‡é¢˜çº§åˆ«
                level = 0
                for char in line:
                    if char == '#':
                        level += 1
                    else:
                        break

                current_section = {
                    'title': line.lstrip('#').strip(),
                    'level': level,
                    'start_line': i,
                    'content_lines': [line],
                    'raw_content': line
                }

            # HTMLæ³¨é‡Šæ ‡è®°
            elif '<!-- =====' in line:
                if current_section:
                    sections.append(current_section)

                current_section = {
                    'title': 'separator_comment',
                    'level': 0,
                    'start_line': i,
                    'content_lines': [line],
                    'raw_content': line,
                    'is_separator': True
                }
                sections.append(current_section)
                current_section = None

            # æ™®é€šå†…å®¹è¡Œ
            else:
                if current_section:
                    current_section['content_lines'].append(line)
                    current_section['raw_content'] += '\n' + line

        if current_section:
            sections.append(current_section)

        return sections

    def _classify_section(self, section: Dict) -> Optional[ContentSegment]:
        """åˆ†ç±»å•ä¸ªç« èŠ‚"""
        if section.get('is_separator'):
            # åˆ†éš”ç¬¦æ³¨é‡Š
            if 'å›ºå®šæ ¸å¿ƒ' in section['raw_content']:
                return self._create_segment(section, 'fixed', 1.0, ['separator', 'fixed_marker'])
            elif 'åŠ¨æ€çŠ¶æ€' in section['raw_content']:
                return self._create_segment(section, 'dynamic', 1.0, ['separator', 'dynamic_marker'])
            return None

        content = section['raw_content']
        title = section['title']

        # è§„åˆ™åŒ¹é…
        best_match = None
        best_confidence = 0

        for rule in self.rules:
            confidence = self._evaluate_rule(rule, content, title)
            if confidence > best_confidence:
                best_confidence = confidence
                best_match = rule

        # å†å²è®°å¿†åŒ¹é…
        memory_result = self._check_memory_match(content)
        if memory_result and memory_result['confidence'] > best_confidence:
            best_confidence = memory_result['confidence']
            content_type = memory_result['type']
            semantic_tags = memory_result.get('tags', [])
        else:
            content_type = best_match.content_type if best_match else 'unknown'
            semantic_tags = best_match.semantic_indicators if best_match else []

        # è¯­ä¹‰åˆ†æå¢å¼º
        semantic_boost = self._semantic_analysis(content, title)
        final_confidence = min(best_confidence + semantic_boost, 1.0)

        return self._create_segment(section, content_type, final_confidence, semantic_tags)

    def _evaluate_rule(self, rule: ClassificationRule, content: str, title: str) -> float:
        """è¯„ä¼°è§„åˆ™åŒ¹é…åº¦"""
        confidence = 0

        # æ¨¡å¼åŒ¹é…
        if re.search(rule.pattern, content, re.IGNORECASE):
            confidence += rule.confidence * 0.6

        if re.search(rule.pattern, title, re.IGNORECASE):
            confidence += rule.confidence * 0.4

        # åæ¨¡å¼æ£€æŸ¥
        if rule.anti_patterns:
            for anti_pattern in rule.anti_patterns:
                if re.search(anti_pattern, content, re.IGNORECASE):
                    confidence *= 0.5  # å¤§å¹…é™ä½ç½®ä¿¡åº¦

        # è¯­ä¹‰æŒ‡æ ‡æ£€æŸ¥
        semantic_matches = 0
        for indicator in rule.semantic_indicators:
            if indicator.lower() in content.lower():
                semantic_matches += 1

        if semantic_matches > 0:
            semantic_ratio = semantic_matches / len(rule.semantic_indicators)
            confidence += semantic_ratio * 0.3

        return min(confidence, 1.0)

    def _semantic_analysis(self, content: str, title: str) -> float:
        """è¯­ä¹‰åˆ†æå¢å¼º"""
        boost = 0
        content_lower = content.lower()
        title_lower = title.lower()

        # å›ºå®šå†…å®¹æŒ‡æ ‡
        fixed_indicators = [
            'ä¸å˜', 'æ°¸è¿œ', 'å§‹ç»ˆ', 'æ ¸å¿ƒ', 'æœ¬è´¨', 'åŸåˆ™',
            'å®šä¹‰', 'æ¶æ„', 'è®¾è®¡', 'è§„åˆ™', 'åŸºæœ¬'
        ]

        # åŠ¨æ€å†…å®¹æŒ‡æ ‡
        dynamic_indicators = [
            'å½“å‰', 'æœ€æ–°', 'çŠ¶æ€', 'ç‰ˆæœ¬', 'æ›´æ–°', 'è¿è¡Œ',
            'æ­£å¸¸', 'å¼‚å¸¸', 'âœ…', 'âŒ', 'æ•°æ®'
        ]

        # æ˜“å˜å†…å®¹æŒ‡æ ‡
        volatile_indicators = [
            'ä¸´æ—¶', 'å¾…å®š', 'todo', 'è®¡åˆ’', 'å³å°†', 'æµ‹è¯•',
            'è°ƒè¯•', 'å¼€å‘ä¸­', 'ä¿®å¤', 'æ”¹è¿›'
        ]

        # è®¡ç®—å„ç±»å‹æŒ‡æ ‡å¯†åº¦
        fixed_count = sum(1 for ind in fixed_indicators if ind in content_lower or ind in title_lower)
        dynamic_count = sum(1 for ind in dynamic_indicators if ind in content_lower or ind in title_lower)
        volatile_count = sum(1 for ind in volatile_indicators if ind in content_lower or ind in title_lower)

        total_indicators = fixed_count + dynamic_count + volatile_count
        if total_indicators > 0:
            # æ ¹æ®æŒ‡æ ‡åˆ†å¸ƒç»™å‡ºè¯­ä¹‰å¢å¼º
            if fixed_count > dynamic_count and fixed_count > volatile_count:
                boost = 0.1  # åå‘å›ºå®š
            elif dynamic_count > volatile_count:
                boost = 0.05  # åå‘åŠ¨æ€
            else:
                boost = 0.02  # åå‘æ˜“å˜

        return boost

    def _create_segment(self, section: Dict, content_type: str, confidence: float, semantic_tags: List[str]) -> ContentSegment:
        """åˆ›å»ºå†…å®¹ç‰‡æ®µ"""
        content = section['raw_content']
        fingerprint = hashlib.md5(content.encode()).hexdigest()

        return ContentSegment(
            id=f"{content_type}_{section['start_line']}_{fingerprint[:8]}",
            content=content,
            type=content_type,
            confidence=confidence,
            start_line=section['start_line'],
            end_line=section['start_line'] + len(section['content_lines']) - 1,
            fingerprint=fingerprint,
            last_seen=datetime.now().isoformat(),
            change_frequency=self._get_change_frequency(fingerprint),
            semantic_tags=semantic_tags
        )

    def _get_change_frequency(self, fingerprint: str) -> int:
        """è·å–å†…å®¹å˜æ›´é¢‘ç‡"""
        if fingerprint in self.memory:
            return self.memory[fingerprint].get('change_count', 0)
        return 0

    def _check_memory_match(self, content: str) -> Optional[Dict]:
        """æ£€æŸ¥è®°å¿†åŒ¹é…"""
        fingerprint = hashlib.md5(content.encode()).hexdigest()

        if fingerprint in self.memory:
            memory_item = self.memory[fingerprint]
            # æ—¶æ•ˆæ€§æ£€æŸ¥
            last_seen = datetime.fromisoformat(memory_item['last_seen'])
            days_old = (datetime.now() - last_seen).days

            if days_old < 30:  # 30å¤©å†…çš„è®°å¿†æœ‰æ•ˆ
                return {
                    'type': memory_item['type'],
                    'confidence': memory_item['confidence'] * (1 - days_old * 0.01),  # æ—¶é—´è¡°å‡
                    'tags': memory_item.get('tags', [])
                }

        # ç›¸ä¼¼åº¦åŒ¹é…
        for stored_fingerprint, stored_data in self.memory.items():
            if stored_data.get('content'):
                similarity = self._calculate_similarity(content, stored_data['content'])
                if similarity > 0.8:  # 80%ç›¸ä¼¼åº¦
                    return {
                        'type': stored_data['type'],
                        'confidence': stored_data['confidence'] * similarity * 0.8,
                        'tags': stored_data.get('tags', [])
                    }

        return None

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

    def _update_memory(self, segments: List[ContentSegment]):
        """æ›´æ–°åˆ†ç±»è®°å¿†"""
        for segment in segments:
            if segment.fingerprint in self.memory:
                # æ›´æ–°ç°æœ‰è®°å¿†
                self.memory[segment.fingerprint]['last_seen'] = segment.last_seen
                self.memory[segment.fingerprint]['change_count'] += 1
                # ç½®ä¿¡åº¦å­¦ä¹ è°ƒæ•´
                old_confidence = self.memory[segment.fingerprint]['confidence']
                self.memory[segment.fingerprint]['confidence'] = (old_confidence + segment.confidence) / 2
            else:
                # åˆ›å»ºæ–°è®°å¿†
                self.memory[segment.fingerprint] = {
                    'type': segment.type,
                    'confidence': segment.confidence,
                    'first_seen': segment.last_seen,
                    'last_seen': segment.last_seen,
                    'change_count': 1,
                    'content': segment.content[:200],  # å­˜å‚¨å‰200å­—ç¬¦ç”¨äºç›¸ä¼¼åº¦åŒ¹é…
                    'tags': segment.semantic_tags
                }

        self._save_memory()

    def _load_memory(self) -> Dict:
        """åŠ è½½åˆ†ç±»è®°å¿†"""
        if os.path.exists(self.memory_file):
            try:
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {}

    def _save_memory(self):
        """ä¿å­˜åˆ†ç±»è®°å¿†"""
        # æ¸…ç†è¿‡æœŸè®°å¿†ï¼ˆè¶…è¿‡6ä¸ªæœˆï¼‰
        cutoff_date = datetime.now() - timedelta(days=180)
        cleaned_memory = {}

        for fingerprint, data in self.memory.items():
            try:
                last_seen = datetime.fromisoformat(data['last_seen'])
                if last_seen > cutoff_date:
                    cleaned_memory[fingerprint] = data
            except:
                pass  # è·³è¿‡æ— æ•ˆæ•°æ®

        self.memory = cleaned_memory

        # ä¿å­˜åˆ°æ–‡ä»¶
        os.makedirs(os.path.dirname(self.memory_file), exist_ok=True)
        with open(self.memory_file, 'w', encoding='utf-8') as f:
            json.dump(self.memory, f, indent=2, ensure_ascii=False)

    def get_classification_report(self, segments: List[ContentSegment]) -> Dict:
        """ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š"""
        report = {
            'total_segments': len(segments),
            'by_type': {'fixed': 0, 'dynamic': 0, 'volatile': 0, 'unknown': 0},
            'confidence_distribution': {'high': 0, 'medium': 0, 'low': 0},
            'segments_detail': []
        }

        for segment in segments:
            # æŒ‰ç±»å‹ç»Ÿè®¡
            report['by_type'][segment.type] += 1

            # æŒ‰ç½®ä¿¡åº¦ç»Ÿè®¡
            if segment.confidence >= 0.8:
                report['confidence_distribution']['high'] += 1
            elif segment.confidence >= 0.5:
                report['confidence_distribution']['medium'] += 1
            else:
                report['confidence_distribution']['low'] += 1

            # è¯¦ç»†ä¿¡æ¯
            report['segments_detail'].append({
                'id': segment.id,
                'type': segment.type,
                'confidence': round(segment.confidence, 3),
                'lines': f"{segment.start_line}-{segment.end_line}",
                'content_preview': segment.content[:100] + "..." if len(segment.content) > 100 else segment.content,
                'tags': segment.semantic_tags
            })

        return report

if __name__ == "__main__":
    # æµ‹è¯•æ™ºèƒ½åˆ†ç±»å™¨
    classifier = SmartClassifier()

    # æµ‹è¯•å†…å®¹
    test_content = """# Perfect21 é¡¹ç›®æ ¸å¿ƒæ–‡æ¡£

> ğŸ¯ **é¡¹ç›®èº«ä»½**: Perfect21 - æ™ºèƒ½ç¼–æ’å™¨

## ğŸ¯ é¡¹ç›®æœ¬è´¨

Perfect21 = **æ™ºèƒ½ç¼–æ’å™¨** + claude-code-unified-agents

### ğŸ”‘ ä¸å˜çš„æ ¸å¿ƒç†å¿µ
- **ä¸é‡å¤é€ è½®å­**: æ°¸è¿œä¼˜å…ˆä½¿ç”¨å®˜æ–¹Agent

## ğŸ“Š å½“å‰çŠ¶æ€

### ğŸš€ ç‰ˆæœ¬ä¿¡æ¯
- **å½“å‰ç‰ˆæœ¬**: v2.3.0 (ç”Ÿäº§å°±ç»ª)
- **æœ€åæ›´æ–°**: 2025-09-16

### ğŸ”§ æ¨¡å—çŠ¶æ€
- **capability_discovery**: âœ… æ­£å¸¸"""

    segments = classifier.classify_content(test_content)
    report = classifier.get_classification_report(segments)

    print("=== åˆ†ç±»ç»“æœ ===")
    print(f"æ€»æ®µè½æ•°: {report['total_segments']}")
    print(f"å›ºå®š: {report['by_type']['fixed']}, åŠ¨æ€: {report['by_type']['dynamic']}, æ˜“å˜: {report['by_type']['volatile']}")
    print(f"é«˜ç½®ä¿¡åº¦: {report['confidence_distribution']['high']}")

    print("\n=== è¯¦ç»†åˆ†ç±» ===")
    for detail in report['segments_detail']:
        print(f"[{detail['type'].upper()}] {detail['confidence']:.2f} - {detail['content_preview'][:50]}...")