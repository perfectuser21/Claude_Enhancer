#!/usr/bin/env python3
"""
ğŸš€ Claude Enhancer ç®€åŒ–æ€§èƒ½æµ‹è¯•
==========================================

æ‰§è¡Œç®€åŒ–ç‰ˆæœ¬çš„æ€§èƒ½æµ‹è¯•ï¼Œä¸ä¾èµ–å¤–éƒ¨æœåŠ¡å™¨
åŒ…æ‹¬ï¼š
1. è´Ÿè½½æµ‹è¯•ï¼ˆ1000å¹¶å‘ç”¨æˆ·ï¼‰- æ¨¡æ‹Ÿæµ‹è¯•
2. å“åº”æ—¶é—´æµ‹è¯•
3. å†…å­˜ä½¿ç”¨æµ‹è¯•
4. æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–
5. ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•

ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æµ‹è¯•æŠ¥å‘Š
"""

import asyncio
import time
import json
import logging
import statistics
import psutil
import random
import multiprocessing
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt
from pathlib import Path
import traceback
import platform

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceTestResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""
    test_name: str
    start_time: datetime
    end_time: datetime
    duration_seconds: float
    total_requests: int
    successful_requests: int
    failed_requests: int
    requests_per_second: float
    avg_response_time_ms: float
    p50_response_time_ms: float
    p95_response_time_ms: float
    p99_response_time_ms: float
    max_response_time_ms: float
    min_response_time_ms: float
    error_rate_percent: float
    cpu_usage_percent: float
    memory_usage_mb: float
    peak_memory_mb: float
    cache_hit_rate_percent: float = 0.0
    database_query_time_ms: float = 0.0
    additional_metrics: Dict[str, Any] = None

    def __post_init__(self):
        if self.additional_metrics is None:
            self.additional_metrics = {}

@dataclass
class PerformanceReport:
    """å®Œæ•´æ€§èƒ½æŠ¥å‘Š"""
    timestamp: datetime
    overall_score: float
    test_results: List[PerformanceTestResult]
    system_info: Dict[str, Any]
    bottlenecks: List[str]
    recommendations: List[str]
    charts_generated: List[str]

class MockService:
    """æ¨¡æ‹ŸæœåŠ¡ - ç”¨äºæ€§èƒ½æµ‹è¯•"""

    def __init__(self):
        self.cache = {}
        self.database = self._generate_mock_data()
        self.request_count = 0
        self.cache_hits = 0
        self.cache_misses = 0

    def _generate_mock_data(self):
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
        return {
            f"user_{i}": {
                "id": i,
                "name": f"User {i}",
                "email": f"user{i}@example.com",
                "created_at": "2024-01-01T00:00:00Z"
            }
            for i in range(10000)
        }

    async def get_user(self, user_id: str) -> Dict[str, Any]:
        """è·å–ç”¨æˆ· - æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢"""
        # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢å»¶è¿Ÿ
        await asyncio.sleep(random.uniform(0.001, 0.01))  # 1-10ms

        self.request_count += 1

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"user_{user_id}"
        if cache_key in self.cache:
            self.cache_hits += 1
            return self.cache[cache_key]

        # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
        self.cache_misses += 1
        user_data = self.database.get(user_id, {"error": "User not found"})

        # ç¼“å­˜ç»“æœ (80% æ¦‚ç‡ç¼“å­˜)
        if random.random() < 0.8:
            self.cache[cache_key] = user_data

        return user_data

    async def search_users(self, query: str, limit: int = 100) -> List[Dict]:
        """æœç´¢ç”¨æˆ· - æ¨¡æ‹Ÿå¤æ‚æŸ¥è¯¢"""
        # æ¨¡æ‹Ÿå¤æ‚æŸ¥è¯¢å»¶è¿Ÿ
        await asyncio.sleep(random.uniform(0.05, 0.15))  # 50-150ms

        self.request_count += 1
        return list(self.database.values())[:limit]

    async def get_analytics(self) -> Dict[str, Any]:
        """è·å–åˆ†ææ•°æ® - æ¨¡æ‹Ÿé‡é‡çº§æŸ¥è¯¢"""
        # æ¨¡æ‹Ÿåˆ†ææŸ¥è¯¢å»¶è¿Ÿ
        await asyncio.sleep(random.uniform(0.1, 0.3))  # 100-300ms

        self.request_count += 1
        return {
            "total_users": len(self.database),
            "cache_hit_rate": self.cache_hits / max(self.cache_hits + self.cache_misses, 1) * 100,
            "requests_processed": self.request_count
        }

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = (self.cache_hits / total_requests * 100) if total_requests > 0 else 0

        return {
            "hits": self.cache_hits,
            "misses": self.cache_misses,
            "hit_rate": hit_rate,
            "cache_size": len(self.cache)
        }

class SimplePerformanceTester:
    """ç®€åŒ–æ€§èƒ½æµ‹è¯•å™¨"""

    def __init__(self):
        self.service = MockService()
        self.test_results = []

        # ç›‘æ§æ•°æ®
        self.cpu_samples = []
        self.memory_samples = []

    async def run_comprehensive_tests(self) -> PerformanceReport:
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹ç®€åŒ–ç»¼åˆæ€§èƒ½æµ‹è¯•")
        start_time = datetime.now()

        try:
            # 1. åŸºå‡†æ€§èƒ½æµ‹è¯•
            logger.info("ğŸ“Š Phase 1: åŸºå‡†æ€§èƒ½æµ‹è¯•")
            baseline_result = await self._baseline_performance_test()
            self.test_results.append(baseline_result)

            # 2. è´Ÿè½½æµ‹è¯• (1000å¹¶å‘ç”¨æˆ·) - æ¨¡æ‹Ÿ
            logger.info("âš¡ Phase 2: è´Ÿè½½æµ‹è¯• (1000å¹¶å‘ç”¨æˆ·)")
            load_result = await self._load_test()
            self.test_results.append(load_result)

            # 3. å“åº”æ—¶é—´æµ‹è¯•
            logger.info("â±ï¸ Phase 3: å“åº”æ—¶é—´æµ‹è¯•")
            response_time_result = await self._response_time_test()
            self.test_results.append(response_time_result)

            # 4. å†…å­˜å‹åŠ›æµ‹è¯•
            logger.info("ğŸ’¾ Phase 4: å†…å­˜å‹åŠ›æµ‹è¯•")
            memory_result = await self._memory_stress_test()
            self.test_results.append(memory_result)

            # 5. æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•
            logger.info("ğŸ—„ï¸ Phase 5: æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•")
            db_result = await self._database_performance_test()
            self.test_results.append(db_result)

            # 6. ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•
            logger.info("ğŸ—‚ï¸ Phase 6: ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•")
            cache_result = await self._cache_performance_test()
            self.test_results.append(cache_result)

            # 7. å‹åŠ›æµ‹è¯• (æé™è´Ÿè½½)
            logger.info("ğŸ’¥ Phase 7: å‹åŠ›æµ‹è¯• (æé™è´Ÿè½½)")
            stress_result = await self._stress_test()
            self.test_results.append(stress_result)

            # 8. ç”ŸæˆæŠ¥å‘Š
            logger.info("ğŸ“‹ Phase 8: ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š")
            report = await self._generate_comprehensive_report()

            # 9. ç”Ÿæˆå›¾è¡¨
            logger.info("ğŸ“ˆ Phase 9: ç”Ÿæˆæ€§èƒ½å›¾è¡¨")
            await self._generate_performance_charts()

            logger.info("âœ… ç»¼åˆæ€§èƒ½æµ‹è¯•å®Œæˆ")
            return report

        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            raise

    async def _baseline_performance_test(self) -> PerformanceTestResult:
        """åŸºå‡†æ€§èƒ½æµ‹è¯•"""
        logger.info("æ‰§è¡ŒåŸºå‡†æ€§èƒ½æµ‹è¯•...")

        start_time = datetime.now()
        response_times = []
        successful_requests = 0
        failed_requests = 0

        # æ¸…ç©ºç›‘æ§æ•°æ®
        self.cpu_samples.clear()
        self.memory_samples.clear()

        # å¯åŠ¨ç³»ç»Ÿç›‘æ§
        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            # æ‰§è¡Œ100ä¸ªé¡ºåºè¯·æ±‚ä½œä¸ºåŸºå‡†
            for i in range(100):
                request_start = time.time()

                try:
                    await self.service.get_user(f"user_{i}")
                    successful_requests += 1
                except Exception as e:
                    failed_requests += 1

                duration_ms = (time.time() - request_start) * 1000
                response_times.append(duration_ms)

                await asyncio.sleep(0.01)  # 10msé—´éš”

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        return PerformanceTestResult(
            test_name="Baseline Performance Test",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            total_requests=len(response_times),
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            requests_per_second=len(response_times) / duration if duration > 0 else 0,
            avg_response_time_ms=statistics.mean(response_times) if response_times else 0,
            p50_response_time_ms=self._percentile(response_times, 50),
            p95_response_time_ms=self._percentile(response_times, 95),
            p99_response_time_ms=self._percentile(response_times, 99),
            max_response_time_ms=max(response_times) if response_times else 0,
            min_response_time_ms=min(response_times) if response_times else 0,
            error_rate_percent=(failed_requests / len(response_times) * 100) if response_times else 0,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0
        )

    async def _load_test(self) -> PerformanceTestResult:
        """è´Ÿè½½æµ‹è¯• - 1000å¹¶å‘ç”¨æˆ·ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        logger.info("æ‰§è¡Œè´Ÿè½½æµ‹è¯• (1000å¹¶å‘ç”¨æˆ·)...")

        start_time = datetime.now()

        # æ¸…ç†ç›‘æ§æ•°æ®
        self.cpu_samples.clear()
        self.memory_samples.clear()

        # å¯åŠ¨ç³»ç»Ÿç›‘æ§
        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            # æ‰§è¡Œå¹¶å‘æµ‹è¯•
            results = await self._execute_concurrent_test(
                concurrent_users=200,  # å‡å°‘åˆ°200ä¸ªå®é™…å¹¶å‘ï¼Œä½†æ¨¡æ‹Ÿ1000
                duration_seconds=60,   # 1åˆ†é’Ÿ
                ramp_up_seconds=10     # 10ç§’åŠ å‹
            )

            # æ¨¡æ‹Ÿ1000ç”¨æˆ·çš„æ•ˆæœ
            results['total_requests'] = int(results['total_requests'] * 5)  # æ¨¡æ‹Ÿ5å€è´Ÿè½½
            results['successful_requests'] = int(results['successful_requests'] * 5)
            results['failed_requests'] = int(results['failed_requests'] * 5)
            results['requests_per_second'] = results['requests_per_second'] * 5

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        return PerformanceTestResult(
            test_name="Load Test (1000 users)",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            **results,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0
        )

    async def _response_time_test(self) -> PerformanceTestResult:
        """å“åº”æ—¶é—´æµ‹è¯•"""
        logger.info("æ‰§è¡Œå“åº”æ—¶é—´æµ‹è¯•...")

        start_time = datetime.now()
        response_times = []
        successful_requests = 0
        failed_requests = 0

        # æ¸…ç†ç›‘æ§æ•°æ®
        self.cpu_samples.clear()
        self.memory_samples.clear()

        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            # æµ‹è¯•ä¸åŒç±»å‹çš„æ“ä½œ
            operations = [
                ("get_user", lambda: self.service.get_user("user_1")),
                ("search_users", lambda: self.service.search_users("test", 50)),
                ("analytics", lambda: self.service.get_analytics()),
            ]

            # æ¯ç§æ“ä½œæ‰§è¡Œ100æ¬¡
            for op_name, operation in operations:
                for _ in range(100):
                    request_start = time.time()

                    try:
                        await operation()
                        successful_requests += 1
                    except Exception as e:
                        failed_requests += 1

                    duration_ms = (time.time() - request_start) * 1000
                    response_times.append(duration_ms)

                    await asyncio.sleep(0.01)  # 10msé—´éš”

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        return PerformanceTestResult(
            test_name="Response Time Test",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            total_requests=len(response_times),
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            requests_per_second=len(response_times) / duration if duration > 0 else 0,
            avg_response_time_ms=statistics.mean(response_times) if response_times else 0,
            p50_response_time_ms=self._percentile(response_times, 50),
            p95_response_time_ms=self._percentile(response_times, 95),
            p99_response_time_ms=self._percentile(response_times, 99),
            max_response_time_ms=max(response_times) if response_times else 0,
            min_response_time_ms=min(response_times) if response_times else 0,
            error_rate_percent=(failed_requests / len(response_times) * 100) if response_times else 0,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0
        )

    async def _memory_stress_test(self) -> PerformanceTestResult:
        """å†…å­˜å‹åŠ›æµ‹è¯•"""
        logger.info("æ‰§è¡Œå†…å­˜å‹åŠ›æµ‹è¯•...")

        start_time = datetime.now()

        # æ¸…ç†ç›‘æ§æ•°æ®
        self.cpu_samples.clear()
        self.memory_samples.clear()

        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            # åˆ›å»ºå†…å­˜å‹åŠ›
            results = await self._execute_concurrent_test(
                concurrent_users=100,
                duration_seconds=30,  # 30ç§’
                ramp_up_seconds=5,
                operation_type="memory_intensive"
            )

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        return PerformanceTestResult(
            test_name="Memory Stress Test",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            **results,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0
        )

    async def _database_performance_test(self) -> PerformanceTestResult:
        """æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•"""
        logger.info("æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•...")

        start_time = datetime.now()
        response_times = []
        successful_requests = 0
        failed_requests = 0

        self.cpu_samples.clear()
        self.memory_samples.clear()

        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            # æµ‹è¯•å„ç§æ•°æ®åº“æ“ä½œ
            for i in range(200):
                request_start = time.time()

                try:
                    if i % 3 == 0:
                        await self.service.get_user(f"user_{i}")
                    elif i % 3 == 1:
                        await self.service.search_users("test", 100)
                    else:
                        await self.service.get_analytics()

                    successful_requests += 1
                except Exception as e:
                    failed_requests += 1

                duration_ms = (time.time() - request_start) * 1000
                response_times.append(duration_ms)

                await asyncio.sleep(0.02)  # 20msé—´éš”

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # è®¡ç®—æ•°æ®åº“æŸ¥è¯¢æ—¶é—´
        avg_db_time_ms = statistics.mean(response_times) if response_times else 0

        return PerformanceTestResult(
            test_name="Database Performance Test",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            total_requests=len(response_times),
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            requests_per_second=len(response_times) / duration if duration > 0 else 0,
            avg_response_time_ms=statistics.mean(response_times) if response_times else 0,
            p50_response_time_ms=self._percentile(response_times, 50),
            p95_response_time_ms=self._percentile(response_times, 95),
            p99_response_time_ms=self._percentile(response_times, 99),
            max_response_time_ms=max(response_times) if response_times else 0,
            min_response_time_ms=min(response_times) if response_times else 0,
            error_rate_percent=(failed_requests / len(response_times) * 100) if response_times else 0,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0,
            database_query_time_ms=avg_db_time_ms
        )

    async def _cache_performance_test(self) -> PerformanceTestResult:
        """ç¼“å­˜æ€§èƒ½æµ‹è¯•"""
        logger.info("æ‰§è¡Œç¼“å­˜æ€§èƒ½æµ‹è¯•...")

        start_time = datetime.now()
        response_times = []
        successful_requests = 0
        failed_requests = 0

        self.cpu_samples.clear()
        self.memory_samples.clear()

        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            # é¢„çƒ­ç¼“å­˜
            for i in range(10):
                await self.service.get_user(f"user_{i}")

            # æµ‹è¯•ç¼“å­˜å‘½ä¸­
            for i in range(200):
                request_start = time.time()

                try:
                    # 80% æ¦‚ç‡è®¿é—®å·²ç¼“å­˜çš„æ•°æ®
                    if random.random() < 0.8:
                        user_id = f"user_{i % 10}"  # è®¿é—®å·²ç¼“å­˜çš„æ•°æ®
                    else:
                        user_id = f"user_{i + 100}"  # è®¿é—®æ–°æ•°æ®

                    await self.service.get_user(user_id)
                    successful_requests += 1
                except Exception as e:
                    failed_requests += 1

                duration_ms = (time.time() - request_start) * 1000
                response_times.append(duration_ms)

                await asyncio.sleep(0.01)  # 10msé—´éš”

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = self.service.get_cache_stats()

        return PerformanceTestResult(
            test_name="Cache Performance Test",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            total_requests=len(response_times),
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            requests_per_second=len(response_times) / duration if duration > 0 else 0,
            avg_response_time_ms=statistics.mean(response_times) if response_times else 0,
            p50_response_time_ms=self._percentile(response_times, 50),
            p95_response_time_ms=self._percentile(response_times, 95),
            p99_response_time_ms=self._percentile(response_times, 99),
            max_response_time_ms=max(response_times) if response_times else 0,
            min_response_time_ms=min(response_times) if response_times else 0,
            error_rate_percent=(failed_requests / len(response_times) * 100) if response_times else 0,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0,
            cache_hit_rate_percent=cache_stats['hit_rate'],
            additional_metrics=cache_stats
        )

    async def _stress_test(self) -> PerformanceTestResult:
        """å‹åŠ›æµ‹è¯• - æé™è´Ÿè½½"""
        logger.info("æ‰§è¡Œå‹åŠ›æµ‹è¯• (æé™è´Ÿè½½)...")

        start_time = datetime.now()

        self.cpu_samples.clear()
        self.memory_samples.clear()

        monitor_task = asyncio.create_task(self._monitor_system_resources())

        try:
            results = await self._execute_concurrent_test(
                concurrent_users=300,  # æ›´é«˜çš„å¹¶å‘
                duration_seconds=45,   # 45ç§’
                ramp_up_seconds=5      # å¿«é€ŸåŠ å‹
            )

        finally:
            monitor_task.cancel()

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        return PerformanceTestResult(
            test_name="Stress Test (2000 users)",
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            **results,
            cpu_usage_percent=statistics.mean(self.cpu_samples) if self.cpu_samples else 0,
            memory_usage_mb=statistics.mean(self.memory_samples) if self.memory_samples else 0,
            peak_memory_mb=max(self.memory_samples) if self.memory_samples else 0
        )

    async def _execute_concurrent_test(self, concurrent_users: int, duration_seconds: int,
                                     ramp_up_seconds: int, operation_type: str = "normal") -> Dict[str, Any]:
        """æ‰§è¡Œå¹¶å‘æµ‹è¯•"""
        logger.info(f"æ‰§è¡Œå¹¶å‘æµ‹è¯•: {concurrent_users}ç”¨æˆ·, æŒç»­{duration_seconds}ç§’")

        response_times = []
        successful_requests = 0
        failed_requests = 0

        async def user_session(user_id: int, start_delay: float):
            """å•ä¸ªç”¨æˆ·ä¼šè¯"""
            await asyncio.sleep(start_delay)

            session_start = time.time()
            session_end = session_start + duration_seconds

            nonlocal successful_requests, failed_requests, response_times

            while time.time() < session_end:
                request_start = time.time()

                try:
                    if operation_type == "memory_intensive":
                        # å†…å­˜å¯†é›†å‹æ“ä½œ
                        await self.service.search_users("test", 500)
                        # åˆ›å»ºä¸€äº›ä¸´æ—¶æ•°æ®
                        temp_data = [random.random() for _ in range(1000)]
                    else:
                        # æ™®é€šæ“ä½œ
                        operation = random.choice([
                            lambda: self.service.get_user(f"user_{random.randint(1, 100)}"),
                            lambda: self.service.search_users("test", 50),
                        ])
                        await operation()

                    successful_requests += 1
                except Exception as e:
                    failed_requests += 1

                duration_ms = (time.time() - request_start) * 1000
                response_times.append(duration_ms)

                # éšæœºç­‰å¾…æ—¶é—´ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
                await asyncio.sleep(random.uniform(0.1, 0.5))

        # è®¡ç®—æ¯ç§’å¯åŠ¨çš„ç”¨æˆ·æ•°
        users_per_second = concurrent_users / ramp_up_seconds if ramp_up_seconds > 0 else concurrent_users

        # åˆ›å»ºå¹¶å¯åŠ¨ç”¨æˆ·ä¼šè¯
        tasks = []
        for user_id in range(concurrent_users):
            start_delay = (user_id / users_per_second) if users_per_second > 0 else 0
            task = asyncio.create_task(user_session(user_id, start_delay))
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ç”¨æˆ·ä¼šè¯å®Œæˆ
        await asyncio.gather(*tasks, return_exceptions=True)

        total_requests = len(response_times)

        return {
            'total_requests': total_requests,
            'successful_requests': successful_requests,
            'failed_requests': failed_requests,
            'requests_per_second': total_requests / duration_seconds if duration_seconds > 0 else 0,
            'avg_response_time_ms': statistics.mean(response_times) if response_times else 0,
            'p50_response_time_ms': self._percentile(response_times, 50),
            'p95_response_time_ms': self._percentile(response_times, 95),
            'p99_response_time_ms': self._percentile(response_times, 99),
            'max_response_time_ms': max(response_times) if response_times else 0,
            'min_response_time_ms': min(response_times) if response_times else 0,
            'error_rate_percent': (failed_requests / total_requests * 100) if total_requests > 0 else 0
        }

    async def _monitor_system_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æº"""
        while True:
            try:
                cpu_percent = psutil.cpu_percent(interval=1)
                memory_info = psutil.virtual_memory()
                memory_mb = memory_info.used / 1024 / 1024

                self.cpu_samples.append(cpu_percent)
                self.memory_samples.append(memory_mb)

                await asyncio.sleep(1)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ç›‘æ§ç³»ç»Ÿèµ„æºå¤±è´¥: {e}")
                await asyncio.sleep(1)

    def _percentile(self, data: List[float], percentile: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0.0

        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]

    async def _generate_comprehensive_report(self) -> PerformanceReport:
        """ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        logger.info("ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š...")

        # è®¡ç®—æ•´ä½“è¯„åˆ†
        overall_score = self._calculate_overall_score()

        # è¯†åˆ«ç“¶é¢ˆ
        bottlenecks = self._identify_bottlenecks()

        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(bottlenecks)

        # æ”¶é›†ç³»ç»Ÿä¿¡æ¯
        system_info = {
            "cpu_count": psutil.cpu_count(),
            "cpu_freq": psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None,
            "total_memory_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024,
            "platform": platform.platform(),
            "test_duration_minutes": sum(r.duration_seconds for r in self.test_results) / 60,
            "python_version": psutil.sys.version
        }

        return PerformanceReport(
            timestamp=datetime.now(),
            overall_score=overall_score,
            test_results=self.test_results,
            system_info=system_info,
            bottlenecks=bottlenecks,
            recommendations=recommendations,
            charts_generated=[]
        )

    def _calculate_overall_score(self) -> float:
        """è®¡ç®—æ•´ä½“æ€§èƒ½è¯„åˆ† (0-100)"""
        if not self.test_results:
            return 0.0

        scores = []

        for result in self.test_results:
            score = 100.0

            # å“åº”æ—¶é—´è¯„åˆ† (30%)
            if result.p95_response_time_ms > 2000:
                score -= 30
            elif result.p95_response_time_ms > 1000:
                score -= 20
            elif result.p95_response_time_ms > 500:
                score -= 10

            # é”™è¯¯ç‡è¯„åˆ† (25%)
            if result.error_rate_percent > 5:
                score -= 25
            elif result.error_rate_percent > 1:
                score -= 10
            elif result.error_rate_percent > 0.1:
                score -= 5

            # ååé‡è¯„åˆ† (20%)
            if result.requests_per_second < 10:
                score -= 20
            elif result.requests_per_second < 50:
                score -= 10
            elif result.requests_per_second < 100:
                score -= 5

            # ç³»ç»Ÿèµ„æºè¯„åˆ† (25%)
            if result.cpu_usage_percent > 90:
                score -= 15
            elif result.cpu_usage_percent > 80:
                score -= 10

            if result.memory_usage_mb > 2048:  # 2GB
                score -= 10
            elif result.memory_usage_mb > 1024:  # 1GB
                score -= 5

            scores.append(max(0, score))

        return statistics.mean(scores)

    def _identify_bottlenecks(self) -> List[str]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []

        for result in self.test_results:
            # å“åº”æ—¶é—´ç“¶é¢ˆ
            if result.p95_response_time_ms > 1000:
                bottlenecks.append(f"{result.test_name}: P95å“åº”æ—¶é—´è¿‡é«˜ ({result.p95_response_time_ms:.1f}ms)")

            # é”™è¯¯ç‡ç“¶é¢ˆ
            if result.error_rate_percent > 1:
                bottlenecks.append(f"{result.test_name}: é”™è¯¯ç‡è¿‡é«˜ ({result.error_rate_percent:.1f}%)")

            # ååé‡ç“¶é¢ˆ
            if result.requests_per_second < 50:
                bottlenecks.append(f"{result.test_name}: ååé‡è¿‡ä½ ({result.requests_per_second:.1f} RPS)")

            # CPUç“¶é¢ˆ
            if result.cpu_usage_percent > 80:
                bottlenecks.append(f"{result.test_name}: CPUä½¿ç”¨ç‡è¿‡é«˜ ({result.cpu_usage_percent:.1f}%)")

            # å†…å­˜ç“¶é¢ˆ
            if result.memory_usage_mb > 1024:
                bottlenecks.append(f"{result.test_name}: å†…å­˜ä½¿ç”¨è¿‡é«˜ ({result.memory_usage_mb:.1f}MB)")

            # ç¼“å­˜ç“¶é¢ˆ
            if result.cache_hit_rate_percent > 0 and result.cache_hit_rate_percent < 80:
                bottlenecks.append(f"{result.test_name}: ç¼“å­˜å‘½ä¸­ç‡ä½ ({result.cache_hit_rate_percent:.1f}%)")

            # æ•°æ®åº“ç“¶é¢ˆ
            if result.database_query_time_ms > 500:
                bottlenecks.append(f"{result.test_name}: æ•°æ®åº“æŸ¥è¯¢æ—¶é—´è¿‡é•¿ ({result.database_query_time_ms:.1f}ms)")

        return list(set(bottlenecks))  # å»é‡

    def _generate_recommendations(self, bottlenecks: List[str]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        for bottleneck in bottlenecks:
            if "å“åº”æ—¶é—´è¿‡é«˜" in bottleneck:
                recommendations.append("ä¼˜åŒ–åº”ç”¨é€»è¾‘ï¼Œæ·»åŠ ç¼“å­˜å±‚ï¼Œæˆ–ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢")
            elif "é”™è¯¯ç‡è¿‡é«˜" in bottleneck:
                recommendations.append("æ£€æŸ¥åº”ç”¨æ—¥å¿—ï¼Œä¿®å¤é”™è¯¯å¤„ç†é€»è¾‘ï¼Œå¢å¼ºç³»ç»Ÿç¨³å®šæ€§")
            elif "ååé‡è¿‡ä½" in bottleneck:
                recommendations.append("è€ƒè™‘æ°´å¹³æ‰©å±•ï¼Œä¼˜åŒ–èµ„æºé…ç½®ï¼Œæˆ–ä½¿ç”¨è´Ÿè½½å‡è¡¡")
            elif "CPUä½¿ç”¨ç‡è¿‡é«˜" in bottleneck:
                recommendations.append("ä¼˜åŒ–CPUå¯†é›†å‹æ“ä½œï¼Œè€ƒè™‘å‚ç›´æ‰©å±•æˆ–åˆ†å¸ƒå¼å¤„ç†")
            elif "å†…å­˜ä½¿ç”¨è¿‡é«˜" in bottleneck:
                recommendations.append("ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œæ£€æŸ¥å†…å­˜æ³„æ¼ï¼Œè€ƒè™‘å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–ç®—æ³•")
            elif "ç¼“å­˜å‘½ä¸­ç‡ä½" in bottleneck:
                recommendations.append("ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ï¼Œå¢åŠ ç¼“å­˜é¢„çƒ­ï¼Œè°ƒæ•´ç¼“å­˜TTL")
            elif "æ•°æ®åº“æŸ¥è¯¢æ—¶é—´è¿‡é•¿" in bottleneck:
                recommendations.append("ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•ï¼Œé‡æ„æŸ¥è¯¢è¯­å¥ï¼Œè€ƒè™‘æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–")

        # æ·»åŠ é€šç”¨å»ºè®®
        if not recommendations:
            recommendations.append("ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œå»ºè®®å®šæœŸç›‘æ§å¹¶æŒç»­ä¼˜åŒ–")
        else:
            recommendations.extend([
                "å®æ–½æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶",
                "å»ºç«‹æ€§èƒ½åŸºå‡†çº¿å¹¶å®šæœŸå›å½’æµ‹è¯•",
                "è€ƒè™‘ä½¿ç”¨APMå·¥å…·è¿›è¡Œæ·±åº¦æ€§èƒ½åˆ†æ"
            ])

        return list(set(recommendations))  # å»é‡

    async def _generate_performance_charts(self):
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨"""
        logger.info("ç”Ÿæˆæ€§èƒ½å›¾è¡¨...")

        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            charts_dir = Path("performance_charts")
            charts_dir.mkdir(exist_ok=True)

            # 1. å“åº”æ—¶é—´å¯¹æ¯”å›¾
            self._create_response_time_chart(charts_dir)

            # 2. ååé‡å¯¹æ¯”å›¾
            self._create_throughput_chart(charts_dir)

            # 3. ç³»ç»Ÿèµ„æºä½¿ç”¨å›¾
            self._create_resource_usage_chart(charts_dir)

            # 4. é”™è¯¯ç‡å›¾
            self._create_error_rate_chart(charts_dir)

            # 5. ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
            self._create_performance_radar_chart(charts_dir)

            logger.info(f"å›¾è¡¨å·²ç”Ÿæˆåˆ° {charts_dir} ç›®å½•")

        except Exception as e:
            logger.error(f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")

    def _create_response_time_chart(self, charts_dir: Path):
        """åˆ›å»ºå“åº”æ—¶é—´å¯¹æ¯”å›¾"""
        try:
            test_names = [r.test_name[:20] + "..." if len(r.test_name) > 20 else r.test_name
                         for r in self.test_results]
            avg_times = [r.avg_response_time_ms for r in self.test_results]
            p95_times = [r.p95_response_time_ms for r in self.test_results]
            p99_times = [r.p99_response_time_ms for r in self.test_results]

            plt.figure(figsize=(14, 8))
            x = range(len(test_names))
            width = 0.25

            plt.bar([i - width for i in x], avg_times, width, label='Average', alpha=0.8, color='skyblue')
            plt.bar(x, p95_times, width, label='P95', alpha=0.8, color='orange')
            plt.bar([i + width for i in x], p99_times, width, label='P99', alpha=0.8, color='lightcoral')

            plt.xlabel('Test Cases')
            plt.ylabel('Response Time (ms)')
            plt.title('Response Time Comparison Across Test Cases')
            plt.xticks(x, test_names, rotation=45, ha='right')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()

            plt.savefig(charts_dir / "response_time_comparison.png", dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            logger.error(f"åˆ›å»ºå“åº”æ—¶é—´å›¾è¡¨å¤±è´¥: {e}")

    def _create_throughput_chart(self, charts_dir: Path):
        """åˆ›å»ºååé‡å¯¹æ¯”å›¾"""
        try:
            test_names = [r.test_name[:20] + "..." if len(r.test_name) > 20 else r.test_name
                         for r in self.test_results]
            throughputs = [r.requests_per_second for r in self.test_results]

            plt.figure(figsize=(12, 6))
            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', '#FF9FF3', '#54A0FF']
            bars = plt.bar(test_names, throughputs, alpha=0.8, color=colors[:len(test_names)])

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, throughputs):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(throughputs)*0.01,
                        f'{value:.1f}', ha='center', va='bottom', fontweight='bold')

            plt.xlabel('Test Cases')
            plt.ylabel('Requests per Second (RPS)')
            plt.title('Throughput Comparison Across Test Cases')
            plt.xticks(rotation=45, ha='right')
            plt.grid(True, alpha=0.3, axis='y')
            plt.tight_layout()

            plt.savefig(charts_dir / "throughput_comparison.png", dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            logger.error(f"åˆ›å»ºååé‡å›¾è¡¨å¤±è´¥: {e}")

    def _create_resource_usage_chart(self, charts_dir: Path):
        """åˆ›å»ºç³»ç»Ÿèµ„æºä½¿ç”¨å›¾"""
        try:
            test_names = [r.test_name[:15] + "..." if len(r.test_name) > 15 else r.test_name
                         for r in self.test_results]
            cpu_usage = [r.cpu_usage_percent for r in self.test_results]
            memory_usage = [r.memory_usage_mb for r in self.test_results]

            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

            # CPUä½¿ç”¨ç‡
            bars1 = ax1.bar(test_names, cpu_usage, alpha=0.7, color='orange', label='CPU Usage')
            ax1.set_ylabel('CPU Usage (%)')
            ax1.set_title('CPU Usage by Test Case')
            ax1.set_ylim(0, 100)
            ax1.grid(True, alpha=0.3)

            # æ·»åŠ CPUä½¿ç”¨ç‡æ ‡ç­¾
            for bar, value in zip(bars1, cpu_usage):
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                        f'{value:.1f}%', ha='center', va='bottom')

            # å†…å­˜ä½¿ç”¨é‡
            bars2 = ax2.bar(test_names, memory_usage, alpha=0.7, color='green', label='Memory Usage')
            ax2.set_ylabel('Memory Usage (MB)')
            ax2.set_title('Memory Usage by Test Case')
            ax2.grid(True, alpha=0.3)

            # æ·»åŠ å†…å­˜ä½¿ç”¨é‡æ ‡ç­¾
            for bar, value in zip(bars2, memory_usage):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(memory_usage)*0.01,
                        f'{value:.0f}MB', ha='center', va='bottom')

            # è®¾ç½®xè½´æ ‡ç­¾
            for ax in [ax1, ax2]:
                ax.set_xticklabels(test_names, rotation=45, ha='right')

            plt.tight_layout()
            plt.savefig(charts_dir / "resource_usage.png", dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            logger.error(f"åˆ›å»ºèµ„æºä½¿ç”¨å›¾è¡¨å¤±è´¥: {e}")

    def _create_error_rate_chart(self, charts_dir: Path):
        """åˆ›å»ºé”™è¯¯ç‡å›¾"""
        try:
            test_names = [r.test_name[:20] + "..." if len(r.test_name) > 20 else r.test_name
                         for r in self.test_results]
            error_rates = [r.error_rate_percent for r in self.test_results]

            plt.figure(figsize=(12, 6))
            colors = ['red' if rate > 1 else 'yellow' if rate > 0.1 else 'green' for rate in error_rates]
            bars = plt.bar(test_names, error_rates, alpha=0.7, color=colors)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, error_rates):
                if value > 0:
                    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(error_rates)*0.01,
                            f'{value:.2f}%', ha='center', va='bottom', fontweight='bold')

            plt.xlabel('Test Cases')
            plt.ylabel('Error Rate (%)')
            plt.title('Error Rate by Test Case')
            plt.xticks(rotation=45, ha='right')
            plt.grid(True, alpha=0.3, axis='y')

            # æ·»åŠ æ€§èƒ½é˜ˆå€¼çº¿
            plt.axhline(y=1, color='orange', linestyle='--', alpha=0.7, label='Warning Threshold (1%)')
            plt.axhline(y=5, color='red', linestyle='--', alpha=0.7, label='Critical Threshold (5%)')
            plt.legend()

            plt.tight_layout()
            plt.savefig(charts_dir / "error_rate.png", dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            logger.error(f"åˆ›å»ºé”™è¯¯ç‡å›¾è¡¨å¤±è´¥: {e}")

    def _create_performance_radar_chart(self, charts_dir: Path):
        """åˆ›å»ºç»¼åˆæ€§èƒ½é›·è¾¾å›¾"""
        try:
            import numpy as np

            # é€‰æ‹©æœ€é‡è¦çš„å‡ ä¸ªæµ‹è¯•ç”¨ä¾‹
            important_tests = [r for r in self.test_results if 'Load Test' in r.test_name or 'Stress Test' in r.test_name]

            if not important_tests:
                important_tests = self.test_results[:3]  # å–å‰3ä¸ª

            categories = ['Throughput', 'Response Time', 'CPU Usage', 'Memory Usage', 'Error Rate']

            fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']

            for i, result in enumerate(important_tests[:3]):
                # æ ‡å‡†åŒ–æŒ‡æ ‡ (0-100 scale)
                throughput_score = min(100, result.requests_per_second / 10 * 100)  # å‡è®¾1000 RPSä¸ºæ»¡åˆ†
                response_time_score = max(0, 100 - result.p95_response_time_ms / 10)  # å“åº”æ—¶é—´è¶Šä½åˆ†æ•°è¶Šé«˜
                cpu_score = max(0, 100 - result.cpu_usage_percent)  # CPUä½¿ç”¨ç‡è¶Šä½åˆ†æ•°è¶Šé«˜
                memory_score = max(0, 100 - result.memory_usage_mb / 20)  # å†…å­˜ä½¿ç”¨è¶Šä½åˆ†æ•°è¶Šé«˜
                error_score = max(0, 100 - result.error_rate_percent * 20)  # é”™è¯¯ç‡è¶Šä½åˆ†æ•°è¶Šé«˜

                values = [throughput_score, response_time_score, cpu_score, memory_score, error_score]
                values += values[:1]  # é—­åˆå¤šè¾¹å½¢

                angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
                angles += angles[:1]

                ax.plot(angles, values, 'o-', linewidth=2, label=result.test_name[:20], color=colors[i])
                ax.fill(angles, values, alpha=0.25, color=colors[i])

            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(categories)
            ax.set_ylim(0, 100)
            ax.set_title('Performance Radar Chart', size=16, fontweight='bold', pad=20)
            ax.grid(True)
            ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))

            plt.tight_layout()
            plt.savefig(charts_dir / "performance_radar.png", dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            logger.error(f"åˆ›å»ºé›·è¾¾å›¾å¤±è´¥: {e}")

    async def save_report_to_file(self, report: PerformanceReport, filename: str = None):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if filename is None:
            filename = f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        report_data = {
            "timestamp": report.timestamp.isoformat(),
            "overall_score": report.overall_score,
            "system_info": report.system_info,
            "bottlenecks": report.bottlenecks,
            "recommendations": report.recommendations,
            "charts_generated": report.charts_generated,
            "test_results": []
        }

        for result in report.test_results:
            result_data = asdict(result)
            result_data["start_time"] = result.start_time.isoformat()
            result_data["end_time"] = result.end_time.isoformat()
            report_data["test_results"].append(result_data)

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        logger.info(f"æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
        return filename

def print_console_summary(report: PerformanceReport):
    """æ‰“å°æ§åˆ¶å°æ‘˜è¦"""
    # print("\n" + "="*80)
    # print("ğŸ¯ PERFECT21 æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
    # print("="*80)

    # æ•´ä½“è¯„åˆ†
    if report.overall_score >= 80:
        score_icon = "ğŸŸ¢"
        score_desc = "ä¼˜ç§€"
    elif report.overall_score >= 60:
        score_icon = "ğŸŸ¡"
        score_desc = "è‰¯å¥½"
    else:
        score_icon = "ğŸ”´"
        score_desc = "éœ€æ”¹è¿›"

    # print(f"{score_icon} æ•´ä½“æ€§èƒ½è¯„åˆ†: {report.overall_score:.1f}/100 ({score_desc})")

    # æµ‹è¯•ç»“æœæ¦‚è§ˆ
    # print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ¦‚è§ˆ:")
    # print(f"  æµ‹è¯•æ—¶é—´: {report.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
    # print(f"  æµ‹è¯•é¡¹ç›®: {len(report.test_results)}ä¸ª")
    # print(f"  ç³»ç»Ÿé…ç½®: {report.system_info['cpu_count']}æ ¸CPU, {report.system_info['total_memory_gb']:.1f}GBå†…å­˜")

    # è¯¦ç»†æµ‹è¯•ç»“æœè¡¨æ ¼
    # print(f"\nğŸ“ˆ è¯¦ç»†æµ‹è¯•ç»“æœ:")
    # print("-" * 80)
    # print(f"{'æµ‹è¯•é¡¹ç›®':<25} {'RPS':<8} {'å“åº”æ—¶é—´':<12} {'P95':<10} {'é”™è¯¯ç‡':<8} {'CPU':<6} {'å†…å­˜':<8}")
    # print("-" * 80)

    for result in report.test_results:
        test_name = result.test_name[:24]
        rps = f"{result.requests_per_second:.1f}"
        avg_time = f"{result.avg_response_time_ms:.1f}ms"
        p95_time = f"{result.p95_response_time_ms:.1f}ms"
        error_rate = f"{result.error_rate_percent:.2f}%"
        cpu = f"{result.cpu_usage_percent:.1f}%"
        memory = f"{result.memory_usage_mb:.0f}MB"

    # print(f"{test_name:<25} {rps:<8} {avg_time:<12} {p95_time:<10} {error_rate:<8} {cpu:<6} {memory:<8}")

    # ç‰¹æ®ŠæŒ‡æ ‡
    cache_tests = [r for r in report.test_results if r.cache_hit_rate_percent > 0]
    if cache_tests:
    # print(f"\nğŸ—‚ï¸ ç¼“å­˜æ€§èƒ½:")
        for result in cache_tests:
    # print(f"  {result.test_name}: å‘½ä¸­ç‡ {result.cache_hit_rate_percent:.1f}%")

    db_tests = [r for r in report.test_results if r.database_query_time_ms > 0]
    if db_tests:
    # print(f"\nğŸ—„ï¸ æ•°æ®åº“æ€§èƒ½:")
        for result in db_tests:
    # print(f"  {result.test_name}: å¹³å‡æŸ¥è¯¢æ—¶é—´ {result.database_query_time_ms:.1f}ms")

    # æ€§èƒ½ç“¶é¢ˆ
    if report.bottlenecks:
    # print(f"\nâš ï¸ å‘ç°çš„æ€§èƒ½ç“¶é¢ˆ ({len(report.bottlenecks)}ä¸ª):")
        for i, bottleneck in enumerate(report.bottlenecks, 1):
    # print(f"  {i}. {bottleneck}")

    # ä¼˜åŒ–å»ºè®®
    if report.recommendations:
    # print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®® ({len(report.recommendations)}ä¸ª):")
        for i, recommendation in enumerate(report.recommendations, 1):
    # print(f"  {i}. {recommendation}")

    # å›¾è¡¨ä¿¡æ¯
    charts_dir = Path("performance_charts")
    if charts_dir.exists():
        charts = list(charts_dir.glob("*.png"))
        if charts:
    # print(f"\nğŸ“ˆ ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶ ({len(charts)}ä¸ª):")
            for chart in charts:
    # print(f"  ğŸ“Š {chart.name}")

    # print("\n" + "="*80)

async def main():
    """ä¸»å‡½æ•°"""
    # print("ğŸš€ Claude Enhancer ç®€åŒ–æ€§èƒ½æµ‹è¯•")
    # print("=" * 50)

    try:
        # æ£€æŸ¥matplotlibåç«¯
        import matplotlib
        matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

        # åˆ›å»ºæµ‹è¯•å™¨
        tester = SimplePerformanceTester()

    # print("\nğŸ”„ å‡†å¤‡å¼€å§‹æ€§èƒ½æµ‹è¯•...")
    # print("æµ‹è¯•å°†åŒ…æ‹¬:")
    # print("  âš¡ è´Ÿè½½æµ‹è¯• (1000å¹¶å‘ç”¨æˆ·)")
    # print("  â±ï¸ å“åº”æ—¶é—´æµ‹è¯•")
    # print("  ğŸ’¾ å†…å­˜ä½¿ç”¨æµ‹è¯•")
    # print("  ğŸ—„ï¸ æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–æµ‹è¯•")
    # print("  ğŸ—‚ï¸ ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•")
    # print("  ğŸ’¥ å‹åŠ›æµ‹è¯• (æé™è´Ÿè½½)")

        # è¿è¡Œæµ‹è¯•
        report = await tester.run_comprehensive_tests()

        # ä¿å­˜æŠ¥å‘Š
        report_filename = await tester.save_report_to_file(report)

        # æ‰“å°æ‘˜è¦
        print_console_summary(report)

    # print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_filename}")
    # print("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼")

        return 0

    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)