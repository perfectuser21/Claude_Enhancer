#!/usr/bin/env python3
"""
Claude Enhancer å•å…ƒæµ‹è¯•å¥—ä»¶
é’ˆå¯¹å„ä¸ªç»„ä»¶çš„ç‹¬ç«‹åŠŸèƒ½æµ‹è¯•
"""

import unittest
import json
import yaml
import os
import sys
import tempfile
import shutil
from pathlib import Path
from unittest.mock import patch, mock_open, MagicMock
import subprocess


class TestConfigurationParsing(unittest.TestCase):
    """é…ç½®æ–‡ä»¶è§£ææµ‹è¯•"""

    def setUp(self):
        self.test_dir = Path(tempfile.mkdtemp())
        self.claude_dir = self.test_dir / ".claude"
        self.claude_dir.mkdir(parents=True)

    def tearDown(self):
        shutil.rmtree(self.test_dir, ignore_errors=True)

    def test_valid_json_config(self):
        """æµ‹è¯•æœ‰æ•ˆçš„JSONé…ç½®è§£æ"""
        config_data = {
            "version": "4.0.0",
            "project": "Test Project",
            "hooks": {
                "PreToolUse": [
                    {
                        "matcher": "Task",
                        "type": "command",
                        "command": "echo test",
                        "blocking": False,
                    }
                ]
            },
        }

        config_file = self.claude_dir / "settings.json"
        with open(config_file, "w") as f:
            json.dump(config_data, f)

        # æµ‹è¯•è§£æ
        with open(config_file, "r") as f:
            parsed_config = json.load(f)

        self.assertEqual(parsed_config["version"], "4.0.0")
        self.assertEqual(parsed_config["project"], "Test Project")
        self.assertIn("hooks", parsed_config)
        self.assertIn("PreToolUse", parsed_config["hooks"])

    def test_invalid_json_config(self):
        """æµ‹è¯•æ— æ•ˆçš„JSONé…ç½®å¤„ç†"""
        config_file = self.claude_dir / "settings.json"
        with open(config_file, "w") as f:
            f.write('{"invalid": json syntax}')  # æ•…æ„å†™é”™è¯¯çš„JSON

        with self.assertRaises(json.JSONDecodeError):
            with open(config_file, "r") as f:
                json.load(f)

    def test_yaml_config_parsing(self):
        """æµ‹è¯•YAMLé…ç½®è§£æ"""
        config_data = {
            "system": {"mode": "production", "debug": False},
            "agents": {"minimum_count": 3, "maximum_count": 8},
        }

        config_file = self.claude_dir / "config.yaml"
        with open(config_file, "w") as f:
            yaml.dump(config_data, f)

        # æµ‹è¯•è§£æ
        with open(config_file, "r") as f:
            parsed_config = yaml.safe_load(f)

        self.assertEqual(parsed_config["system"]["mode"], "production")
        self.assertEqual(parsed_config["agents"]["minimum_count"], 3)

    def test_missing_config_file(self):
        """æµ‹è¯•ç¼ºå¤±é…ç½®æ–‡ä»¶çš„å¤„ç†"""
        config_file = self.claude_dir / "nonexistent.json"

        with self.assertRaises(FileNotFoundError):
            with open(config_file, "r") as f:
                json.load(f)


class TestHookValidation(unittest.TestCase):
    """HookéªŒè¯æµ‹è¯•"""

    def setUp(self):
        self.test_dir = Path(tempfile.mkdtemp())
        self.hooks_dir = self.test_dir / ".claude" / "hooks"
        self.hooks_dir.mkdir(parents=True)

    def tearDown(self):
        shutil.rmtree(self.test_dir, ignore_errors=True)

    def test_hook_file_permissions(self):
        """æµ‹è¯•Hookæ–‡ä»¶æƒé™"""
        hook_file = self.hooks_dir / "test_hook.sh"
        with open(hook_file, "w") as f:
            f.write("#!/bin/bash\necho 'test hook'\n")

        # åˆå§‹æƒé™å¯èƒ½ä¸å¯æ‰§è¡Œ
        self.assertTrue(hook_file.exists())

        # è®¾ç½®å¯æ‰§è¡Œæƒé™
        os.chmod(hook_file, 0o755)
        self.assertTrue(os.access(hook_file, os.X_OK))

    def test_hook_script_syntax(self):
        """æµ‹è¯•Hookè„šæœ¬è¯­æ³•"""
        # æœ‰æ•ˆçš„bashè„šæœ¬
        valid_hook = self.hooks_dir / "valid_hook.sh"
        with open(valid_hook, "w") as f:
            f.write(
                """#!/bin/bash
set -e
echo "Valid hook script"
exit 0
"""
            )
        os.chmod(valid_hook, 0o755)

        # æµ‹è¯•è¯­æ³•æ£€æŸ¥
        result = subprocess.run(
            ["bash", "-n", str(valid_hook)], capture_output=True, text=True
        )
        self.assertEqual(result.returncode, 0, "Valid hook should pass syntax check")

        # æ— æ•ˆçš„bashè„šæœ¬
        invalid_hook = self.hooks_dir / "invalid_hook.sh"
        with open(invalid_hook, "w") as f:
            f.write(
                """#!/bin/bash
if [ missing bracket
echo "Invalid syntax"
"""
            )
        os.chmod(invalid_hook, 0o755)

        result = subprocess.run(
            ["bash", "-n", str(invalid_hook)], capture_output=True, text=True
        )
        self.assertNotEqual(
            result.returncode, 0, "Invalid hook should fail syntax check"
        )

    def test_hook_timeout_handling(self):
        """æµ‹è¯•Hookè¶…æ—¶å¤„ç†"""
        timeout_hook = self.hooks_dir / "timeout_hook.sh"
        with open(timeout_hook, "w") as f:
            f.write(
                """#!/bin/bash
# æ¨¡æ‹Ÿé•¿æ—¶é—´è¿è¡Œçš„Hook
sleep 10
echo "This should timeout"
"""
            )
        os.chmod(timeout_hook, 0o755)

        # æµ‹è¯•è¶…æ—¶
        try:
            result = subprocess.run(
                ["bash", str(timeout_hook)],
                timeout=2,  # 2ç§’è¶…æ—¶
                capture_output=True,
                text=True,
            )
            self.fail("Hook should have timed out")
        except subprocess.TimeoutExpired:
            pass  # æœŸæœ›çš„è¶…æ—¶


class TestPerformanceMetrics(unittest.TestCase):
    """æ€§èƒ½æŒ‡æ ‡æµ‹è¯•"""

    def test_execution_time_measurement(self):
        """æµ‹è¯•æ‰§è¡Œæ—¶é—´æµ‹é‡å‡†ç¡®æ€§"""
        import time

        start_time = time.perf_counter()
        time.sleep(0.1)  # ç¡çœ 100æ¯«ç§’
        end_time = time.perf_counter()

        execution_time = end_time - start_time
        # å…è®¸10%çš„è¯¯å·®
        self.assertGreaterEqual(execution_time, 0.09)
        self.assertLessEqual(execution_time, 0.11)

    def test_memory_usage_tracking(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨è·Ÿè¸ª"""
        try:
            import psutil

            # è·å–å½“å‰å†…å­˜ä½¿ç”¨
            process = psutil.Process()
            memory_before = process.memory_info().rss

            # åˆ†é…ä¸€äº›å†…å­˜
            large_data = [0] * 100000
            memory_after = process.memory_info().rss

            # å†…å­˜ä½¿ç”¨åº”è¯¥å¢åŠ 
            self.assertGreater(memory_after, memory_before)

            # æ¸…ç†
            del large_data

        except ImportError:
            self.skipTest("psutil not available")

    def test_cpu_usage_monitoring(self):
        """æµ‹è¯•CPUä½¿ç”¨ç›‘æ§"""
        try:
            import psutil
            import time

            # ç›‘æ§CPUä½¿ç”¨
            cpu_percent = psutil.cpu_percent(interval=0.1)
            self.assertGreaterEqual(cpu_percent, 0.0)
            self.assertLessEqual(cpu_percent, 100.0)

        except ImportError:
            self.skipTest("psutil not available")


class TestErrorHandling(unittest.TestCase):
    """é”™è¯¯å¤„ç†æµ‹è¯•"""

    def test_command_execution_error(self):
        """æµ‹è¯•å‘½ä»¤æ‰§è¡Œé”™è¯¯å¤„ç†"""
        # æ‰§è¡Œä¸å­˜åœ¨çš„å‘½ä»¤ï¼Œæ•è·å¼‚å¸¸
        with self.assertRaises(FileNotFoundError):
            subprocess.run(
                ["nonexistent_command_12345"],
                capture_output=True,
                text=True,
                check=True,
            )

    def test_file_not_found_error(self):
        """æµ‹è¯•æ–‡ä»¶æœªæ‰¾åˆ°é”™è¯¯å¤„ç†"""
        with self.assertRaises(FileNotFoundError):
            with open("/nonexistent/path/file.txt", "r") as f:
                f.read()

    def test_permission_error(self):
        """æµ‹è¯•æƒé™é”™è¯¯å¤„ç†"""
        # ç®€åŒ–æµ‹è¯•ï¼Œåªæ£€æŸ¥æ–‡ä»¶æƒé™çš„æ”¹å˜
        with tempfile.TemporaryDirectory() as temp_dir:
            test_file = Path(temp_dir) / "test_file.txt"
            test_file.write_text("test content")

            # æ£€æŸ¥åˆå§‹å¯è¯»æ€§
            self.assertTrue(os.access(test_file, os.R_OK))

            # ç§»é™¤è¯»æƒé™
            os.chmod(test_file, 0o000)

            # æ£€æŸ¥æ–‡ä»¶ç°åœ¨ä¸å¯è¯»
            self.assertFalse(os.access(test_file, os.R_OK))

            # æ¢å¤æƒé™ä»¥ä¾¿æ¸…ç†
            os.chmod(test_file, 0o644)


class TestSystemIntegration(unittest.TestCase):
    """ç³»ç»Ÿé›†æˆæµ‹è¯•"""

    def setUp(self):
        self.test_dir = Path(tempfile.mkdtemp())
        self.claude_dir = self.test_dir / ".claude"
        self.claude_dir.mkdir(parents=True)

    def tearDown(self):
        shutil.rmtree(self.test_dir, ignore_errors=True)

    def test_environment_variables(self):
        """æµ‹è¯•ç¯å¢ƒå˜é‡è®¾ç½®"""
        test_env = os.environ.copy()
        test_env.update(
            {
                "CLAUDE_TEST_MODE": "1",
                "HOOK_TEST_MODE": "1",
                "CLAUDE_DIR": str(self.claude_dir),
            }
        )

        # è¿è¡Œç®€å•å‘½ä»¤éªŒè¯ç¯å¢ƒå˜é‡
        result = subprocess.run(
            ["bash", "-c", "echo $CLAUDE_TEST_MODE"],
            env=test_env,
            capture_output=True,
            text=True,
        )

        self.assertEqual(result.returncode, 0)
        self.assertEqual(result.stdout.strip(), "1")

    def test_working_directory_change(self):
        """æµ‹è¯•å·¥ä½œç›®å½•å˜æ›´"""
        # åˆ›å»ºæµ‹è¯•ç›®å½•
        test_subdir = self.test_dir / "subdir"
        test_subdir.mkdir()

        # åœ¨å­ç›®å½•ä¸­è¿è¡Œå‘½ä»¤
        result = subprocess.run(
            ["pwd"], cwd=test_subdir, capture_output=True, text=True
        )

        self.assertEqual(result.returncode, 0)
        self.assertEqual(Path(result.stdout.strip()), test_subdir)

    def test_hook_system_integration(self):
        """æµ‹è¯•Hookç³»ç»Ÿé›†æˆ"""
        # åˆ›å»ºç®€å•çš„Hooké…ç½®
        settings = {
            "hooks": {
                "PreToolUse": [
                    {
                        "matcher": ".*",
                        "type": "command",
                        "command": "echo 'PreToolUse hook executed'",
                        "blocking": False,
                        "timeout": 1000,
                    }
                ]
            }
        }

        settings_file = self.claude_dir / "settings.json"
        with open(settings_file, "w") as f:
            json.dump(settings, f)

        # éªŒè¯é…ç½®å¯ä»¥æ­£ç¡®è§£æ
        with open(settings_file, "r") as f:
            loaded_settings = json.load(f)

        self.assertIn("hooks", loaded_settings)
        self.assertIn("PreToolUse", loaded_settings["hooks"])
        self.assertEqual(len(loaded_settings["hooks"]["PreToolUse"]), 1)

        hook_config = loaded_settings["hooks"]["PreToolUse"][0]
        self.assertEqual(hook_config["command"], "echo 'PreToolUse hook executed'")
        self.assertFalse(hook_config["blocking"])


class TestDataValidation(unittest.TestCase):
    """æ•°æ®éªŒè¯æµ‹è¯•"""

    def test_json_schema_validation(self):
        """æµ‹è¯•JSONæ¨¡å¼éªŒè¯"""
        # æœ‰æ•ˆçš„Hooké…ç½®
        valid_hook_config = {
            "matcher": "Task",
            "type": "command",
            "command": "echo test",
            "timeout": 1000,
            "blocking": False,
        }

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ["matcher", "type", "command"]
        for field in required_fields:
            self.assertIn(field, valid_hook_config)

        # æ£€æŸ¥å­—æ®µç±»å‹
        self.assertIsInstance(valid_hook_config["matcher"], str)
        self.assertIsInstance(valid_hook_config["type"], str)
        self.assertIsInstance(valid_hook_config["command"], str)
        self.assertIsInstance(valid_hook_config["timeout"], int)
        self.assertIsInstance(valid_hook_config["blocking"], bool)

    def test_timeout_value_validation(self):
        """æµ‹è¯•è¶…æ—¶å€¼éªŒè¯"""
        valid_timeouts = [100, 1000, 5000, 30000]
        invalid_timeouts = [-1, 0, "1000", None, 3.14]

        for timeout in valid_timeouts:
            self.assertIsInstance(timeout, int)
            self.assertGreater(timeout, 0)

        for timeout in invalid_timeouts:
            if timeout is None:
                continue
            if isinstance(timeout, str):
                # å­—ç¬¦ä¸²è¶…æ—¶å€¼åº”è¯¥è¢«æ£€æµ‹ä¸ºæ— æ•ˆ
                self.assertIsInstance(timeout, str)
            elif isinstance(timeout, (int, float)) and timeout <= 0:
                self.assertLessEqual(timeout, 0)

    def test_command_string_validation(self):
        """æµ‹è¯•å‘½ä»¤å­—ç¬¦ä¸²éªŒè¯"""
        valid_commands = [
            "echo test",
            "bash /path/to/script.sh",
            "python3 script.py --arg value",
        ]

        invalid_commands = [
            "",  # ç©ºå‘½ä»¤
            None,  # Noneå€¼
            123,  # éå­—ç¬¦ä¸²
            "command; rm -rf /",  # å±é™©å‘½ä»¤
        ]

        for cmd in valid_commands:
            self.assertIsInstance(cmd, str)
            self.assertGreater(len(cmd.strip()), 0)

        for cmd in invalid_commands:
            if cmd is None:
                self.assertIsNone(cmd)
            elif isinstance(cmd, int):
                self.assertIsInstance(cmd, int)
            elif isinstance(cmd, str) and len(cmd.strip()) == 0:
                self.assertEqual(len(cmd.strip()), 0)


def create_test_suite():
    """åˆ›å»ºæµ‹è¯•å¥—ä»¶"""
    suite = unittest.TestSuite()

    # æ·»åŠ æ‰€æœ‰æµ‹è¯•ç±»
    test_classes = [
        TestConfigurationParsing,
        TestHookValidation,
        TestPerformanceMetrics,
        TestErrorHandling,
        TestSystemIntegration,
        TestDataValidation,
    ]

    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        suite.addTests(tests)

    return suite


def run_tests():
    """è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•"""
    print("ğŸ§ª è¿è¡ŒClaude Enhancerå•å…ƒæµ‹è¯•å¥—ä»¶...")
    print("=" * 60)

    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    suite = create_test_suite()

    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(
        verbosity=2, stream=sys.stdout, descriptions=True, failfast=False
    )

    result = runner.run(suite)

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    print(f"è¿è¡Œæµ‹è¯•æ•°é‡: {result.testsRun}")
    print(f"æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"å¤±è´¥: {len(result.failures)}")
    print(f"é”™è¯¯: {len(result.errors)}")
    print(f"è·³è¿‡: {len(result.skipped) if hasattr(result, 'skipped') else 0}")

    if result.failures:
        print("\nâŒ å¤±è´¥çš„æµ‹è¯•:")
        for test, traceback in result.failures:
            print(
                f"  - {test}: {traceback.split('AssertionError: ')[-1].split(chr(10))[0]}"
            )

    if result.errors:
        print("\nâš ï¸ é”™è¯¯çš„æµ‹è¯•:")
        for test, traceback in result.errors:
            print(f"  - {test}: {traceback.split(chr(10))[-2]}")

    success_rate = (
        result.testsRun - len(result.failures) - len(result.errors)
    ) / result.testsRun
    print(f"\nâœ… æ€»ä½“æˆåŠŸç‡: {success_rate:.1%}")

    return result.wasSuccessful()


if __name__ == "__main__":
    success = run_tests()
    sys.exit(0 if success else 1)
