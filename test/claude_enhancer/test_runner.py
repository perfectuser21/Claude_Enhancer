#!/usr/bin/env python3
"""
Claude Enhancer æµ‹è¯•è¿è¡Œå™¨
ç»Ÿä¸€è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶çš„ä¸»æ§åˆ¶å™¨
"""

import sys
import time
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import logging
import subprocess

# å¯¼å…¥æµ‹è¯•å¥—ä»¶
try:
    from test_framework import ComprehensiveTestSuite
    from unit_tests import run_tests as run_unit_tests
    from benchmark_suite import ComprehensiveBenchmarkSuite
    from stress_test_suite import ComprehensiveStressTestSuite
except ImportError as e:
    print(f"âŒ å¯¼å…¥æµ‹è¯•æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰æµ‹è¯•æ–‡ä»¶éƒ½åœ¨åŒä¸€ç›®å½•ä¸‹")
    sys.exit(1)


class TestOrchestrator:
    """æµ‹è¯•ç¼–æ’å™¨ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æµ‹è¯•å¥—ä»¶"""

    def __init__(self, claude_dir: str = "/home/xx/dev/Claude_Enhancer/.claude"):
        self.claude_dir = Path(claude_dir)
        self.project_dir = self.claude_dir.parent
        self.test_dir = self.project_dir / "test" / "claude_enhancer"
        self.setup_logging()

        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "test_environment": self.get_test_environment(),
            "test_suites": {},
            "summary": {},
            "recommendations": [],
            "performance_trends": {},
        }

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = self.test_dir / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)

        log_file = (
            log_dir
            / f"test_orchestrator_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        )

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[logging.FileHandler(log_file), logging.StreamHandler(sys.stdout)],
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"æµ‹è¯•æ—¥å¿—ä¿å­˜åˆ°: {log_file}")

    def get_test_environment(self) -> Dict[str, Any]:
        """è·å–æµ‹è¯•ç¯å¢ƒä¿¡æ¯"""
        try:
            import psutil

            env_info = {
                "python_version": sys.version,
                "platform": sys.platform,
                "cpu_count": psutil.cpu_count(),
                "cpu_freq": psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None,
                "memory_total_gb": psutil.virtual_memory().total / (1024**3),
                "memory_available_gb": psutil.virtual_memory().available / (1024**3),
                "disk_total_gb": psutil.disk_usage("/").total / (1024**3),
                "disk_free_gb": psutil.disk_usage("/").free / (1024**3),
                "load_average": getattr(__import__("os"), "getloadavg", lambda: None)(),
                "claude_enhancer_path": str(self.claude_dir),
                "test_time": datetime.now().isoformat(),
            }

            # æ£€æŸ¥å…³é”®ä¾èµ–
            dependencies = [
                "psutil",
                "json",
                "subprocess",
                "threading",
                "concurrent.futures",
            ]
            env_info["dependencies_available"] = {}

            for dep in dependencies:
                try:
                    __import__(dep)
                    env_info["dependencies_available"][dep] = True
                except ImportError:
                    env_info["dependencies_available"][dep] = False

            return env_info

        except Exception as e:
            self.logger.error(f"è·å–ç¯å¢ƒä¿¡æ¯å¤±è´¥: {e}")
            return {"error": str(e), "timestamp": datetime.now().isoformat()}

    def run_unit_tests_suite(self) -> Dict[str, Any]:
        """è¿è¡Œå•å…ƒæµ‹è¯•å¥—ä»¶"""
        self.logger.info("ğŸ§ª å¼€å§‹è¿è¡Œå•å…ƒæµ‹è¯•å¥—ä»¶...")
        start_time = time.perf_counter()

        try:
            pass  # Auto-fixed empty block
            # è¿è¡Œå•å…ƒæµ‹è¯•
            success = run_unit_tests()
            duration = time.perf_counter() - start_time

            result = {
                "suite_name": "unit_tests",
                "success": success,
                "duration": duration,
                "timestamp": datetime.now().isoformat(),
                "details": "è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹å•å…ƒæµ‹è¯•æ—¥å¿—",
            }

            self.logger.info(f"âœ… å•å…ƒæµ‹è¯•å®Œæˆ - æˆåŠŸ: {success}, è€—æ—¶: {duration:.2f}s")
            return result

        except Exception as e:
            duration = time.perf_counter() - start_time
            self.logger.error(f"âŒ å•å…ƒæµ‹è¯•å¤±è´¥: {e}")
            return {
                "suite_name": "unit_tests",
                "success": False,
                "duration": duration,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def run_comprehensive_tests_suite(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæµ‹è¯•å¥—ä»¶"""
        self.logger.info("ğŸ”¬ å¼€å§‹è¿è¡Œç»¼åˆæµ‹è¯•å¥—ä»¶...")
        start_time = time.perf_counter()

        try:
            suite = ComprehensiveTestSuite(str(self.claude_dir))
            results = suite.run_full_test_suite()
            report_path = suite.save_detailed_report()

            duration = time.perf_counter() - start_time

            result = {
                "suite_name": "comprehensive_tests",
                "success": True,
                "duration": duration,
                "results": results,
                "report_path": str(report_path),
                "timestamp": datetime.now().isoformat(),
            }

            self.logger.info(f"âœ… ç»¼åˆæµ‹è¯•å®Œæˆ - è€—æ—¶: {duration:.2f}s")
            return result

        except Exception as e:
            duration = time.perf_counter() - start_time
            self.logger.error(f"âŒ ç»¼åˆæµ‹è¯•å¤±è´¥: {e}")
            return {
                "suite_name": "comprehensive_tests",
                "success": False,
                "duration": duration,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def run_benchmark_suite(self) -> Dict[str, Any]:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•å¥—ä»¶"""
        self.logger.info("ğŸƒ å¼€å§‹è¿è¡ŒåŸºå‡†æµ‹è¯•å¥—ä»¶...")
        start_time = time.perf_counter()

        try:
            suite = ComprehensiveBenchmarkSuite(str(self.claude_dir))
            results = suite.run_full_benchmark_suite()
            report_path = suite.save_report(results)

            duration = time.perf_counter() - start_time

            result = {
                "suite_name": "benchmark_tests",
                "success": True,
                "duration": duration,
                "results": results,
                "report_path": str(report_path),
                "timestamp": datetime.now().isoformat(),
            }

            self.logger.info(f"âœ… åŸºå‡†æµ‹è¯•å®Œæˆ - è€—æ—¶: {duration:.2f}s")
            return result

        except Exception as e:
            duration = time.perf_counter() - start_time
            self.logger.error(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return {
                "suite_name": "benchmark_tests",
                "success": False,
                "duration": duration,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def run_stress_tests_suite(self, duration: float = 120.0) -> Dict[str, Any]:
        """è¿è¡Œå‹åŠ›æµ‹è¯•å¥—ä»¶"""
        self.logger.info("ğŸ’ª å¼€å§‹è¿è¡Œå‹åŠ›æµ‹è¯•å¥—ä»¶...")
        start_time = time.perf_counter()

        try:
            suite = ComprehensiveStressTestSuite(str(self.claude_dir))
            results = suite.run_full_stress_test_suite(duration)
            report_path = suite.save_stress_test_report(results)

            actual_duration = time.perf_counter() - start_time

            result = {
                "suite_name": "stress_tests",
                "success": True,
                "duration": actual_duration,
                "test_duration": duration,
                "results": results,
                "report_path": str(report_path),
                "timestamp": datetime.now().isoformat(),
            }

            self.logger.info(f"âœ… å‹åŠ›æµ‹è¯•å®Œæˆ - è€—æ—¶: {actual_duration:.2f}s")
            return result

        except Exception as e:
            actual_duration = time.perf_counter() - start_time
            self.logger.error(f"âŒ å‹åŠ›æµ‹è¯•å¤±è´¥: {e}")
            return {
                "suite_name": "stress_tests",
                "success": False,
                "duration": actual_duration,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def run_quick_validation(self) -> Dict[str, Any]:
        """è¿è¡Œå¿«é€ŸéªŒè¯æµ‹è¯•"""
        self.logger.info("âš¡ å¼€å§‹è¿è¡Œå¿«é€ŸéªŒè¯æµ‹è¯•...")
        start_time = time.perf_counter()

        validation_results = []

        try:
            pass  # Auto-fixed empty block
            # 1. æ£€æŸ¥é…ç½®æ–‡ä»¶
            config_files = [
                self.claude_dir / "settings.json",
                self.claude_dir / "config.yaml",
            ]

            for config_file in config_files:
                if config_file.exists():
                    try:
                        if config_file.suffix == ".json":
                            with open(config_file, "r") as f:
                                json.load(f)
                        elif config_file.suffix in [".yaml", ".yml"]:
                            import yaml

                            with open(config_file, "r") as f:
                                yaml.safe_load(f)

                        validation_results.append(
                            {
                                "test": f"config_parse_{config_file.name}",
                                "success": True,
                                "message": f"é…ç½®æ–‡ä»¶ {config_file.name} è§£ææˆåŠŸ",
                            }
                        )
                    except Exception as e:
                        validation_results.append(
                            {
                                "test": f"config_parse_{config_file.name}",
                                "success": False,
                                "error": str(e),
                            }
                        )

            # 2. æ£€æŸ¥Hookæ–‡ä»¶
            hooks_dir = self.claude_dir / "hooks"
            if hooks_dir.exists():
                hook_count = len(list(hooks_dir.glob("*.sh")))
                validation_results.append(
                    {
                        "test": "hook_files_check",
                        "success": hook_count > 0,
                        "message": f"å‘ç° {hook_count} ä¸ªHookæ–‡ä»¶",
                    }
                )

                # æ£€æŸ¥å‡ ä¸ªå…³é”®Hook
                key_hooks = [
                    "smart_agent_selector.sh",
                    "quality_gate.sh",
                    "performance_monitor.sh",
                ]
                for hook_name in key_hooks:
                    hook_path = hooks_dir / hook_name
                    if hook_path.exists():
                        pass  # Auto-fixed empty block
                        # è¯­æ³•æ£€æŸ¥
                        try:
                            result = subprocess.run(
                                ["bash", "-n", str(hook_path)],
                                capture_output=True,
                                text=True,
                                timeout=5,
                            )
                            validation_results.append(
                                {
                                    "test": f"hook_syntax_{hook_name}",
                                    "success": result.returncode == 0,
                                    "message": "è¯­æ³•æ£€æŸ¥é€šè¿‡"
                                    if result.returncode == 0
                                    else f"è¯­æ³•é”™è¯¯: {result.stderr}",
                                }
                            )
                        except Exception as e:
                            validation_results.append(
                                {
                                    "test": f"hook_syntax_{hook_name}",
                                    "success": False,
                                    "error": str(e),
                                }
                            )

            # 3. æ£€æŸ¥Pythonä¾èµ–
            required_packages = ["psutil", "json", "yaml", "subprocess", "threading"]
            for package in required_packages:
                try:
                    __import__(package)
                    validation_results.append(
                        {
                            "test": f"dependency_{package}",
                            "success": True,
                            "message": f"ä¾èµ– {package} å¯ç”¨",
                        }
                    )
                except ImportError:
                    validation_results.append(
                        {
                            "test": f"dependency_{package}",
                            "success": False,
                            "error": f"ç¼ºå°‘ä¾èµ–: {package}",
                        }
                    )

            duration = time.perf_counter() - start_time

            # ç»Ÿè®¡ç»“æœ
            total_tests = len(validation_results)
            successful_tests = sum(1 for r in validation_results if r["success"])

            result = {
                "suite_name": "quick_validation",
                "success": successful_tests == total_tests,
                "duration": duration,
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "failed_tests": total_tests - successful_tests,
                "details": validation_results,
                "timestamp": datetime.now().isoformat(),
            }

            self.logger.info(
                f"âœ… å¿«é€ŸéªŒè¯å®Œæˆ - æˆåŠŸ: {successful_tests}/{total_tests}, è€—æ—¶: {duration:.2f}s"
            )
            return result

        except Exception as e:
            duration = time.perf_counter() - start_time
            self.logger.error(f"âŒ å¿«é€ŸéªŒè¯å¤±è´¥: {e}")
            return {
                "suite_name": "quick_validation",
                "success": False,
                "duration": duration,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def run_all_tests(
        self, include_stress: bool = True, stress_duration: float = 120.0
    ) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶"""
        self.logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶...")
        overall_start_time = time.perf_counter()

        print("=" * 80)
        print("ğŸ§ª Claude Enhancer ç»¼åˆæµ‹è¯•æ¡†æ¶")
        print("=" * 80)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æµ‹è¯•ç¯å¢ƒ: {self.claude_dir}")
        print("=" * 80)

        # 1. å¿«é€ŸéªŒè¯
        print("\nâš¡ Phase 1: å¿«é€ŸéªŒè¯æµ‹è¯•")
        self.results["test_suites"]["quick_validation"] = self.run_quick_validation()

        # 2. å•å…ƒæµ‹è¯•
        print("\nğŸ§ª Phase 2: å•å…ƒæµ‹è¯•")
        self.results["test_suites"]["unit_tests"] = self.run_unit_tests_suite()

        # 3. ç»¼åˆæµ‹è¯•
        print("\nğŸ”¬ Phase 3: ç»¼åˆåŠŸèƒ½æµ‹è¯•")
        self.results["test_suites"][
            "comprehensive_tests"
        ] = self.run_comprehensive_tests_suite()

        # 4. åŸºå‡†æµ‹è¯•
        print("\nğŸƒ Phase 4: åŸºå‡†æ€§èƒ½æµ‹è¯•")
        self.results["test_suites"]["benchmark_tests"] = self.run_benchmark_suite()

        # 5. å‹åŠ›æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
        if include_stress:
            print(f"\nğŸ’ª Phase 5: å‹åŠ›æµ‹è¯• (æŒç»­æ—¶é—´: {stress_duration}s)")
            self.results["test_suites"]["stress_tests"] = self.run_stress_tests_suite(
                stress_duration
            )

        # 6. ç”Ÿæˆæ€»ç»“å’Œå»ºè®®
        print("\nğŸ“Š Phase 6: ç»“æœåˆ†æå’Œå»ºè®®ç”Ÿæˆ")
        self.analyze_overall_results()
        self.generate_comprehensive_recommendations()

        overall_duration = time.perf_counter() - overall_start_time
        self.results["total_duration"] = overall_duration

        print("\n" + "=" * 80)
        print("âœ… æ‰€æœ‰æµ‹è¯•å¥—ä»¶æ‰§è¡Œå®Œæˆ!")
        print(f"æ€»è€—æ—¶: {overall_duration:.2f}ç§’")
        print("=" * 80)

        return self.results

    def analyze_overall_results(self):
        """åˆ†ææ€»ä½“æµ‹è¯•ç»“æœ"""
        summary = {
            "total_suites": len(self.results["test_suites"]),
            "successful_suites": 0,
            "failed_suites": 0,
            "total_duration": 0,
            "suite_details": {},
        }

        for suite_name, suite_result in self.results["test_suites"].items():
            summary["total_duration"] += suite_result.get("duration", 0)

            if suite_result.get("success", False):
                summary["successful_suites"] += 1
            else:
                summary["failed_suites"] += 1

            # æå–æ¯ä¸ªå¥—ä»¶çš„å…³é”®æŒ‡æ ‡
            if suite_name == "quick_validation":
                summary["suite_details"][suite_name] = {
                    "total_tests": suite_result.get("total_tests", 0),
                    "successful_tests": suite_result.get("successful_tests", 0),
                    "success_rate": suite_result.get("successful_tests", 0)
                    / max(suite_result.get("total_tests", 1), 1),
                }
            elif suite_name in [
                "comprehensive_tests",
                "benchmark_tests",
                "stress_tests",
            ]:
                # ä»åµŒå¥—ç»“æœä¸­æå–ä¿¡æ¯
                results = suite_result.get("results", {})
                if isinstance(results, dict):
                    if "analysis" in results:
                        analysis = results["analysis"]
                        if "overall" in analysis:
                            overall = analysis["overall"]
                            summary["suite_details"][suite_name] = {
                                "total_tests": overall.get("total_tests", 0),
                                "successful_tests": overall.get("successful_tests", 0),
                                "success_rate": overall.get("success_rate", 0),
                            }

        summary["overall_success_rate"] = summary["successful_suites"] / max(
            summary["total_suites"], 1
        )
        self.results["summary"] = summary

    def generate_comprehensive_recommendations(self):
        """ç”Ÿæˆç»¼åˆå»ºè®®"""
        recommendations = []

        # åŸºäºæ€»ä½“æˆåŠŸç‡çš„å»ºè®®
        overall_success_rate = self.results["summary"]["overall_success_rate"]
        if overall_success_rate < 0.8:
            recommendations.append(
                {
                    "category": "Overall System Health",
                    "priority": "CRITICAL",
                    "issue": f"æµ‹è¯•å¥—ä»¶æ•´ä½“æˆåŠŸç‡è¾ƒä½: {overall_success_rate:.1%}",
                    "recommendation": "éœ€è¦ç«‹å³æ’æŸ¥ç³»ç»ŸåŸºç¡€é—®é¢˜",
                    "affected_suites": [
                        name
                        for name, result in self.results["test_suites"].items()
                        if not result.get("success", False)
                    ],
                }
            )

        # æ”¶é›†å„å¥—ä»¶çš„å…·ä½“å»ºè®®
        for suite_name, suite_result in self.results["test_suites"].items():
            if not suite_result.get("success", False):
                recommendations.append(
                    {
                        "category": f"{suite_name.title()} Issues",
                        "priority": "HIGH",
                        "issue": f"{suite_name} æµ‹è¯•å¤±è´¥",
                        "recommendation": f"æ£€æŸ¥ {suite_name} çš„å…·ä½“é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤",
                        "error": suite_result.get("error", "æœªçŸ¥é”™è¯¯"),
                    }
                )

            # ä»å¥—ä»¶ç»“æœä¸­æå–å…·ä½“å»ºè®®
            if "results" in suite_result and isinstance(suite_result["results"], dict):
                suite_recommendations = suite_result["results"].get(
                    "recommendations", []
                )
                for rec in suite_recommendations:
                    rec["source_suite"] = suite_name
                    recommendations.append(rec)

        # æ€§èƒ½ç›¸å…³å»ºè®®
        total_duration = self.results["summary"]["total_duration"]
        if total_duration > 300:  # è¶…è¿‡5åˆ†é’Ÿ
            recommendations.append(
                {
                    "category": "Performance",
                    "priority": "MEDIUM",
                    "issue": f"æµ‹è¯•æ‰§è¡Œæ—¶é—´è¿‡é•¿: {total_duration:.1f}ç§’",
                    "recommendation": "è€ƒè™‘ä¼˜åŒ–æµ‹è¯•æ‰§è¡Œæ•ˆç‡æˆ–å¹¶è¡ŒåŒ–æµ‹è¯•",
                }
            )

        self.results["recommendations"] = recommendations

    def save_final_report(
        self, filename: str = "claude_enhancer_test_report.json"
    ) -> Path:
        """ä¿å­˜æœ€ç»ˆæµ‹è¯•æŠ¥å‘Š"""
        report_path = self.test_dir / filename
        report_path.parent.mkdir(parents=True, exist_ok=True)

        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)

        self.logger.info(f"ğŸ“„ æœ€ç»ˆæµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        return report_path

    def print_final_summary(self):
        """æ‰“å°æœ€ç»ˆæ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ“‹ Claude Enhancer æµ‹è¯•æ¡†æ¶ - æœ€ç»ˆæ‘˜è¦")
        print("=" * 80)

        summary = self.results["summary"]
        print(f"æµ‹è¯•å¥—ä»¶æ€»æ•°: {summary['total_suites']}")
        print(f"æˆåŠŸå¥—ä»¶: {summary['successful_suites']}")
        print(f"å¤±è´¥å¥—ä»¶: {summary['failed_suites']}")
        print(f"æ•´ä½“æˆåŠŸç‡: {summary['overall_success_rate']:.1%}")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {summary['total_duration']:.2f}ç§’")

        # æ˜¾ç¤ºå„å¥—ä»¶è¯¦æƒ…
        print(f"\nğŸ“Š å„å¥—ä»¶è¯¦æƒ…:")
        for suite_name, suite_result in self.results["test_suites"].items():
            status = "âœ…" if suite_result.get("success", False) else "âŒ"
            duration = suite_result.get("duration", 0)
            print(f"  {status} {suite_name}: {duration:.2f}s")

        # æ˜¾ç¤ºå…³é”®å»ºè®®
        recommendations = self.results.get("recommendations", [])
        critical_recs = [r for r in recommendations if r.get("priority") == "CRITICAL"]
        high_recs = [r for r in recommendations if r.get("priority") == "HIGH"]

        if critical_recs:
            print(f"\nğŸš¨ å…³é”®é—®é¢˜ ({len(critical_recs)}ä¸ª):")
            for rec in critical_recs[:3]:
                print(f"  - {rec['issue']}")

        if high_recs:
            print(f"\nâš ï¸  é‡è¦é—®é¢˜ ({len(high_recs)}ä¸ª):")
            for rec in high_recs[:3]:
                print(f"  - {rec['issue']}")

        print(f"\nğŸ’¡ æ€»å»ºè®®æ•°: {len(recommendations)}")
        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Claude Enhancer æµ‹è¯•æ¡†æ¶")
    parser.add_argument(
        "--claude-dir",
        default="/home/xx/dev/Claude_Enhancer/.claude",
        help="Claudeç›®å½•è·¯å¾„",
    )
    parser.add_argument("--quick", action="store_true", help="åªè¿è¡Œå¿«é€ŸéªŒè¯æµ‹è¯•")
    parser.add_argument("--no-stress", action="store_true", help="è·³è¿‡å‹åŠ›æµ‹è¯•")
    parser.add_argument(
        "--stress-duration", type=float, default=120.0, help="å‹åŠ›æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰"
    )
    parser.add_argument(
        "--suite",
        choices=["unit", "comprehensive", "benchmark", "stress", "validation"],
        help="åªè¿è¡ŒæŒ‡å®šçš„æµ‹è¯•å¥—ä»¶",
    )

    args = parser.parse_args()

    try:
        orchestrator = TestOrchestrator(args.claude_dir)

        if args.quick:
            pass  # Auto-fixed empty block
            # åªè¿è¡Œå¿«é€ŸéªŒè¯
            result = orchestrator.run_quick_validation()
            print("\nâš¡ å¿«é€ŸéªŒè¯ç»“æœ:")
            print(f"æˆåŠŸ: {result['successful_tests']}/{result['total_tests']}")
            return result["success"]

        elif args.suite:
            pass  # Auto-fixed empty block
            # è¿è¡ŒæŒ‡å®šå¥—ä»¶
            if args.suite == "unit":
                result = orchestrator.run_unit_tests_suite()
            elif args.suite == "comprehensive":
                result = orchestrator.run_comprehensive_tests_suite()
            elif args.suite == "benchmark":
                result = orchestrator.run_benchmark_suite()
            elif args.suite == "stress":
                result = orchestrator.run_stress_tests_suite(args.stress_duration)
            elif args.suite == "validation":
                result = orchestrator.run_quick_validation()

            print(
                f"\n{args.suite.title()} æµ‹è¯•ç»“æœ: {'âœ… æˆåŠŸ' if result['success'] else 'âŒ å¤±è´¥'}"
            )
            return result["success"]

        else:
            pass  # Auto-fixed empty block
            # è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
            results = orchestrator.run_all_tests(
                include_stress=not args.no_stress, stress_duration=args.stress_duration
            )

            # ä¿å­˜æŠ¥å‘Šå¹¶æ‰“å°æ‘˜è¦
            report_path = orchestrator.save_final_report()
            orchestrator.print_final_summary()

            print(f"\nğŸ“„ å®Œæ•´æŠ¥å‘Š: {report_path}")

            # æ ¹æ®æ€»ä½“æˆåŠŸç‡å†³å®šé€€å‡ºä»£ç 
            overall_success = results["summary"]["overall_success_rate"]
            return overall_success >= 0.8

    except Exception as e:
        print(f"âŒ æµ‹è¯•æ¡†æ¶æ‰§è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
