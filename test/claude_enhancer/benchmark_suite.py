#!/usr/bin/env python3
"""
Claude Enhancer åŸºå‡†æµ‹è¯•å¥—ä»¶
å»ºç«‹æ€§èƒ½åŸºå‡†çº¿ï¼Œè¿›è¡ŒæŒç»­æ€§èƒ½ç›‘æ§
"""

import time
import json
import statistics
import psutil
import subprocess
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict

# å¯é€‰ä¾èµ–ï¼Œå¦‚æœä¸å­˜åœ¨ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    import pandas as pd

    VISUALIZATION_AVAILABLE = True
except ImportError:
    VISUALIZATION_AVAILABLE = False


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""

    test_name: str
    category: str
    execution_time: float
    memory_usage_mb: float
    cpu_percent: float
    success: bool
    timestamp: str
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class SystemBenchmarks:
    """ç³»ç»ŸåŸºå‡†æµ‹è¯•"""

    def __init__(self):
        self.results: List[BenchmarkResult] = []

    def run_cpu_benchmark(self, duration: float = 1.0) -> BenchmarkResult:
        """CPUå¯†é›†å‹åŸºå‡†æµ‹è¯•"""
        start_time = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        # CPUå¯†é›†å‹è®¡ç®—
        total = 0
        for i in range(1000000):
            total += i * i

        end_time = time.perf_counter()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        cpu_percent = psutil.cpu_percent(interval=0.1)

        return BenchmarkResult(
            test_name="cpu_intensive_calculation",
            category="system",
            execution_time=end_time - start_time,
            memory_usage_mb=end_memory - start_memory,
            cpu_percent=cpu_percent,
            success=True,
            timestamp=datetime.now().isoformat(),
            metadata={"iterations": 1000000, "calculation": "sum_of_squares"},
        )

    def run_memory_benchmark(self) -> BenchmarkResult:
        """å†…å­˜åˆ†é…åŸºå‡†æµ‹è¯•"""
        start_time = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        # å†…å­˜å¯†é›†å‹æ“ä½œ
        large_list = [i for i in range(1000000)]
        matrix = [[j for j in range(100)] for i in range(1000)]

        end_time = time.perf_counter()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        cpu_percent = psutil.cpu_percent(interval=0.1)

        # æ¸…ç†å†…å­˜
        del large_list, matrix

        return BenchmarkResult(
            test_name="memory_allocation",
            category="system",
            execution_time=end_time - start_time,
            memory_usage_mb=end_memory - start_memory,
            cpu_percent=cpu_percent,
            success=True,
            timestamp=datetime.now().isoformat(),
            metadata={"list_size": 1000000, "matrix_size": "1000x100"},
        )

    def run_disk_io_benchmark(self) -> BenchmarkResult:
        """ç£ç›˜I/OåŸºå‡†æµ‹è¯•"""
        start_time = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        with tempfile.TemporaryDirectory() as temp_dir:
            test_file = Path(temp_dir) / "benchmark_file.txt"

            # å†™å…¥æµ‹è¯•
            test_data = "x" * 1024 * 100  # 100KBæ•°æ®
            for i in range(100):  # å†™å…¥10MB
                with open(test_file, "a") as f:
                    f.write(test_data)

            # è¯»å–æµ‹è¯•
            total_read = 0
            with open(test_file, "r") as f:
                while True:
                    chunk = f.read(1024)
                    if not chunk:
                        break
                    total_read += len(chunk)

        end_time = time.perf_counter()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        cpu_percent = psutil.cpu_percent(interval=0.1)

        return BenchmarkResult(
            test_name="disk_io_operations",
            category="system",
            execution_time=end_time - start_time,
            memory_usage_mb=end_memory - start_memory,
            cpu_percent=cpu_percent,
            success=True,
            timestamp=datetime.now().isoformat(),
            metadata={"data_size_mb": 10, "total_read_bytes": total_read},
        )


class HookSystemBenchmarks:
    """Hookç³»ç»ŸåŸºå‡†æµ‹è¯•"""

    def __init__(self, claude_dir: str = "/home/xx/dev/Perfect21/.claude"):
        self.claude_dir = Path(claude_dir)
        self.results: List[BenchmarkResult] = []

    def run_hook_execution_benchmark(self) -> List[BenchmarkResult]:
        """Hookæ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        results = []
        hooks_dir = self.claude_dir / "hooks"

        if not hooks_dir.exists():
            return results

        # æµ‹è¯•ç°æœ‰çš„Hookè„šæœ¬
        hook_files = [
            "smart_agent_selector.sh",
            "quality_gate.sh",
            "performance_monitor.sh",
            "error_handler.sh",
            "task_type_detector.sh",
        ]

        for hook_name in hook_files:
            hook_path = hooks_dir / hook_name
            if hook_path.exists():
                result = self._benchmark_single_hook(hook_path)
                if result:
                    results.append(result)

        return results

    def _benchmark_single_hook(self, hook_path: Path) -> Optional[BenchmarkResult]:
        """å¯¹å•ä¸ªHookè¿›è¡ŒåŸºå‡†æµ‹è¯•"""
        try:
            start_time = time.perf_counter()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024

            # è¿è¡ŒHookï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
            result = subprocess.run(
                ["bash", str(hook_path), "--test"],
                capture_output=True,
                text=True,
                timeout=10,
                env={"CLAUDE_TEST_MODE": "1", "HOOK_BENCHMARK": "1"},
            )

            end_time = time.perf_counter()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            cpu_percent = psutil.cpu_percent(interval=0.1)

            return BenchmarkResult(
                test_name=f"hook_{hook_path.name}",
                category="hook_system",
                execution_time=end_time - start_time,
                memory_usage_mb=end_memory - start_memory,
                cpu_percent=cpu_percent,
                success=result.returncode == 0,
                timestamp=datetime.now().isoformat(),
                metadata={
                    "return_code": result.returncode,
                    "stdout_length": len(result.stdout),
                    "stderr_length": len(result.stderr),
                },
            )

        except Exception as e:
            return BenchmarkResult(
                test_name=f"hook_{hook_path.name}",
                category="hook_system",
                execution_time=0.0,
                memory_usage_mb=0.0,
                cpu_percent=0.0,
                success=False,
                timestamp=datetime.now().isoformat(),
                metadata={"error": str(e)},
            )

    def run_hook_concurrency_benchmark(self) -> BenchmarkResult:
        """Hookå¹¶å‘æ‰§è¡ŒåŸºå‡†æµ‹è¯•"""
        import threading
        import concurrent.futures

        hooks_dir = self.claude_dir / "hooks"
        if not hooks_dir.exists():
            return BenchmarkResult(
                test_name="hook_concurrency",
                category="hook_system",
                execution_time=0.0,
                memory_usage_mb=0.0,
                cpu_percent=0.0,
                success=False,
                timestamp=datetime.now().isoformat(),
                metadata={"error": "hooks directory not found"},
            )

        start_time = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        # é€‰æ‹©å‡ ä¸ªè½»é‡çº§çš„Hookè¿›è¡Œå¹¶å‘æµ‹è¯•
        test_hooks = []
        for hook_file in hooks_dir.glob("*.sh"):
            if hook_file.name in ["performance_monitor.sh", "error_handler.sh"]:
                test_hooks.append(hook_file)

        if not test_hooks:
            return BenchmarkResult(
                test_name="hook_concurrency",
                category="hook_system",
                execution_time=0.0,
                memory_usage_mb=0.0,
                cpu_percent=0.0,
                success=False,
                timestamp=datetime.now().isoformat(),
                metadata={"error": "no suitable hooks found"},
            )

        # å¹¶å‘æ‰§è¡ŒHook
        success_count = 0
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for hook in test_hooks:
                future = executor.submit(self._run_hook_concurrent, hook)
                futures.append(future)

            for future in concurrent.futures.as_completed(futures):
                if future.result():
                    success_count += 1

        end_time = time.perf_counter()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        cpu_percent = psutil.cpu_percent(interval=0.1)

        return BenchmarkResult(
            test_name="hook_concurrency",
            category="hook_system",
            execution_time=end_time - start_time,
            memory_usage_mb=end_memory - start_memory,
            cpu_percent=cpu_percent,
            success=success_count > 0,
            timestamp=datetime.now().isoformat(),
            metadata={
                "total_hooks": len(test_hooks),
                "successful_hooks": success_count,
                "success_rate": success_count / len(test_hooks) if test_hooks else 0,
            },
        )

    def _run_hook_concurrent(self, hook_path: Path) -> bool:
        """å¹¶å‘è¿è¡Œå•ä¸ªHook"""
        try:
            result = subprocess.run(
                ["bash", str(hook_path), "--test"],
                capture_output=True,
                text=True,
                timeout=5,
                env={"CLAUDE_TEST_MODE": "1", "HOOK_CONCURRENT_TEST": "1"},
            )
            return result.returncode == 0
        except Exception:
            return False


class PerformanceComparisonBenchmarks:
    """æ€§èƒ½å¯¹æ¯”åŸºå‡†æµ‹è¯•"""

    def __init__(self, claude_dir: str = "/home/xx/dev/Perfect21/.claude"):
        self.claude_dir = Path(claude_dir)
        self.results: List[BenchmarkResult] = []

    def compare_cleanup_scripts(self) -> List[BenchmarkResult]:
        """å¯¹æ¯”ä¸åŒç‰ˆæœ¬çš„æ¸…ç†è„šæœ¬æ€§èƒ½"""
        results = []
        scripts_dir = self.claude_dir / "scripts"

        cleanup_scripts = [
            "cleanup.sh",
            "performance_optimized_cleanup.sh",
            "ultra_optimized_cleanup.sh",
        ]

        for script_name in cleanup_scripts:
            script_path = scripts_dir / script_name
            if script_path.exists():
                result = self._benchmark_cleanup_script(script_path)
                if result:
                    results.append(result)

        return results

    def _benchmark_cleanup_script(self, script_path: Path) -> Optional[BenchmarkResult]:
        """å¯¹å•ä¸ªæ¸…ç†è„šæœ¬è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
        try:
            # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç¯å¢ƒ
            with tempfile.TemporaryDirectory() as temp_dir:
                test_claude_dir = Path(temp_dir) / ".claude"
                test_claude_dir.mkdir(parents=True)

                # åˆ›å»ºä¸€äº›æµ‹è¯•æ–‡ä»¶ç”¨äºæ¸…ç†
                for i in range(10):
                    (test_claude_dir / f"temp_file_{i}.tmp").write_text("test data")

                start_time = time.perf_counter()
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024

                # è¿è¡Œæ¸…ç†è„šæœ¬
                result = subprocess.run(
                    ["bash", str(script_path)],
                    capture_output=True,
                    text=True,
                    timeout=30,
                    cwd=temp_dir,
                    env={"CLAUDE_TEST_MODE": "1", "CLEANUP_BENCHMARK": "1"},
                )

                end_time = time.perf_counter()
                end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                cpu_percent = psutil.cpu_percent(interval=0.1)

                return BenchmarkResult(
                    test_name=f"cleanup_{script_path.name}",
                    category="performance_comparison",
                    execution_time=end_time - start_time,
                    memory_usage_mb=end_memory - start_memory,
                    cpu_percent=cpu_percent,
                    success=result.returncode == 0,
                    timestamp=datetime.now().isoformat(),
                    metadata={
                        "script_name": script_path.name,
                        "return_code": result.returncode,
                        "stdout_length": len(result.stdout),
                    },
                )

        except Exception as e:
            return BenchmarkResult(
                test_name=f"cleanup_{script_path.name}",
                category="performance_comparison",
                execution_time=0.0,
                memory_usage_mb=0.0,
                cpu_percent=0.0,
                success=False,
                timestamp=datetime.now().isoformat(),
                metadata={"error": str(e)},
            )


class BenchmarkAnalyzer:
    """åŸºå‡†æµ‹è¯•åˆ†æå™¨"""

    def __init__(self):
        self.historical_data: List[Dict[str, Any]] = []

    def load_historical_data(self, data_file: Path) -> bool:
        """åŠ è½½å†å²åŸºå‡†æ•°æ®"""
        try:
            if data_file.exists():
                with open(data_file, "r") as f:
                    self.historical_data = json.load(f)
                return True
        except Exception:
            pass
        return False

    def save_benchmark_data(self, results: List[BenchmarkResult], data_file: Path):
        """ä¿å­˜åŸºå‡†æµ‹è¯•æ•°æ®"""
        benchmark_session = {
            "timestamp": datetime.now().isoformat(),
            "results": [asdict(result) for result in results],
            "system_info": {
                "cpu_count": psutil.cpu_count(),
                "memory_total_gb": psutil.virtual_memory().total / (1024**3),
                "python_version": __import__("sys").version,
            },
        }

        self.historical_data.append(benchmark_session)

        # ä¿å­˜åˆ°æ–‡ä»¶
        data_file.parent.mkdir(parents=True, exist_ok=True)
        with open(data_file, "w") as f:
            json.dump(self.historical_data, f, indent=2)

    def analyze_performance_trends(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        if len(self.historical_data) < 2:
            return {"message": "éœ€è¦è‡³å°‘2æ¬¡åŸºå‡†æµ‹è¯•æ•°æ®æ¥åˆ†æè¶‹åŠ¿"}

        analysis = {
            "total_sessions": len(self.historical_data),
            "date_range": {
                "start": self.historical_data[0]["timestamp"],
                "end": self.historical_data[-1]["timestamp"],
            },
            "trends": {},
            "performance_regression": [],
            "performance_improvement": [],
        }

        # åˆ†ææ¯ä¸ªæµ‹è¯•çš„è¶‹åŠ¿
        test_performance = {}
        for session in self.historical_data:
            for result in session["results"]:
                test_name = result["test_name"]
                if test_name not in test_performance:
                    test_performance[test_name] = []
                test_performance[test_name].append(
                    {
                        "timestamp": session["timestamp"],
                        "execution_time": result["execution_time"],
                        "memory_usage": result["memory_usage_mb"],
                        "success": result["success"],
                    }
                )

        # è®¡ç®—è¶‹åŠ¿
        for test_name, history in test_performance.items():
            if len(history) >= 2:
                recent_times = [h["execution_time"] for h in history[-3:]]
                older_times = (
                    [h["execution_time"] for h in history[:-3]]
                    if len(history) > 3
                    else history[:-1]
                )

                if older_times:
                    recent_avg = statistics.mean(recent_times)
                    older_avg = statistics.mean(older_times)
                    change_percent = ((recent_avg - older_avg) / older_avg) * 100

                    analysis["trends"][test_name] = {
                        "recent_avg_time": recent_avg,
                        "historical_avg_time": older_avg,
                        "change_percent": change_percent,
                        "trend": "improving"
                        if change_percent < -5
                        else "degrading"
                        if change_percent > 5
                        else "stable",
                    }

                    # æ£€æµ‹æ€§èƒ½å›å½’æˆ–æ”¹è¿›
                    if change_percent > 20:  # 20%ä»¥ä¸Šçš„æ€§èƒ½ä¸‹é™
                        analysis["performance_regression"].append(
                            {
                                "test_name": test_name,
                                "degradation_percent": change_percent,
                            }
                        )
                    elif change_percent < -20:  # 20%ä»¥ä¸Šçš„æ€§èƒ½æå‡
                        analysis["performance_improvement"].append(
                            {
                                "test_name": test_name,
                                "improvement_percent": abs(change_percent),
                            }
                        )

        return analysis

    def generate_performance_report(
        self, results: List[BenchmarkResult]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = {
            "summary": {
                "total_tests": len(results),
                "successful_tests": sum(1 for r in results if r.success),
                "failed_tests": sum(1 for r in results if not r.success),
                "success_rate": 0.0,
            },
            "performance_metrics": {
                "execution_times": {},
                "memory_usage": {},
                "cpu_usage": {},
            },
            "category_analysis": {},
            "recommendations": [],
        }

        if results:
            report["summary"]["success_rate"] = report["summary"][
                "successful_tests"
            ] / len(results)

            # æˆåŠŸçš„æµ‹è¯•ç»“æœ
            successful_results = [r for r in results if r.success]

            if successful_results:
                execution_times = [r.execution_time for r in successful_results]
                memory_usage = [r.memory_usage_mb for r in successful_results]
                cpu_usage = [
                    r.cpu_percent
                    for r in successful_results
                    if r.cpu_percent is not None
                ]

                report["performance_metrics"]["execution_times"] = {
                    "mean": statistics.mean(execution_times),
                    "median": statistics.median(execution_times),
                    "min": min(execution_times),
                    "max": max(execution_times),
                    "std_dev": statistics.stdev(execution_times)
                    if len(execution_times) > 1
                    else 0,
                }

                report["performance_metrics"]["memory_usage"] = {
                    "mean": statistics.mean(memory_usage),
                    "median": statistics.median(memory_usage),
                    "min": min(memory_usage),
                    "max": max(memory_usage),
                }

                if cpu_usage:
                    report["performance_metrics"]["cpu_usage"] = {
                        "mean": statistics.mean(cpu_usage),
                        "median": statistics.median(cpu_usage),
                        "min": min(cpu_usage),
                        "max": max(cpu_usage),
                    }

                # æŒ‰ç±»åˆ«åˆ†æ
                categories = {}
                for result in successful_results:
                    category = result.category
                    if category not in categories:
                        categories[category] = []
                    categories[category].append(result)

                for category, cat_results in categories.items():
                    cat_times = [r.execution_time for r in cat_results]
                    report["category_analysis"][category] = {
                        "test_count": len(cat_results),
                        "avg_time": statistics.mean(cat_times),
                        "total_time": sum(cat_times),
                    }

        # ç”Ÿæˆå»ºè®®
        self._generate_recommendations(report, results)

        return report

    def _generate_recommendations(
        self, report: Dict[str, Any], results: List[BenchmarkResult]
    ):
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºæˆåŠŸç‡çš„å»ºè®®
        success_rate = report["summary"]["success_rate"]
        if success_rate < 0.9:
            recommendations.append(
                {
                    "category": "Reliability",
                    "priority": "HIGH",
                    "issue": f"åŸºå‡†æµ‹è¯•æˆåŠŸç‡è¾ƒä½: {success_rate:.1%}",
                    "recommendation": "è°ƒæŸ¥å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ï¼Œä¿®å¤æ½œåœ¨çš„ç¨³å®šæ€§é—®é¢˜",
                }
            )

        # åŸºäºæ‰§è¡Œæ—¶é—´çš„å»ºè®®
        exec_metrics = report["performance_metrics"].get("execution_times", {})
        if exec_metrics:
            max_time = exec_metrics.get("max", 0)
            mean_time = exec_metrics.get("mean", 0)

            if max_time > 10.0:  # è¶…è¿‡10ç§’çš„æµ‹è¯•
                recommendations.append(
                    {
                        "category": "Performance",
                        "priority": "MEDIUM",
                        "issue": f"å‘ç°æ‰§è¡Œæ—¶é—´è¿‡é•¿çš„æµ‹è¯•: {max_time:.2f}ç§’",
                        "recommendation": "ä¼˜åŒ–æ…¢é€Ÿæ“ä½œï¼Œè€ƒè™‘å¼‚æ­¥æ‰§è¡Œæˆ–ç¼“å­˜",
                    }
                )

            if mean_time > 2.0:  # å¹³å‡æ—¶é—´è¶…è¿‡2ç§’
                recommendations.append(
                    {
                        "category": "Performance",
                        "priority": "MEDIUM",
                        "issue": f"å¹³å‡æ‰§è¡Œæ—¶é—´è¾ƒé•¿: {mean_time:.2f}ç§’",
                        "recommendation": "æ•´ä½“æ€§èƒ½ä¼˜åŒ–ï¼Œå‡å°‘ä¸å¿…è¦çš„æ“ä½œ",
                    }
                )

        # åŸºäºå†…å­˜ä½¿ç”¨çš„å»ºè®®
        memory_metrics = report["performance_metrics"].get("memory_usage", {})
        if memory_metrics:
            max_memory = memory_metrics.get("max", 0)
            if max_memory > 100:  # è¶…è¿‡100MB
                recommendations.append(
                    {
                        "category": "Memory",
                        "priority": "LOW",
                        "issue": f"å†…å­˜ä½¿ç”¨é‡è¾ƒé«˜: {max_memory:.1f}MB",
                        "recommendation": "æ£€æŸ¥å†…å­˜æ³„æ¼ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨",
                    }
                )

        report["recommendations"] = recommendations


class ComprehensiveBenchmarkSuite:
    """ç»¼åˆåŸºå‡†æµ‹è¯•å¥—ä»¶"""

    def __init__(self, claude_dir: str = "/home/xx/dev/Perfect21/.claude"):
        self.claude_dir = claude_dir
        self.system_bench = SystemBenchmarks()
        self.hook_bench = HookSystemBenchmarks(claude_dir)
        self.comparison_bench = PerformanceComparisonBenchmarks(claude_dir)
        self.analyzer = BenchmarkAnalyzer()

    def run_full_benchmark_suite(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•å¥—ä»¶"""
        print("ğŸƒ å¼€å§‹è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•å¥—ä»¶...")
        print("=" * 80)

        all_results = []

        # 1. ç³»ç»ŸåŸºå‡†æµ‹è¯•
        print("ğŸ–¥ï¸  è¿è¡Œç³»ç»ŸåŸºå‡†æµ‹è¯•...")
        all_results.append(self.system_bench.run_cpu_benchmark())
        all_results.append(self.system_bench.run_memory_benchmark())
        all_results.append(self.system_bench.run_disk_io_benchmark())

        # 2. Hookç³»ç»ŸåŸºå‡†æµ‹è¯•
        print("ğŸ”— è¿è¡ŒHookç³»ç»ŸåŸºå‡†æµ‹è¯•...")
        hook_results = self.hook_bench.run_hook_execution_benchmark()
        all_results.extend(hook_results)

        concurrency_result = self.hook_bench.run_hook_concurrency_benchmark()
        all_results.append(concurrency_result)

        # 3. æ€§èƒ½å¯¹æ¯”åŸºå‡†æµ‹è¯•
        print("âš–ï¸  è¿è¡Œæ€§èƒ½å¯¹æ¯”åŸºå‡†æµ‹è¯•...")
        comparison_results = self.comparison_bench.compare_cleanup_scripts()
        all_results.extend(comparison_results)

        # 4. ç”ŸæˆæŠ¥å‘Š
        print("ğŸ“Š ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š...")
        report = self.analyzer.generate_performance_report(all_results)

        # 5. ä¿å­˜æ•°æ®
        data_file = (
            Path(self.claude_dir).parent
            / "test"
            / "claude_enhancer"
            / "benchmark_data.json"
        )
        self.analyzer.save_benchmark_data(all_results, data_file)

        # 6. åŠ è½½å†å²æ•°æ®å¹¶åˆ†æè¶‹åŠ¿
        self.analyzer.load_historical_data(data_file)
        trends = self.analyzer.analyze_performance_trends()

        # åˆå¹¶æŠ¥å‘Š
        final_report = {
            "timestamp": datetime.now().isoformat(),
            "benchmark_results": [asdict(result) for result in all_results],
            "performance_report": report,
            "trend_analysis": trends,
            "data_file": str(data_file),
        }

        print("âœ… åŸºå‡†æµ‹è¯•å¥—ä»¶æ‰§è¡Œå®Œæˆ!")
        return final_report

    def save_report(
        self, report: Dict[str, Any], filename: str = "benchmark_report.json"
    ) -> Path:
        """ä¿å­˜åŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        report_path = (
            Path(self.claude_dir).parent / "test" / "claude_enhancer" / filename
        )
        report_path.parent.mkdir(parents=True, exist_ok=True)

        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ åŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        return report_path


def main():
    """ä¸»å‡½æ•°"""
    try:
        suite = ComprehensiveBenchmarkSuite()
        report = suite.run_full_benchmark_suite()
        report_path = suite.save_report(report)

        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 80)
        print("ğŸ“ˆ åŸºå‡†æµ‹è¯•æ‘˜è¦")
        print("=" * 80)

        perf_report = report.get("performance_report", {})
        summary = perf_report.get("summary", {})

        print(f"æ€»æµ‹è¯•æ•°é‡: {summary.get('total_tests', 0)}")
        print(f"æˆåŠŸæµ‹è¯•: {summary.get('successful_tests', 0)}")
        print(f"æˆåŠŸç‡: {summary.get('success_rate', 0):.1%}")

        exec_metrics = perf_report.get("performance_metrics", {}).get(
            "execution_times", {}
        )
        if exec_metrics:
            print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {exec_metrics.get('mean', 0):.4f}s")
            print(f"æœ€å¿«æµ‹è¯•: {exec_metrics.get('min', 0):.4f}s")
            print(f"æœ€æ…¢æµ‹è¯•: {exec_metrics.get('max', 0):.4f}s")

        # æ˜¾ç¤ºç±»åˆ«åˆ†æ
        category_analysis = perf_report.get("category_analysis", {})
        if category_analysis:
            print("\nğŸ“Š æŒ‰ç±»åˆ«åˆ†æ:")
            for category, stats in category_analysis.items():
                print(
                    f"  {category}: {stats['test_count']}ä¸ªæµ‹è¯•, å¹³å‡æ—¶é—´: {stats['avg_time']:.4f}s"
                )

        # æ˜¾ç¤ºå»ºè®®
        recommendations = perf_report.get("recommendations", [])
        if recommendations:
            print(f"\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®® ({len(recommendations)}æ¡):")
            for rec in recommendations[:3]:  # æ˜¾ç¤ºå‰3æ¡
                print(f"  [{rec['priority']}] {rec['issue']}")

        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_path}")
        return True

    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
