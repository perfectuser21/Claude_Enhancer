#!/usr/bin/env python3
"""
Claude Enhancer å‡çº§ç‰ˆæµ‹è¯•æ¡†æ¶
å‡†ç¡®ã€å…¨é¢çš„æ€§èƒ½æµ‹è¯•å·¥å…·ï¼Œç¡®ä¿æµ‹è¯•ç»“æœçœŸå®åæ˜ ç³»ç»Ÿæ€§èƒ½
"""

import os
import time
import subprocess
import json
import sys
import threading
import psutil
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
import statistics
from contextlib import contextmanager
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
import logging


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""

    test_name: str
    description: str
    success: bool
    execution_time: float
    memory_peak: Optional[float] = None
    cpu_percent: Optional[float] = None
    stdout: str = ""
    stderr: str = ""
    return_code: int = 0
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class BenchmarkSuite:
    """åŸºå‡†æµ‹è¯•å¥—ä»¶"""

    name: str
    description: str
    tests: List[TestResult]
    baseline_time: Optional[float] = None
    performance_target: Optional[float] = None


class SystemMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""

    def __init__(self):
        self.monitoring = False
        self.metrics = {
            "cpu_percent": [],
            "memory_percent": [],
            "memory_mb": [],
            "disk_io": [],
            "network_io": [],
        }
        self.monitor_thread = None

    def start_monitoring(self, interval: float = 0.1):
        """å¼€å§‹ç›‘æ§ç³»ç»Ÿèµ„æº"""
        self.monitoring = True
        self.metrics = {key: [] for key in self.metrics.keys()}

        def monitor():
            while self.monitoring:
                try:
                    # CPUä½¿ç”¨ç‡
                    cpu_percent = psutil.cpu_percent(interval=None)
                    self.metrics["cpu_percent"].append(cpu_percent)

                    # å†…å­˜ä½¿ç”¨
                    memory = psutil.virtual_memory()
                    self.metrics["memory_percent"].append(memory.percent)
                    self.metrics["memory_mb"].append(memory.used / 1024 / 1024)

                    # ç£ç›˜IO
                    disk_io = psutil.disk_io_counters()
                    if disk_io:
                        self.metrics["disk_io"].append(
                            disk_io.read_bytes + disk_io.write_bytes
                        )

                    # ç½‘ç»œIO
                    net_io = psutil.net_io_counters()
                    if net_io:
                        self.metrics["network_io"].append(
                            net_io.bytes_sent + net_io.bytes_recv
                        )

                    time.sleep(interval)
                except Exception:
                    pass

        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()

    def stop_monitoring(self) -> Dict[str, float]:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›ç»Ÿè®¡æ•°æ®"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)

        stats = {}
        for key, values in self.metrics.items():
            if values:
                stats[f"{key}_avg"] = statistics.mean(values)
                stats[f"{key}_max"] = max(values)
                stats[f"{key}_min"] = min(values)

        return stats


class AccurateTestRunner:
    """å‡†ç¡®çš„æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, claude_dir: str = "/home/xx/dev/Claude_Enhancer/.claude"):
        self.claude_dir = Path(claude_dir)
        self.project_dir = self.claude_dir.parent
        self.temp_dir = None
        self.setup_logging()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_dir = self.project_dir / "test" / "claude_enhancer" / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler(
                    log_dir / f"test_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
                ),
                logging.StreamHandler(),
            ],
        )
        self.logger = logging.getLogger(__name__)

    @contextmanager
    def isolated_environment(self):
        """åˆ›å»ºéš”ç¦»çš„æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp(prefix="claude_test_")
        try:
            # å¤åˆ¶å¿…è¦çš„é…ç½®æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            test_claude_dir = Path(self.temp_dir) / ".claude"
            test_claude_dir.mkdir(parents=True)

            # å¤åˆ¶å…³é”®é…ç½®æ–‡ä»¶
            config_files = ["settings.json", "config.yaml"]
            for config_file in config_files:
                src = self.claude_dir / config_file
                if src.exists():
                    shutil.copy2(src, test_claude_dir / config_file)

            yield test_claude_dir
        finally:
            if self.temp_dir and Path(self.temp_dir).exists():
                shutil.rmtree(self.temp_dir, ignore_errors=True)

    def run_command_with_monitoring(
        self,
        command: str,
        timeout: float = 30.0,
        cwd: Optional[Path] = None,
        env: Optional[Dict[str, str]] = None,
    ) -> TestResult:
        """è¿è¡Œå‘½ä»¤å¹¶ç›‘æ§èµ„æºä½¿ç”¨"""

        monitor = SystemMonitor()
        start_time = time.perf_counter()

        try:
            # å¼€å§‹èµ„æºç›‘æ§
            monitor.start_monitoring()

            # å‡†å¤‡ç¯å¢ƒ
            test_env = os.environ.copy()
            if env:
                test_env.update(env)

            # è¿è¡Œå‘½ä»¤
            process = subprocess.Popen(
                command,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                cwd=cwd or self.project_dir,
                env=test_env,
            )

            try:
                stdout, stderr = process.communicate(timeout=timeout)
                return_code = process.returncode
                success = return_code == 0

            except subprocess.TimeoutExpired:
                process.terminate()
                try:
                    process.wait(timeout=5.0)
                except subprocess.TimeoutExpired:
                    process.kill()
                    process.wait()

                stdout, stderr = "TIMEOUT", "Command timed out"
                return_code = -1
                success = False

        except Exception as e:
            stdout, stderr = "", str(e)
            return_code = -1
            success = False

        finally:
            end_time = time.perf_counter()
            execution_time = end_time - start_time

            # åœæ­¢ç›‘æ§å¹¶è·å–ç»Ÿè®¡æ•°æ®
            system_stats = monitor.stop_monitoring()

        return TestResult(
            test_name=command,
            description=f"Command: {command}",
            success=success,
            execution_time=execution_time,
            memory_peak=system_stats.get("memory_mb_max"),
            cpu_percent=system_stats.get("cpu_percent_avg"),
            stdout=stdout,
            stderr=stderr,
            return_code=return_code,
            metadata=system_stats,
        )

    def verify_file_exists_and_executable(self, file_path: Path) -> bool:
        """éªŒè¯æ–‡ä»¶å­˜åœ¨ä¸”å¯æ‰§è¡Œ"""
        if not file_path.exists():
            self.logger.warning(f"File does not exist: {file_path}")
            return False

        if not os.access(file_path, os.R_OK):
            self.logger.warning(f"File not readable: {file_path}")
            return False

        # å¦‚æœæ˜¯è„šæœ¬æ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦å¯æ‰§è¡Œ
        if file_path.suffix in [".sh", ".py"]:
            if not os.access(file_path, os.X_OK):
                try:
                    os.chmod(file_path, 0o755)
                    self.logger.info(f"Made file executable: {file_path}")
                except OSError as e:
                    self.logger.error(
                        f"Failed to make file executable: {file_path}, error: {e}"
                    )
                    return False

        return True

    def test_hook_system_accuracy(self) -> List[TestResult]:
        """ç²¾ç¡®æµ‹è¯•Hookç³»ç»Ÿ"""
        results = []

        # è·å–æ‰€æœ‰å®é™…å­˜åœ¨çš„Hookæ–‡ä»¶
        hook_files = []
        hooks_dir = self.claude_dir / "hooks"

        if hooks_dir.exists():
            for hook_file in hooks_dir.glob("*.sh"):
                if self.verify_file_exists_and_executable(hook_file):
                    hook_files.append(hook_file)

        self.logger.info(f"Found {len(hook_files)} executable hook files")

        for hook_file in hook_files:
            # æµ‹è¯•Hookçš„åŸºæœ¬åŠŸèƒ½
            test_commands = [
                f"bash {hook_file} --help",
                f"bash {hook_file} --test",
                f"bash {hook_file} --dry-run",
            ]

            for cmd in test_commands:
                result = self.run_command_with_monitoring(
                    cmd,
                    timeout=10.0,
                    env={"CLAUDE_TEST_MODE": "1", "HOOK_TEST_MODE": "1"},
                )
                result.test_name = f"hook_{hook_file.name}_{cmd.split()[-1]}"
                result.description = (
                    f"Testing {hook_file.name} with {cmd.split()[-1]} flag"
                )
                results.append(result)

        return results

    def test_performance_scripts_accuracy(self) -> List[TestResult]:
        """ç²¾ç¡®æµ‹è¯•æ€§èƒ½è„šæœ¬"""
        results = []

        scripts_dir = self.claude_dir / "scripts"
        performance_scripts = []

        if scripts_dir.exists():
            # æŸ¥æ‰¾æ‰€æœ‰æ€§èƒ½ç›¸å…³çš„è„šæœ¬
            for script_file in scripts_dir.glob("*performance*.sh"):
                if self.verify_file_exists_and_executable(script_file):
                    performance_scripts.append(script_file)

            for script_file in scripts_dir.glob("*benchmark*.sh"):
                if self.verify_file_exists_and_executable(script_file):
                    performance_scripts.append(script_file)

        self.logger.info(f"Found {len(performance_scripts)} performance script files")

        for script_file in performance_scripts:
            # ä½¿ç”¨éš”ç¦»ç¯å¢ƒæµ‹è¯•
            with self.isolated_environment() as test_env_dir:
                result = self.run_command_with_monitoring(
                    f"bash {script_file}",
                    timeout=30.0,
                    env={
                        "CLAUDE_TEST_MODE": "1",
                        "PERFORMANCE_TEST_MODE": "1",
                        "CLAUDE_DIR": str(test_env_dir),
                    },
                )
                result.test_name = f"perf_script_{script_file.name}"
                result.description = f"Performance test of {script_file.name}"
                results.append(result)

        return results

    def run_baseline_benchmarks(self) -> BenchmarkSuite:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        baseline_tests = []

        # åŸºæœ¬ç³»ç»Ÿæ“ä½œåŸºå‡†
        basic_operations = [
            ("echo 'test'", "Basic shell command"),
            ("ls /tmp", "Directory listing"),
            ("date", "System date"),
            ("whoami", "User identification"),
            ("pwd", "Working directory"),
        ]

        for cmd, desc in basic_operations:
            result = self.run_command_with_monitoring(cmd, timeout=5.0)
            result.test_name = f"baseline_{cmd.replace(' ', '_').replace(chr(39), '')}"
            result.description = desc
            baseline_tests.append(result)

        # æ–‡ä»¶ç³»ç»Ÿæ“ä½œåŸºå‡†
        with tempfile.TemporaryDirectory() as temp_dir:
            file_operations = [
                (f"touch {temp_dir}/test_file", "File creation"),
                (f"echo 'test content' > {temp_dir}/test_file", "File write"),
                (f"cat {temp_dir}/test_file", "File read"),
                (f"rm {temp_dir}/test_file", "File deletion"),
            ]

            for cmd, desc in file_operations:
                result = self.run_command_with_monitoring(cmd, timeout=5.0)
                result.test_name = f"fs_baseline_{desc.lower().replace(' ', '_')}"
                result.description = desc
                baseline_tests.append(result)

        return BenchmarkSuite(
            name="Baseline System Operations",
            description="Basic system operations to establish performance baseline",
            tests=baseline_tests,
            baseline_time=statistics.mean(
                [t.execution_time for t in baseline_tests if t.success]
            ),
        )

    def run_stress_tests(self, iterations: int = 10) -> List[TestResult]:
        """è¿è¡Œå‹åŠ›æµ‹è¯•"""
        stress_results = []

        # Hookç³»ç»Ÿå‹åŠ›æµ‹è¯•
        hook_files = list((self.claude_dir / "hooks").glob("*.sh"))
        executable_hooks = [
            h for h in hook_files if self.verify_file_exists_and_executable(h)
        ]

        if executable_hooks:
            # é€‰æ‹©ä¸€ä¸ªè½»é‡çº§çš„Hookè¿›è¡Œå‹åŠ›æµ‹è¯•
            test_hook = executable_hooks[0]

            for i in range(iterations):
                result = self.run_command_with_monitoring(
                    f"bash {test_hook} --test",
                    timeout=5.0,
                    env={"CLAUDE_TEST_MODE": "1", "STRESS_TEST_ITERATION": str(i)},
                )
                result.test_name = f"stress_hook_{i:03d}"
                result.description = f"Stress test iteration {i+1}/{iterations}"
                stress_results.append(result)

        return stress_results

    def analyze_test_consistency(self, results: List[TestResult]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœçš„ä¸€è‡´æ€§"""
        analysis = {
            "total_tests": len(results),
            "successful_tests": sum(1 for r in results if r.success),
            "failed_tests": sum(1 for r in results if not r.success),
            "success_rate": 0.0,
            "execution_times": [],
            "consistency_score": 0.0,
            "outliers": [],
        }

        if analysis["total_tests"] > 0:
            analysis["success_rate"] = (
                analysis["successful_tests"] / analysis["total_tests"]
            )

        # åˆ†ææ‰§è¡Œæ—¶é—´
        execution_times = [r.execution_time for r in results if r.success]
        if execution_times:
            analysis["execution_times"] = {
                "mean": statistics.mean(execution_times),
                "median": statistics.median(execution_times),
                "std_dev": statistics.stdev(execution_times)
                if len(execution_times) > 1
                else 0,
                "min": min(execution_times),
                "max": max(execution_times),
            }

            # æ£€æµ‹å¼‚å¸¸å€¼ï¼ˆè¶…å‡º2ä¸ªæ ‡å‡†å·®ï¼‰
            if len(execution_times) > 1:
                mean_time = analysis["execution_times"]["mean"]
                std_dev = analysis["execution_times"]["std_dev"]

                for i, result in enumerate(results):
                    if (
                        result.success
                        and abs(result.execution_time - mean_time) > 2 * std_dev
                    ):
                        analysis["outliers"].append(
                            {
                                "test_name": result.test_name,
                                "execution_time": result.execution_time,
                                "deviation": abs(result.execution_time - mean_time)
                                / std_dev,
                            }
                        )

            # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°ï¼ˆåŸºäºæ ‡å‡†å·®ï¼‰
            if analysis["execution_times"]["mean"] > 0:
                cv = (
                    analysis["execution_times"]["std_dev"]
                    / analysis["execution_times"]["mean"]
                )
                analysis["consistency_score"] = max(0, 1 - cv)  # å˜å¼‚ç³»æ•°è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜

        return analysis


class ComprehensiveTestSuite:
    """ç»¼åˆæµ‹è¯•å¥—ä»¶"""

    def __init__(self, claude_dir: str = "/home/xx/dev/Claude_Enhancer/.claude"):
        self.runner = AccurateTestRunner(claude_dir)
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "test_environment": self.get_test_environment(),
            "baseline_suite": None,
            "hook_tests": [],
            "performance_tests": [],
            "stress_tests": [],
            "unit_tests": [],
            "integration_tests": [],
            "analysis": {},
            "recommendations": [],
        }

    def get_test_environment(self) -> Dict[str, Any]:
        """è·å–æµ‹è¯•ç¯å¢ƒä¿¡æ¯"""
        return {
            "python_version": sys.version,
            "platform": sys.platform,
            "cpu_count": os.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "disk_free_gb": psutil.disk_usage("/").free / (1024**3),
            "load_average": os.getloadavg() if hasattr(os, "getloadavg") else None,
        }

    def run_unit_tests(self) -> List[TestResult]:
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        unit_results = []

        # æµ‹è¯•é…ç½®æ–‡ä»¶è§£æ
        config_files = [
            self.runner.claude_dir / "settings.json",
            self.runner.claude_dir / "config.yaml",
        ]

        for config_file in config_files:
            if config_file.exists():
                if config_file.suffix == ".json":
                    cmd = (
                        f"python3 -c 'import json; json.load(open(\"{config_file}\"))'"
                    )
                elif config_file.suffix in [".yaml", ".yml"]:
                    cmd = f"python3 -c 'import yaml; yaml.safe_load(open(\"{config_file}\"))'"
                else:
                    continue

                result = self.runner.run_command_with_monitoring(cmd, timeout=5.0)
                result.test_name = f"unit_config_parse_{config_file.name}"
                result.description = f"Parse configuration file {config_file.name}"
                unit_results.append(result)

        return unit_results

    def run_integration_tests(self) -> List[TestResult]:
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        integration_results = []

        # æµ‹è¯•Hookç³»ç»Ÿä¸é…ç½®çš„é›†æˆ
        settings_file = self.runner.claude_dir / "settings.json"
        if settings_file.exists():
            # éªŒè¯Hooké…ç½®çš„å®Œæ•´æ€§
            cmd = f"""
            python3 -c "
import json
import os
settings_path = '{settings_file}'
if os.path.exists(settings_path):
    with open(settings_path) as f:
        settings = json.load(f)
    hooks = settings.get('hooks', {{}})
    for hook_type, hook_list in hooks.items():
        if isinstance(hook_list, list):
            for hook in hook_list:
                if 'command' in hook:
                    print(f'Hook found: {{hook_type}} -> {{hook[\"command\"]}}')
print('Integration test completed')
"
            """

            result = self.runner.run_command_with_monitoring(cmd, timeout=10.0)
            result.test_name = "integration_hook_config"
            result.description = "Verify hook system configuration integration"
            integration_results.append(result)

        return integration_results

    def run_full_test_suite(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶"""
        print("ğŸ§ª å¼€å§‹è¿è¡Œç»¼åˆæµ‹è¯•å¥—ä»¶...")
        print("=" * 80)

        # 1. å»ºç«‹åŸºå‡†çº¿
        print("ğŸ“Š å»ºç«‹æ€§èƒ½åŸºå‡†çº¿...")
        self.results["baseline_suite"] = self.runner.run_baseline_benchmarks()

        # 2. å•å…ƒæµ‹è¯•
        print("ğŸ”¬ è¿è¡Œå•å…ƒæµ‹è¯•...")
        self.results["unit_tests"] = self.run_unit_tests()

        # 3. Hookç³»ç»Ÿæµ‹è¯•
        print("ğŸ”— æµ‹è¯•Hookç³»ç»Ÿ...")
        self.results["hook_tests"] = self.runner.test_hook_system_accuracy()

        # 4. æ€§èƒ½è„šæœ¬æµ‹è¯•
        print("âš¡ æµ‹è¯•æ€§èƒ½è„šæœ¬...")
        self.results[
            "performance_tests"
        ] = self.runner.test_performance_scripts_accuracy()

        # 5. é›†æˆæµ‹è¯•
        print("ğŸ”„ è¿è¡Œé›†æˆæµ‹è¯•...")
        self.results["integration_tests"] = self.run_integration_tests()

        # 6. å‹åŠ›æµ‹è¯•
        print("ğŸ’ª è¿è¡Œå‹åŠ›æµ‹è¯•...")
        self.results["stress_tests"] = self.runner.run_stress_tests(iterations=5)

        # 7. åˆ†æç»“æœ
        print("ğŸ“ˆ åˆ†ææµ‹è¯•ç»“æœ...")
        self.analyze_all_results()

        # 8. ç”Ÿæˆå»ºè®®
        print("ğŸ’¡ ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
        self.generate_recommendations()

        print("âœ… æµ‹è¯•å¥—ä»¶æ‰§è¡Œå®Œæˆ!")
        return self.results

    def analyze_all_results(self):
        """åˆ†ææ‰€æœ‰æµ‹è¯•ç»“æœ"""
        all_tests = []

        # æ”¶é›†æ‰€æœ‰æµ‹è¯•ç»“æœ
        if self.results["baseline_suite"]:
            all_tests.extend(self.results["baseline_suite"].tests)
        all_tests.extend(self.results["unit_tests"])
        all_tests.extend(self.results["hook_tests"])
        all_tests.extend(self.results["performance_tests"])
        all_tests.extend(self.results["integration_tests"])
        all_tests.extend(self.results["stress_tests"])

        # æ€»ä½“åˆ†æ
        self.results["analysis"]["overall"] = self.runner.analyze_test_consistency(
            all_tests
        )

        # åˆ†ç±»åˆ†æ
        categories = {
            "unit_tests": self.results["unit_tests"],
            "hook_tests": self.results["hook_tests"],
            "performance_tests": self.results["performance_tests"],
            "integration_tests": self.results["integration_tests"],
            "stress_tests": self.results["stress_tests"],
        }

        for category, tests in categories.items():
            if tests:
                self.results["analysis"][
                    category
                ] = self.runner.analyze_test_consistency(tests)

    def generate_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        overall_analysis = self.results["analysis"].get("overall", {})

        # æˆåŠŸç‡å»ºè®®
        success_rate = overall_analysis.get("success_rate", 0)
        if success_rate < 0.9:
            recommendations.append(
                {
                    "category": "Reliability",
                    "priority": "HIGH",
                    "issue": f"æ•´ä½“æµ‹è¯•æˆåŠŸç‡è¾ƒä½: {success_rate:.1%}",
                    "recommendation": "éœ€è¦æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ï¼Œä¿®å¤æ½œåœ¨é—®é¢˜",
                    "action_items": ["æ£€æŸ¥æµ‹è¯•ç¯å¢ƒé…ç½®", "éªŒè¯ä¾èµ–é¡¹æ˜¯å¦é½å…¨", "ä¿®å¤å¤±è´¥çš„è„šæœ¬æˆ–é…ç½®"],
                }
            )

        # æ€§èƒ½ä¸€è‡´æ€§å»ºè®®
        consistency_score = overall_analysis.get("consistency_score", 0)
        if consistency_score < 0.8:
            recommendations.append(
                {
                    "category": "Performance Consistency",
                    "priority": "MEDIUM",
                    "issue": f"æµ‹è¯•æ‰§è¡Œæ—¶é—´å˜åŒ–è¾ƒå¤§ï¼Œä¸€è‡´æ€§åˆ†æ•°: {consistency_score:.2f}",
                    "recommendation": "ä¼˜åŒ–æµ‹è¯•ç¯å¢ƒç¨³å®šæ€§ï¼Œå‡å°‘æ€§èƒ½æ³¢åŠ¨",
                    "action_items": ["æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½", "ä¼˜åŒ–èµ„æºäº‰ç”¨", "è€ƒè™‘ä½¿ç”¨æ›´ç¨³å®šçš„æµ‹è¯•ç¯å¢ƒ"],
                }
            )

        # å¼‚å¸¸å€¼å»ºè®®
        outliers = overall_analysis.get("outliers", [])
        if outliers:
            recommendations.append(
                {
                    "category": "Performance Outliers",
                    "priority": "MEDIUM",
                    "issue": f"å‘ç° {len(outliers)} ä¸ªæ‰§è¡Œæ—¶é—´å¼‚å¸¸çš„æµ‹è¯•",
                    "recommendation": "è°ƒæŸ¥å¼‚å¸¸æµ‹è¯•ç”¨ä¾‹ï¼Œä¼˜åŒ–æ€§èƒ½ç“¶é¢ˆ",
                    "details": outliers[:5],  # åªæ˜¾ç¤ºå‰5ä¸ª
                    "action_items": ["åˆ†æå¼‚å¸¸æµ‹è¯•çš„å…·ä½“åŸå› ", "ä¼˜åŒ–æ…¢é€Ÿæ“ä½œ", "è€ƒè™‘å¢åŠ è¶…æ—¶å¤„ç†"],
                }
            )

        self.results["recommendations"] = recommendations

    def save_detailed_report(
        self, filename: str = "comprehensive_test_report.json"
    ) -> Path:
        """ä¿å­˜è¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š"""
        report_path = self.runner.project_dir / "test" / "claude_enhancer" / filename
        report_path.parent.mkdir(parents=True, exist_ok=True)

        # è½¬æ¢TestResultå¯¹è±¡ä¸ºå­—å…¸
        serializable_results = {}

        for key, value in self.results.items():
            if key == "baseline_suite" and value:
                serializable_results[key] = {
                    "name": value.name,
                    "description": value.description,
                    "tests": [asdict(test) for test in value.tests],
                    "baseline_time": value.baseline_time,
                    "performance_target": value.performance_target,
                }
            elif (
                isinstance(value, list)
                and value
                and hasattr(value[0], "__dataclass_fields__")
            ):
                serializable_results[key] = [asdict(item) for item in value]
            else:
                serializable_results[key] = value

        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ è¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        return report_path


def main():
    """ä¸»å‡½æ•°"""
    try:
        suite = ComprehensiveTestSuite()
        results = suite.run_full_test_suite()
        report_path = suite.save_detailed_report()

        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 80)
        print("ğŸ“‹ æµ‹è¯•æ‘˜è¦")
        print("=" * 80)

        overall = results["analysis"].get("overall", {})
        print(f"æ€»æµ‹è¯•æ•°é‡: {overall.get('total_tests', 0)}")
        print(f"æˆåŠŸæµ‹è¯•: {overall.get('successful_tests', 0)}")
        print(f"å¤±è´¥æµ‹è¯•: {overall.get('failed_tests', 0)}")
        print(f"æˆåŠŸç‡: {overall.get('success_rate', 0):.1%}")

        exec_times = overall.get("execution_times", {})
        if exec_times:
            print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {exec_times.get('mean', 0):.4f}s")
            print(
                f"æ‰§è¡Œæ—¶é—´èŒƒå›´: {exec_times.get('min', 0):.4f}s - {exec_times.get('max', 0):.4f}s"
            )
            print(f"ä¸€è‡´æ€§åˆ†æ•°: {overall.get('consistency_score', 0):.2f}")

        print(f"\nğŸ’¡ ç”Ÿæˆäº† {len(results.get('recommendations', []))} æ¡ä¼˜åŒ–å»ºè®®")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_path}")

        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¥—ä»¶æ‰§è¡Œå¤±è´¥: {e}")
        logging.exception("Test suite execution failed")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
