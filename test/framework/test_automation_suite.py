#!/usr/bin/env python3
"""
Perfect21 è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶
é«˜è´¨é‡æµ‹è¯•æ¡†æ¶ - åƒä¸“ä¸šçš„è´¨é‡æ£€æŸ¥å‘˜å›¢é˜Ÿ
"""

import os
import sys
import time
import json
import asyncio
import logging
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import importlib
import traceback

# æµ‹è¯•æ¡†æ¶ä¾èµ–
import pytest
import coverage
from memory_profiler import profile
import psutil


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    module: str
    test_name: str
    status: str  # "passed", "failed", "skipped", "error"
    duration: float
    memory_usage: float
    error_message: Optional[str] = None
    coverage_data: Optional[Dict] = None


@dataclass
class TestSuiteMetrics:
    """æµ‹è¯•å¥—ä»¶æŒ‡æ ‡"""
    total_tests: int
    passed: int
    failed: int
    skipped: int
    errors: int
    total_duration: float
    coverage_percentage: float
    memory_peak: float
    success_rate: float


class TestLogger:
    """æµ‹è¯•æ—¥å¿—ç®¡ç†å™¨"""

    def __init__(self, log_file: Path):
        self.log_file = log_file
        self.logger = self._setup_logger()

    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        logger = logging.getLogger('perfect21_test_suite')
        logger.setLevel(logging.INFO)

        # åˆ›å»ºæ—¥å¿—ç›®å½•
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(self.log_file)
        file_handler.setLevel(logging.INFO)

        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)

        # æ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)

        logger.addHandler(file_handler)
        logger.addHandler(console_handler)

        return logger

    def info(self, message: str):
        """è®°å½•ä¿¡æ¯æ—¥å¿—"""
        self.logger.info(message)

    def error(self, message: str):
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        self.logger.error(message)

    def warning(self, message: str):
        """è®°å½•è­¦å‘Šæ—¥å¿—"""
        self.logger.warning(message)


class CoverageAnalyzer:
    """ä»£ç è¦†ç›–ç‡åˆ†æå™¨"""

    def __init__(self, source_dirs: List[str]):
        self.source_dirs = source_dirs
        self.cov = coverage.Coverage()

    def start_coverage(self):
        """å¼€å§‹è¦†ç›–ç‡æ”¶é›†"""
        self.cov.start()

    def stop_coverage(self):
        """åœæ­¢è¦†ç›–ç‡æ”¶é›†"""
        self.cov.stop()
        self.cov.save()

    def generate_report(self, output_dir: Path) -> Dict[str, Any]:
        """ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š"""
        output_dir.mkdir(parents=True, exist_ok=True)

        # HTMLæŠ¥å‘Š
        html_dir = output_dir / "coverage_html"
        self.cov.html_report(directory=str(html_dir))

        # XMLæŠ¥å‘Š (for CI/CD)
        xml_file = output_dir / "coverage.xml"
        self.cov.xml_report(outfile=str(xml_file))

        # JSONæŠ¥å‘Š
        json_file = output_dir / "coverage.json"
        self.cov.json_report(outfile=str(json_file))

        # è·å–è¦†ç›–ç‡æ•°æ®
        total_coverage = self.cov.report()

        return {
            "total_coverage": total_coverage,
            "html_report": str(html_dir),
            "xml_report": str(xml_file),
            "json_report": str(json_file)
        }


class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.process = psutil.Process()
        self.start_time = None
        self.memory_samples = []

    def start_profiling(self):
        """å¼€å§‹æ€§èƒ½åˆ†æ"""
        self.start_time = time.time()
        self.memory_samples = []
        self._sample_memory()

    def _sample_memory(self):
        """é‡‡æ ·å†…å­˜ä½¿ç”¨"""
        try:
            memory_info = self.process.memory_info()
            self.memory_samples.append(memory_info.rss / 1024 / 1024)  # MB
        except:
            pass

    def stop_profiling(self) -> Dict[str, float]:
        """åœæ­¢æ€§èƒ½åˆ†æå¹¶è¿”å›ç»“æœ"""
        end_time = time.time()
        self._sample_memory()

        duration = end_time - self.start_time if self.start_time else 0
        peak_memory = max(self.memory_samples) if self.memory_samples else 0

        return {
            "duration": duration,
            "peak_memory_mb": peak_memory,
            "avg_memory_mb": sum(self.memory_samples) / len(self.memory_samples) if self.memory_samples else 0
        }


class TestModuleRunner:
    """æµ‹è¯•æ¨¡å—è¿è¡Œå™¨"""

    def __init__(self, logger: TestLogger):
        self.logger = logger
        self.results = []

    def run_pytest_module(self, module_path: Path, test_config: Dict) -> List[TestResult]:
        """è¿è¡Œpytestæ¨¡å—"""
        self.logger.info(f"ğŸ§ª è¿è¡Œæµ‹è¯•æ¨¡å—: {module_path}")

        # æ„å»ºpytestå‘½ä»¤
        cmd = [
            sys.executable, "-m", "pytest",
            str(module_path),
            "-v",
            "--tb=short",
            "--json-report",
            f"--json-report-file={module_path.parent}/test-results.json"
        ]

        # æ·»åŠ è¦†ç›–ç‡å‚æ•°
        if test_config.get("coverage", True):
            cmd.extend([
                f"--cov={test_config.get('source_dir', 'src')}",
                "--cov-report=term-missing"
            ])

        # æ·»åŠ æ€§èƒ½æ ‡è®°
        if test_config.get("performance_tests", False):
            cmd.append("-m performance")

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=test_config.get("timeout", 300)
            )

            return self._parse_pytest_results(module_path, result)

        except subprocess.TimeoutExpired:
            self.logger.error(f"âŒ æµ‹è¯•æ¨¡å—è¶…æ—¶: {module_path}")
            return [TestResult(
                module=str(module_path),
                test_name="module_timeout",
                status="error",
                duration=test_config.get("timeout", 300),
                memory_usage=0,
                error_message="Test module timed out"
            )]

    def _parse_pytest_results(self, module_path: Path, result: subprocess.CompletedProcess) -> List[TestResult]:
        """è§£æpytestç»“æœ"""
        test_results = []

        # å°è¯•è§£æJSONæŠ¥å‘Š
        json_report_path = module_path.parent / "test-results.json"
        if json_report_path.exists():
            try:
                with open(json_report_path, 'r') as f:
                    report_data = json.load(f)

                for test in report_data.get("tests", []):
                    test_results.append(TestResult(
                        module=str(module_path),
                        test_name=test["nodeid"],
                        status=test["outcome"],
                        duration=test.get("duration", 0),
                        memory_usage=0,  # TODO: ä»å†…å­˜åˆ†æå™¨è·å–
                        error_message=test.get("call", {}).get("longrepr", None) if test["outcome"] == "failed" else None
                    ))
            except Exception as e:
                self.logger.error(f"è§£ææµ‹è¯•ç»“æœå¤±è´¥: {e}")

        # å¦‚æœæ²¡æœ‰JSONæŠ¥å‘Šï¼Œä»è¾“å‡ºè§£æ
        if not test_results:
            test_results = self._parse_stdout_results(module_path, result)

        return test_results

    def _parse_stdout_results(self, module_path: Path, result: subprocess.CompletedProcess) -> List[TestResult]:
        """ä»æ ‡å‡†è¾“å‡ºè§£æç»“æœ"""
        lines = result.stdout.split('\n')
        test_results = []

        for line in lines:
            if '::' in line and any(status in line for status in ['PASSED', 'FAILED', 'SKIPPED', 'ERROR']):
                parts = line.split()
                if len(parts) >= 2:
                    test_name = parts[0]
                    status = parts[1].lower()

                    test_results.append(TestResult(
                        module=str(module_path),
                        test_name=test_name,
                        status=status,
                        duration=0,  # æ— æ³•ä»è¾“å‡ºè·å–
                        memory_usage=0,
                        error_message=None
                    ))

        return test_results


class TestSuiteRunner:
    """æµ‹è¯•å¥—ä»¶ä¸»è¿è¡Œå™¨"""

    def __init__(self, project_root: Path, config: Dict):
        self.project_root = project_root
        self.config = config
        self.logger = TestLogger(project_root / "test-results" / "test-suite.log")
        self.coverage_analyzer = CoverageAnalyzer(config.get("source_dirs", ["src"]))
        self.performance_profiler = PerformanceProfiler()
        self.module_runner = TestModuleRunner(self.logger)
        self.all_results = []

    def run_complete_suite(self) -> TestSuiteMetrics:
        """è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶"""
        self.logger.info("ğŸš€ å¼€å§‹Perfect21å…¨é¢æµ‹è¯•å¥—ä»¶æ‰§è¡Œ")
        self.logger.info(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")

        # å¼€å§‹æ€§èƒ½åˆ†æå’Œè¦†ç›–ç‡æ”¶é›†
        self.performance_profiler.start_profiling()
        self.coverage_analyzer.start_coverage()

        try:
            # æŒ‰ä¼˜å…ˆçº§è¿è¡Œæµ‹è¯•
            self._run_unit_tests()
            self._run_integration_tests()
            self._run_security_tests()
            self._run_performance_tests()
            self._run_e2e_tests()

        except Exception as e:
            self.logger.error(f"ğŸ’¥ æµ‹è¯•å¥—ä»¶æ‰§è¡Œå¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())

        finally:
            # åœæ­¢åˆ†æ
            self.coverage_analyzer.stop_coverage()
            perf_data = self.performance_profiler.stop_profiling()

        # ç”ŸæˆæŠ¥å‘Š
        metrics = self._calculate_metrics(perf_data)
        self._generate_comprehensive_report(metrics)

        return metrics

    def _run_unit_tests(self):
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        self.logger.info("ğŸ§ª æ‰§è¡Œå•å…ƒæµ‹è¯•å¥—ä»¶...")

        unit_test_dirs = [
            self.project_root / "test" / "unit",
            self.project_root / "test" / "auth" / "unit"
        ]

        self._run_test_categories(unit_test_dirs, {
            "timeout": 300,
            "coverage": True,
            "parallel": True
        })

    def _run_integration_tests(self):
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        self.logger.info("ğŸ”— æ‰§è¡Œé›†æˆæµ‹è¯•å¥—ä»¶...")

        integration_test_dirs = [
            self.project_root / "test" / "integration",
            self.project_root / "test" / "auth" / "integration"
        ]

        self._run_test_categories(integration_test_dirs, {
            "timeout": 600,
            "coverage": True,
            "setup_required": True
        })

    def _run_security_tests(self):
        """è¿è¡Œå®‰å…¨æµ‹è¯•"""
        self.logger.info("ğŸ”’ æ‰§è¡Œå®‰å…¨æµ‹è¯•å¥—ä»¶...")

        security_test_dirs = [
            self.project_root / "test" / "security",
            self.project_root / "test" / "auth" / "security"
        ]

        # è¿è¡Œå®‰å…¨æµ‹è¯•
        self._run_test_categories(security_test_dirs, {
            "timeout": 300,
            "security_focus": True
        })

        # è¿è¡Œå®‰å…¨æ‰«æ
        self._run_security_scans()

    def _run_performance_tests(self):
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        self.logger.info("âš¡ æ‰§è¡Œæ€§èƒ½æµ‹è¯•å¥—ä»¶...")

        performance_test_dirs = [
            self.project_root / "test" / "performance",
            self.project_root / "test" / "auth" / "performance"
        ]

        self._run_test_categories(performance_test_dirs, {
            "timeout": 900,
            "performance_tests": True,
            "benchmark": True
        })

    def _run_e2e_tests(self):
        """è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•"""
        self.logger.info("ğŸ¯ æ‰§è¡Œç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶...")

        e2e_test_dirs = [
            self.project_root / "test" / "e2e"
        ]

        self._run_test_categories(e2e_test_dirs, {
            "timeout": 1800,
            "browser_required": True
        })

    def _run_test_categories(self, test_dirs: List[Path], config: Dict):
        """è¿è¡Œæµ‹è¯•ç±»åˆ«"""
        for test_dir in test_dirs:
            if test_dir.exists():
                test_files = list(test_dir.glob("test_*.py"))
                if test_files:
                    self.logger.info(f"ğŸ“‚ è¿è¡Œæµ‹è¯•ç›®å½•: {test_dir}")

                    for test_file in test_files:
                        results = self.module_runner.run_pytest_module(test_file, config)
                        self.all_results.extend(results)
                else:
                    self.logger.warning(f"âš ï¸  æµ‹è¯•ç›®å½•ä¸ºç©º: {test_dir}")
            else:
                self.logger.warning(f"âš ï¸  æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {test_dir}")

    def _run_security_scans(self):
        """è¿è¡Œå®‰å…¨æ‰«æå·¥å…·"""
        self.logger.info("ğŸ›¡ï¸  è¿è¡Œå®‰å…¨æ‰«æ...")

        scans = [
            {
                "name": "Banditå®‰å…¨æ‰«æ",
                "cmd": ["bandit", "-r", ".", "-f", "json", "-o", "test-results/bandit-report.json"]
            },
            {
                "name": "Safetyä¾èµ–æ£€æŸ¥",
                "cmd": ["safety", "check", "--json", "--output", "test-results/safety-report.json"]
            }
        ]

        for scan in scans:
            try:
                self.logger.info(f"ğŸ” æ‰§è¡Œ: {scan['name']}")
                subprocess.run(scan["cmd"], cwd=self.project_root, timeout=300)
                self.logger.info(f"âœ… {scan['name']} å®Œæˆ")
            except Exception as e:
                self.logger.error(f"âŒ {scan['name']} å¤±è´¥: {e}")

    def _calculate_metrics(self, perf_data: Dict) -> TestSuiteMetrics:
        """è®¡ç®—æµ‹è¯•æŒ‡æ ‡"""
        total_tests = len(self.all_results)
        passed = len([r for r in self.all_results if r.status == "passed"])
        failed = len([r for r in self.all_results if r.status == "failed"])
        skipped = len([r for r in self.all_results if r.status == "skipped"])
        errors = len([r for r in self.all_results if r.status == "error"])

        total_duration = sum(r.duration for r in self.all_results)
        success_rate = (passed / total_tests * 100) if total_tests > 0 else 0

        # ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
        coverage_report = self.coverage_analyzer.generate_report(
            self.project_root / "test-results" / "coverage"
        )

        return TestSuiteMetrics(
            total_tests=total_tests,
            passed=passed,
            failed=failed,
            skipped=skipped,
            errors=errors,
            total_duration=total_duration,
            coverage_percentage=coverage_report.get("total_coverage", 0),
            memory_peak=perf_data["peak_memory_mb"],
            success_rate=success_rate
        )

    def _generate_comprehensive_report(self, metrics: TestSuiteMetrics):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        report_dir = self.project_root / "test-results"
        report_dir.mkdir(parents=True, exist_ok=True)

        # ç”ŸæˆJSONæŠ¥å‘Š
        json_report = {
            "timestamp": time.time(),
            "metrics": asdict(metrics),
            "detailed_results": [asdict(r) for r in self.all_results],
            "environment": {
                "python_version": sys.version,
                "platform": sys.platform,
                "working_directory": str(self.project_root)
            }
        }

        with open(report_dir / "comprehensive-report.json", "w") as f:
            json.dump(json_report, f, indent=2)

        # ç”ŸæˆHTMLæŠ¥å‘Š
        self._generate_html_report(metrics, report_dir)

        # æ‰“å°æ‘˜è¦
        self._print_summary(metrics)

    def _generate_html_report(self, metrics: TestSuiteMetrics, report_dir: Path):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Perfect21 æµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #2196F3; color: white; padding: 20px; border-radius: 5px; }}
        .metrics {{ display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px; margin: 20px 0; }}
        .metric {{ background: #f5f5f5; padding: 15px; border-radius: 5px; text-align: center; }}
        .metric h3 {{ margin: 0; color: #333; }}
        .metric .value {{ font-size: 24px; font-weight: bold; color: #2196F3; }}
        .status-passed {{ color: #4CAF50; }}
        .status-failed {{ color: #F44336; }}
        .status-skipped {{ color: #FF9800; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ§ª Perfect21 ç»¼åˆæµ‹è¯•æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>

    <div class="metrics">
        <div class="metric">
            <h3>æ€»æµ‹è¯•æ•°</h3>
            <div class="value">{metrics.total_tests}</div>
        </div>
        <div class="metric">
            <h3>æˆåŠŸç‡</h3>
            <div class="value">{metrics.success_rate:.1f}%</div>
        </div>
        <div class="metric">
            <h3>è¦†ç›–ç‡</h3>
            <div class="value">{metrics.coverage_percentage:.1f}%</div>
        </div>
        <div class="metric">
            <h3>æ‰§è¡Œæ—¶é—´</h3>
            <div class="value">{metrics.total_duration:.1f}s</div>
        </div>
    </div>

    <h2>ğŸ“Š æµ‹è¯•ç»“æœè¯¦æƒ…</h2>
    <table>
        <tr>
            <th>çŠ¶æ€</th>
            <th>æ•°é‡</th>
            <th>ç™¾åˆ†æ¯”</th>
        </tr>
        <tr class="status-passed">
            <td>âœ… é€šè¿‡</td>
            <td>{metrics.passed}</td>
            <td>{(metrics.passed/metrics.total_tests*100):.1f}%</td>
        </tr>
        <tr class="status-failed">
            <td>âŒ å¤±è´¥</td>
            <td>{metrics.failed}</td>
            <td>{(metrics.failed/metrics.total_tests*100):.1f}%</td>
        </tr>
        <tr class="status-skipped">
            <td>â­ï¸ è·³è¿‡</td>
            <td>{metrics.skipped}</td>
            <td>{(metrics.skipped/metrics.total_tests*100):.1f}%</td>
        </tr>
    </table>

    <h2>ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡</h2>
    <p>å³°å€¼å†…å­˜ä½¿ç”¨: {metrics.memory_peak:.1f} MB</p>
    <p>å¹³å‡æµ‹è¯•æ‰§è¡Œæ—¶é—´: {(metrics.total_duration/metrics.total_tests):.2f}s</p>

</body>
</html>
        """

        with open(report_dir / "test-report.html", "w", encoding="utf-8") as f:
            f.write(html_content)

    def _print_summary(self, metrics: TestSuiteMetrics):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ¯ Perfect21 æµ‹è¯•å¥—ä»¶æ‰§è¡Œå®Œæˆ")
        print("="*60)

        if metrics.success_rate >= 95:
            print("ğŸ‰ æµ‹è¯•ç»“æœ: ä¼˜ç§€")
        elif metrics.success_rate >= 85:
            print("âœ… æµ‹è¯•ç»“æœ: è‰¯å¥½")
        else:
            print("âš ï¸  æµ‹è¯•ç»“æœ: éœ€è¦æ”¹è¿›")

        print(f"ğŸ“Š æ€»è®¡: {metrics.total_tests} ä¸ªæµ‹è¯•")
        print(f"âœ… é€šè¿‡: {metrics.passed} ä¸ª ({metrics.passed/metrics.total_tests*100:.1f}%)")
        print(f"âŒ å¤±è´¥: {metrics.failed} ä¸ª ({metrics.failed/metrics.total_tests*100:.1f}%)")
        print(f"â­ï¸  è·³è¿‡: {metrics.skipped} ä¸ª ({metrics.skipped/metrics.total_tests*100:.1f}%)")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {metrics.success_rate:.1f}%")
        print(f"ğŸ“Š è¦†ç›–ç‡: {metrics.coverage_percentage:.1f}%")
        print(f"â±ï¸  æ€»è€—æ—¶: {metrics.total_duration:.1f}s")
        print(f"ğŸ’¾ å³°å€¼å†…å­˜: {metrics.memory_peak:.1f}MB")
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Perfect21 è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶")
    parser.add_argument("--project-root", type=str, default=".", help="é¡¹ç›®æ ¹ç›®å½•")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--categories", nargs="+",
                       choices=["unit", "integration", "security", "performance", "e2e"],
                       help="è¦è¿è¡Œçš„æµ‹è¯•ç±»åˆ«")

    args = parser.parse_args()

    project_root = Path(args.project_root).resolve()

    # é»˜è®¤é…ç½®
    config = {
        "source_dirs": ["src", "backend", "auth-system"],
        "test_timeout": 1800,
        "coverage_threshold": 85,
        "performance_threshold": {
            "max_response_time": 200,
            "max_memory_usage": 512
        }
    }

    # åŠ è½½è‡ªå®šä¹‰é…ç½®
    if args.config and Path(args.config).exists():
        with open(args.config, 'r') as f:
            custom_config = json.load(f)
            config.update(custom_config)

    # è¿è¡Œæµ‹è¯•å¥—ä»¶
    runner = TestSuiteRunner(project_root, config)
    metrics = runner.run_complete_suite()

    # æ ¹æ®ç»“æœè®¾ç½®é€€å‡ºç 
    if metrics.success_rate >= 95 and metrics.coverage_percentage >= config["coverage_threshold"]:
        print("ğŸ‰ æ‰€æœ‰è´¨é‡æ ‡å‡†é€šè¿‡ï¼")
        sys.exit(0)
    elif metrics.failed == 0:
        print("âš ï¸  è´¨é‡æ ‡å‡†éƒ¨åˆ†é€šè¿‡")
        sys.exit(1)
    else:
        print("âŒ è´¨é‡æ ‡å‡†æœªé€šè¿‡")
        sys.exit(2)


if __name__ == "__main__":
    main()