#!/usr/bin/env python3
"""
Claude Enhancer 5.0 - æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸“ç”¨å·¥å…·
ä½œä¸ºtest-engineerè®¾è®¡çš„é«˜ç²¾åº¦æ€§èƒ½æµ‹è¯•æ¡†æ¶

ä¸»è¦åŠŸèƒ½:
1. å¾®ç§’çº§ç²¾åº¦çš„æ€§èƒ½æµ‹é‡
2. å†…å­˜ä½¿ç”¨ç›‘æ§å’Œæ³„æ¼æ£€æµ‹
3. CPUä½¿ç”¨ç‡åˆ†æ
4. å¹¶å‘æ€§èƒ½åŸºå‡†æµ‹è¯•
5. å›å½’æ€§èƒ½åˆ†æ
6. è‡ªåŠ¨åŒ–æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ
"""

import time
import psutil
import threading
import subprocess
import json
import statistics
import os
import sys
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import contextmanager
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""

    test_name: str
    execution_time_ms: float
    memory_usage_mb: float
    cpu_usage_percent: float
    iterations: int
    success_rate: float
    percentile_95: float
    percentile_99: float
    min_time: float
    max_time: float
    std_deviation: float


@dataclass
class BenchmarkConfig:
    """åŸºå‡†æµ‹è¯•é…ç½®"""

    max_execution_time_ms: float
    max_memory_usage_mb: float
    max_cpu_usage_percent: float
    min_success_rate: float
    iterations: int
    warmup_iterations: int


class PerformanceMonitor:
    """å®æ—¶æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.process = psutil.Process()
        self.baseline_memory = self.process.memory_info().rss / 1024 / 1024
        self.monitoring = False
        self.metrics = []

    @contextmanager
    def monitor_performance(self, sample_interval: float = 0.1):
        """æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        self.monitoring = True
        self.metrics = []

        def monitor_worker():
            while self.monitoring:
                try:
                    memory_mb = self.process.memory_info().rss / 1024 / 1024
                    cpu_percent = self.process.cpu_percent()

                    self.metrics.append(
                        {
                            "timestamp": time.time(),
                            "memory_mb": memory_mb,
                            "cpu_percent": cpu_percent,
                        }
                    )

                    time.sleep(sample_interval)
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    break

        monitor_thread = threading.Thread(target=monitor_worker, daemon=True)
        monitor_thread.start()

        try:
            yield self
        finally:
            self.monitoring = False
            monitor_thread.join(timeout=1.0)

    def get_peak_memory(self) -> float:
        """è·å–å³°å€¼å†…å­˜ä½¿ç”¨"""
        if not self.metrics:
            return 0.0
        return max(metric["memory_mb"] for metric in self.metrics)

    def get_average_cpu(self) -> float:
        """è·å–å¹³å‡CPUä½¿ç”¨ç‡"""
        if not self.metrics:
            return 0.0
        return statistics.mean(metric["cpu_percent"] for metric in self.metrics)

    def detect_memory_leak(self, threshold_mb: float = 10.0) -> bool:
        """æ£€æµ‹å†…å­˜æ³„æ¼"""
        if len(self.metrics) < 10:
            return False

        # è®¡ç®—å†…å­˜ä½¿ç”¨è¶‹åŠ¿
        memory_values = [metric["memory_mb"] for metric in self.metrics]
        start_avg = statistics.mean(memory_values[:5])
        end_avg = statistics.mean(memory_values[-5:])

        return (end_avg - start_avg) > threshold_mb


class HooksBenchmarkSuite:
    """Hooksæ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""

    def __init__(self, project_root: str):
        self.project_root = project_root
        self.hooks_dir = os.path.join(project_root, ".claude", "hooks")
        self.configs = {
            "quality_gate": BenchmarkConfig(
                max_execution_time_ms=100.0,
                max_memory_usage_mb=10.0,
                max_cpu_usage_percent=5.0,
                min_success_rate=0.98,
                iterations=100,
                warmup_iterations=10,
            ),
            "smart_agent_selector": BenchmarkConfig(
                max_execution_time_ms=50.0,
                max_memory_usage_mb=8.0,
                max_cpu_usage_percent=3.0,
                min_success_rate=0.98,
                iterations=100,
                warmup_iterations=10,
            ),
        }

    def benchmark_quality_gate(self) -> PerformanceMetrics:
        """åŸºå‡†æµ‹è¯•: quality_gate.shæ€§èƒ½"""
        script_path = os.path.join(self.hooks_dir, "quality_gate.sh")
        config = self.configs["quality_gate"]

        test_inputs = [
            '{"prompt": "å®ç°ç”¨æˆ·è®¤è¯ç³»ç»Ÿ"}',
            '{"prompt": "ä¿®å¤æ•°æ®åº“è¿æ¥é—®é¢˜"}',
            '{"prompt": "ä¼˜åŒ–å‰ç«¯æ€§èƒ½"}',
            '{"prompt": "æ·»åŠ å®‰å…¨æ£€æŸ¥"}',
            '{"prompt": "éƒ¨ç½²ç”Ÿäº§ç¯å¢ƒ"}',
        ]

        return self._run_hook_benchmark(
            script_path, "quality_gate", test_inputs, config
        )

    def benchmark_smart_agent_selector(self) -> PerformanceMetrics:
        """åŸºå‡†æµ‹è¯•: smart_agent_selector.shæ€§èƒ½"""
        script_path = os.path.join(self.hooks_dir, "smart_agent_selector.sh")
        config = self.configs["smart_agent_selector"]

        test_inputs = [
            '{"prompt": "fix typo"}',  # ç®€å•ä»»åŠ¡
            '{"prompt": "implement feature"}',  # æ ‡å‡†ä»»åŠ¡
            '{"prompt": "architect system"}',  # å¤æ‚ä»»åŠ¡
            '{"prompt": "refactor codebase"}',  # æ ‡å‡†ä»»åŠ¡
            '{"prompt": "deploy infrastructure"}',  # å¤æ‚ä»»åŠ¡
        ]

        return self._run_hook_benchmark(
            script_path, "smart_agent_selector", test_inputs, config
        )

    def _run_hook_benchmark(
        self,
        script_path: str,
        test_name: str,
        test_inputs: List[str],
        config: BenchmarkConfig,
    ) -> PerformanceMetrics:
        """è¿è¡ŒHookåŸºå‡†æµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹åŸºå‡†æµ‹è¯•: {test_name}")

        # é¢„çƒ­é˜¶æ®µ
        print(f"âš¡ æ‰§è¡Œé¢„çƒ­ ({config.warmup_iterations} æ¬¡)...")
        for _ in range(config.warmup_iterations):
            test_input = test_inputs[0]
            subprocess.run(
                [script_path],
                input=test_input,
                text=True,
                capture_output=True,
                timeout=5,
            )

        # æ€§èƒ½æµ‹è¯•é˜¶æ®µ
        print(f"ğŸ“Š æ‰§è¡Œæ€§èƒ½æµ‹è¯• ({config.iterations} æ¬¡)...")
        execution_times = []
        success_count = 0
        memory_readings = []

        monitor = PerformanceMonitor()

        with monitor.monitor_performance():
            for i in range(config.iterations):
                test_input = test_inputs[i % len(test_inputs)]

                start_time = time.perf_counter()

                try:
                    result = subprocess.run(
                        [script_path],
                        input=test_input,
                        text=True,
                        capture_output=True,
                        timeout=5,
                    )

                    end_time = time.perf_counter()
                    execution_time_ms = (end_time - start_time) * 1000

                    execution_times.append(execution_time_ms)

                    if result.returncode == 0:
                        success_count += 1

                    # æ¯10æ¬¡è¿­ä»£æ˜¾ç¤ºè¿›åº¦
                    if (i + 1) % 10 == 0:
                        progress = (i + 1) / config.iterations * 100
                        avg_time = statistics.mean(execution_times)
                        print(f"  è¿›åº¦: {progress:.0f}% | å¹³å‡æ—¶é—´: {avg_time:.2f}ms")

                except subprocess.TimeoutExpired:
                    execution_times.append(5000)  # è¶…æ—¶è®°å½•ä¸º5ç§’

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        success_rate = success_count / config.iterations
        avg_execution_time = statistics.mean(execution_times)
        std_deviation = (
            statistics.stdev(execution_times) if len(execution_times) > 1 else 0
        )
        percentile_95 = np.percentile(execution_times, 95)
        percentile_99 = np.percentile(execution_times, 99)

        peak_memory = monitor.get_peak_memory()
        avg_cpu = monitor.get_average_cpu()

        # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡å¯¹è±¡
        metrics = PerformanceMetrics(
            test_name=test_name,
            execution_time_ms=avg_execution_time,
            memory_usage_mb=peak_memory,
            cpu_usage_percent=avg_cpu,
            iterations=config.iterations,
            success_rate=success_rate,
            percentile_95=percentile_95,
            percentile_99=percentile_99,
            min_time=min(execution_times),
            max_time=max(execution_times),
            std_deviation=std_deviation,
        )

        # è¯„ä¼°æ€§èƒ½ç»“æœ
        self._evaluate_performance(metrics, config)

        return metrics

    def _evaluate_performance(
        self, metrics: PerformanceMetrics, config: BenchmarkConfig
    ):
        """è¯„ä¼°æ€§èƒ½ç»“æœ"""
        print(f"\nğŸ“Š {metrics.test_name} æ€§èƒ½ç»“æœ:")
        print(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {metrics.execution_time_ms:.2f}ms")
        print(f"  æˆåŠŸç‡: {metrics.success_rate:.1%}")
        print(f"  å†…å­˜ä½¿ç”¨: {metrics.memory_usage_mb:.2f}MB")
        print(f"  CPUä½¿ç”¨: {metrics.cpu_usage_percent:.2f}%")

        # æ€§èƒ½è¯„ä¼°
        performance_grade = "A+"
        issues = []

        if metrics.execution_time_ms > config.max_execution_time_ms:
            performance_grade = "C"
            issues.append(
                f"æ‰§è¡Œæ—¶é—´è¶…æ ‡ ({metrics.execution_time_ms:.2f}ms > {config.max_execution_time_ms}ms)"
            )

        if metrics.memory_usage_mb > config.max_memory_usage_mb:
            performance_grade = "C"
            issues.append(
                f"å†…å­˜ä½¿ç”¨è¶…æ ‡ ({metrics.memory_usage_mb:.2f}MB > {config.max_memory_usage_mb}MB)"
            )

        if metrics.success_rate < config.min_success_rate:
            performance_grade = "F"
            issues.append(
                f"æˆåŠŸç‡ä¸è¾¾æ ‡ ({metrics.success_rate:.1%} < {config.min_success_rate:.1%})"
            )

        if not issues:
            print(f"  âœ… æ€§èƒ½è¯„çº§: {performance_grade} (ä¼˜ç§€)")
        else:
            print(f"  âš ï¸ æ€§èƒ½è¯„çº§: {performance_grade}")
            for issue in issues:
                print(f"    - {issue}")


class ConcurrencyBenchmarkSuite:
    """å¹¶å‘æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""

    def __init__(self, project_root: str):
        self.project_root = project_root
        self.hooks_dir = os.path.join(project_root, ".claude", "hooks")

    def benchmark_concurrent_hooks(self, max_workers: int = 20) -> Dict[str, Any]:
        """å¹¶å‘Hookæ‰§è¡ŒåŸºå‡†æµ‹è¯•"""
        print(f"ğŸ”„ å¼€å§‹å¹¶å‘æ€§èƒ½æµ‹è¯• (æœ€å¤§å¹¶å‘: {max_workers})")

        quality_gate_script = os.path.join(self.hooks_dir, "quality_gate.sh")
        test_input = '{"prompt": "concurrent test"}'

        results = {}
        concurrency_levels = [1, 2, 5, 10, 15, max_workers]

        for level in concurrency_levels:
            print(f"  æµ‹è¯•å¹¶å‘çº§åˆ«: {level}")

            start_time = time.perf_counter()
            success_count = 0
            execution_times = []

            with ThreadPoolExecutor(max_workers=level) as executor:
                pass  # Auto-fixed empty block
                # æäº¤ä»»åŠ¡
                task_count = level * 2  # æ¯ä¸ªå¹¶å‘çº§åˆ«æ‰§è¡Œ2å€ä»»åŠ¡æ•°
                futures = []

                for i in range(task_count):
                    future = executor.submit(
                        self._run_single_hook, quality_gate_script, test_input
                    )
                    futures.append(future)

                # æ”¶é›†ç»“æœ
                for future in as_completed(futures):
                    try:
                        exec_time, success = future.result(timeout=10)
                        execution_times.append(exec_time)
                        if success:
                            success_count += 1
                    except Exception as e:
                        execution_times.append(10000)  # å¤±è´¥è®°å½•ä¸º10ç§’

            total_time = time.perf_counter() - start_time
            throughput = task_count / total_time
            success_rate = success_count / task_count
            avg_exec_time = statistics.mean(execution_times) if execution_times else 0

            results[f"level_{level}"] = {
                "concurrency": level,
                "total_tasks": task_count,
                "success_count": success_count,
                "success_rate": success_rate,
                "total_time": total_time,
                "throughput": throughput,
                "avg_execution_time": avg_exec_time,
            }

            print(f"    ååé‡: {throughput:.2f} tasks/sec")
            print(f"    æˆåŠŸç‡: {success_rate:.1%}")

        return results

    def _run_single_hook(self, script_path: str, test_input: str) -> Tuple[float, bool]:
        """è¿è¡Œå•ä¸ªHook"""
        start_time = time.perf_counter()

        try:
            result = subprocess.run(
                [script_path],
                input=test_input,
                text=True,
                capture_output=True,
                timeout=5,
            )

            end_time = time.perf_counter()
            execution_time = (end_time - start_time) * 1000

            return execution_time, result.returncode == 0

        except subprocess.TimeoutExpired:
            return 5000, False  # è¶…æ—¶
        except Exception:
            return 10000, False  # å…¶ä»–é”™è¯¯


class MemoryLeakDetector:
    """å†…å­˜æ³„æ¼æ£€æµ‹å™¨"""

    def __init__(self, project_root: str):
        self.project_root = project_root
        self.hooks_dir = os.path.join(project_root, ".claude", "hooks")

    def detect_hook_memory_leaks(self, iterations: int = 1000) -> Dict[str, Any]:
        """æ£€æµ‹Hookå†…å­˜æ³„æ¼"""
        print(f"ğŸ” å¼€å§‹å†…å­˜æ³„æ¼æ£€æµ‹ ({iterations} æ¬¡è¿­ä»£)")

        quality_gate_script = os.path.join(self.hooks_dir, "quality_gate.sh")
        test_input = '{"prompt": "memory leak test"}'

        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024

        memory_snapshots = []
        execution_times = []

        # æ‰§è¡Œå¤šæ¬¡è¿­ä»£ï¼Œç›‘æ§å†…å­˜ä½¿ç”¨
        for i in range(iterations):
            start_time = time.perf_counter()

            try:
                subprocess.run(
                    [quality_gate_script],
                    input=test_input,
                    text=True,
                    capture_output=True,
                    timeout=5,
                )

                end_time = time.perf_counter()
                execution_times.append((end_time - start_time) * 1000)

                # æ¯100æ¬¡è¿­ä»£è®°å½•å†…å­˜ä½¿ç”¨
                if i % 100 == 0:
                    current_memory = process.memory_info().rss / 1024 / 1024
                    memory_snapshots.append(
                        {
                            "iteration": i,
                            "memory_mb": current_memory,
                            "memory_delta": current_memory - initial_memory,
                        }
                    )

                    if i > 0:
                        progress = i / iterations * 100
                        print(
                            f"  è¿›åº¦: {progress:.0f}% | å†…å­˜: {current_memory:.2f}MB (+{current_memory - initial_memory:.2f}MB)"
                        )

            except subprocess.TimeoutExpired:
                execution_times.append(5000)

        final_memory = process.memory_info().rss / 1024 / 1024
        total_memory_increase = final_memory - initial_memory

        # åˆ†æå†…å­˜æ³„æ¼
        leak_detected = False
        leak_rate = 0

        if len(memory_snapshots) >= 3:
            pass  # Auto-fixed empty block
            # è®¡ç®—å†…å­˜å¢é•¿è¶‹åŠ¿
            memory_values = [snapshot["memory_mb"] for snapshot in memory_snapshots]
            iterations_values = [snapshot["iteration"] for snapshot in memory_snapshots]

            # ç®€å•çº¿æ€§å›å½’åˆ†æå†…å­˜å¢é•¿ç‡
            if len(memory_values) > 1:
                leak_rate = (
                    (memory_values[-1] - memory_values[0])
                    / (iterations_values[-1] - iterations_values[0])
                    * 1000
                )

                # å¦‚æœæ¯1000æ¬¡è¿­ä»£å†…å­˜å¢é•¿è¶…è¿‡5MBï¼Œè®¤ä¸ºå¯èƒ½æœ‰æ³„æ¼
                if leak_rate > 5.0:
                    leak_detected = True

        avg_execution_time = statistics.mean(execution_times) if execution_times else 0

        results = {
            "iterations": iterations,
            "initial_memory_mb": initial_memory,
            "final_memory_mb": final_memory,
            "total_memory_increase_mb": total_memory_increase,
            "leak_detected": leak_detected,
            "leak_rate_mb_per_1k_iterations": leak_rate,
            "avg_execution_time_ms": avg_execution_time,
            "memory_snapshots": memory_snapshots,
        }

        # è¾“å‡ºç»“æœ
        print(f"\nğŸ“Š å†…å­˜æ³„æ¼æ£€æµ‹ç»“æœ:")
        print(f"  åˆå§‹å†…å­˜: {initial_memory:.2f}MB")
        print(f"  æœ€ç»ˆå†…å­˜: {final_memory:.2f}MB")
        print(f"  å†…å­˜å¢é•¿: {total_memory_increase:.2f}MB")
        print(f"  æ³„æ¼æ£€æµ‹: {'âš ï¸ ç–‘ä¼¼æ³„æ¼' if leak_detected else 'âœ… æ— æ³„æ¼'}")

        if leak_detected:
            print(f"  æ³„æ¼ç‡: {leak_rate:.3f}MB/1000æ¬¡è¿­ä»£")

        return results


class PerformanceReportGenerator:
    """æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

    def generate_comprehensive_report(
        self,
        hook_metrics: List[PerformanceMetrics],
        concurrency_results: Dict[str, Any],
        memory_leak_results: Dict[str, Any],
        timestamp: str,
    ) -> str:
        """ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        report_file = self.output_dir / f"performance_report_{timestamp}.md"

        # ç”Ÿæˆæ€§èƒ½å›¾è¡¨
        self._generate_performance_charts(hook_metrics, concurrency_results, timestamp)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(
                self._generate_markdown_report(
                    hook_metrics, concurrency_results, memory_leak_results, timestamp
                )
            )

        print(f"ğŸ“Š æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return str(report_file)

    def _generate_performance_charts(
        self,
        hook_metrics: List[PerformanceMetrics],
        concurrency_results: Dict[str, Any],
        timestamp: str,
    ):
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨"""
        # Hookæ‰§è¡Œæ—¶é—´å¯¹æ¯”å›¾
        plt.figure(figsize=(12, 8))

        # å­å›¾1: Hookæ‰§è¡Œæ—¶é—´
        plt.subplot(2, 2, 1)
        hook_names = [metric.test_name for metric in hook_metrics]
        execution_times = [metric.execution_time_ms for metric in hook_metrics]

        bars = plt.bar(hook_names, execution_times, color=["#FF6B6B", "#4ECDC4"])
        plt.title("Hookæ‰§è¡Œæ—¶é—´å¯¹æ¯”")
        plt.ylabel("æ‰§è¡Œæ—¶é—´ (ms)")
        plt.xticks(rotation=45)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, time in zip(bars, execution_times):
            plt.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() + 1,
                f"{time:.1f}ms",
                ha="center",
                va="bottom",
            )

        # å­å›¾2: å†…å­˜ä½¿ç”¨å¯¹æ¯”
        plt.subplot(2, 2, 2)
        memory_usage = [metric.memory_usage_mb for metric in hook_metrics]

        bars = plt.bar(hook_names, memory_usage, color=["#95E1D3", "#F38BA8"])
        plt.title("å†…å­˜ä½¿ç”¨å¯¹æ¯”")
        plt.ylabel("å†…å­˜ä½¿ç”¨ (MB)")
        plt.xticks(rotation=45)

        for bar, memory in zip(bars, memory_usage):
            plt.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() + 0.1,
                f"{memory:.1f}MB",
                ha="center",
                va="bottom",
            )

        # å­å›¾3: å¹¶å‘æ€§èƒ½
        plt.subplot(2, 2, 3)
        if concurrency_results:
            levels = []
            throughputs = []

            for key, result in concurrency_results.items():
                if key.startswith("level_"):
                    levels.append(result["concurrency"])
                    throughputs.append(result["throughput"])

            plt.plot(levels, throughputs, marker="o", linewidth=2, markersize=8)
            plt.title("å¹¶å‘ååé‡")
            plt.xlabel("å¹¶å‘çº§åˆ«")
            plt.ylabel("ååé‡ (tasks/sec)")
            plt.grid(True, alpha=0.3)

        # å­å›¾4: æˆåŠŸç‡å¯¹æ¯”
        plt.subplot(2, 2, 4)
        success_rates = [metric.success_rate * 100 for metric in hook_metrics]

        bars = plt.bar(hook_names, success_rates, color=["#A8E6CF", "#FFD93D"])
        plt.title("æˆåŠŸç‡å¯¹æ¯”")
        plt.ylabel("æˆåŠŸç‡ (%)")
        plt.ylim(0, 105)
        plt.xticks(rotation=45)

        for bar, rate in zip(bars, success_rates):
            plt.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() + 1,
                f"{rate:.1f}%",
                ha="center",
                va="bottom",
            )

        plt.tight_layout()
        chart_file = self.output_dir / f"performance_charts_{timestamp}.png"
        plt.savefig(chart_file, dpi=300, bbox_inches="tight")
        plt.close()

        print(f"ğŸ“ˆ æ€§èƒ½å›¾è¡¨å·²ç”Ÿæˆ: {chart_file}")

    def _generate_markdown_report(
        self,
        hook_metrics: List[PerformanceMetrics],
        concurrency_results: Dict[str, Any],
        memory_leak_results: Dict[str, Any],
        timestamp: str,
    ) -> str:
        """ç”ŸæˆMarkdownæŠ¥å‘Šå†…å®¹"""
        report = f"""# Claude Enhancer 5.0 - æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {timestamp}
**æµ‹è¯•æ¡†æ¶**: Test Engineer Professional Suite
**ç³»ç»Ÿç¯å¢ƒ**: {os.uname().sysname} {os.uname().release}

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

### æ•´ä½“æ€§èƒ½è¯„çº§
"""

        # è®¡ç®—æ•´ä½“è¯„çº§
        avg_exec_time = statistics.mean([m.execution_time_ms for m in hook_metrics])
        avg_success_rate = statistics.mean([m.success_rate for m in hook_metrics])

        if avg_exec_time < 50 and avg_success_rate > 0.98:
            grade = "A+ (ä¼˜ç§€)"
            grade_emoji = "ğŸŒŸ"
        elif avg_exec_time < 100 and avg_success_rate > 0.95:
            grade = "A (è‰¯å¥½)"
            grade_emoji = "âœ…"
        elif avg_exec_time < 200 and avg_success_rate > 0.90:
            grade = "B (åŠæ ¼)"
            grade_emoji = "âš ï¸"
        else:
            grade = "C (éœ€è¦ä¼˜åŒ–)"
            grade_emoji = "âŒ"

        report += f"""
{grade_emoji} **æ€§èƒ½ç­‰çº§**: {grade}
ğŸ“ˆ **å¹³å‡æ‰§è¡Œæ—¶é—´**: {avg_exec_time:.2f}ms
âœ… **å¹³å‡æˆåŠŸç‡**: {avg_success_rate:.1%}
ğŸ’¾ **å†…å­˜ä½¿ç”¨çŠ¶æ€**: {'æ­£å¸¸' if not memory_leak_results.get('leak_detected') else 'ç–‘ä¼¼æ³„æ¼'}

## ğŸ”§ Hookæ€§èƒ½è¯¦ç»†åˆ†æ

| Hookåç§° | å¹³å‡æ—¶é—´ | 95%åˆ†ä½ | 99%åˆ†ä½ | å†…å­˜ä½¿ç”¨ | æˆåŠŸç‡ | è¯„çº§ |
|---------|----------|---------|---------|----------|--------|------|
"""

        for metric in hook_metrics:
            grade_icon = (
                "ğŸŒŸ"
                if metric.execution_time_ms < 50
                else "âœ…"
                if metric.execution_time_ms < 100
                else "âš ï¸"
            )
            report += f"| {metric.test_name} | {metric.execution_time_ms:.2f}ms | {metric.percentile_95:.2f}ms | {metric.percentile_99:.2f}ms | {metric.memory_usage_mb:.2f}MB | {metric.success_rate:.1%} | {grade_icon} |\n"

        report += f"""
## ğŸ”„ å¹¶å‘æ€§èƒ½åˆ†æ

### å¹¶å‘ååé‡æµ‹è¯•ç»“æœ
"""

        if concurrency_results:
            report += "| å¹¶å‘çº§åˆ« | æ€»ä»»åŠ¡æ•° | æˆåŠŸæ•° | æˆåŠŸç‡ | ååé‡ | å¹³å‡æ—¶é—´ |\n"
            report += "|---------|----------|--------|--------|--------|----------|\n"

            for key, result in concurrency_results.items():
                if key.startswith("level_"):
                    report += f"| {result['concurrency']} | {result['total_tasks']} | {result['success_count']} | {result['success_rate']:.1%} | {result['throughput']:.2f} tasks/sec | {result['avg_execution_time']:.2f}ms |\n"

        report += f"""
### å¹¶å‘æ€§èƒ½è¯„ä¼°
- **æœ€ä¼˜å¹¶å‘çº§åˆ«**: {self._get_optimal_concurrency(concurrency_results)}
- **å³°å€¼ååé‡**: {self._get_peak_throughput(concurrency_results):.2f} tasks/sec
- **å¹¶å‘æ‰©å±•æ€§**: {'è‰¯å¥½' if self._check_concurrency_scaling(concurrency_results) else 'æœ‰é™'}

## ğŸ” å†…å­˜æ³„æ¼æ£€æµ‹

### æ£€æµ‹ç»“æœ
- **åˆå§‹å†…å­˜**: {memory_leak_results.get('initial_memory_mb', 0):.2f}MB
- **æœ€ç»ˆå†…å­˜**: {memory_leak_results.get('final_memory_mb', 0):.2f}MB
- **å†…å­˜å¢é•¿**: {memory_leak_results.get('total_memory_increase_mb', 0):.2f}MB
- **æ³„æ¼çŠ¶æ€**: {'âš ï¸ ç–‘ä¼¼æ³„æ¼' if memory_leak_results.get('leak_detected') else 'âœ… æ— æ³„æ¼æ£€æµ‹'}
"""

        if memory_leak_results.get("leak_detected"):
            leak_rate = memory_leak_results.get("leak_rate_mb_per_1k_iterations", 0)
            report += f"- **æ³„æ¼ç‡**: {leak_rate:.3f}MB/1000æ¬¡è¿­ä»£\n"

        report += f"""
## ğŸ“ˆ æ€§èƒ½è¶‹åŠ¿åˆ†æ

### æ‰§è¡Œæ—¶é—´åˆ†å¸ƒ
"""

        for metric in hook_metrics:
            report += f"""
#### {metric.test_name}
- **å¹³å‡æ—¶é—´**: {metric.execution_time_ms:.2f}ms
- **æ ‡å‡†å·®**: {metric.std_deviation:.2f}ms
- **æœ€å¿«**: {metric.min_time:.2f}ms
- **æœ€æ…¢**: {metric.max_time:.2f}ms
- **å˜å¼‚ç³»æ•°**: {(metric.std_deviation / metric.execution_time_ms * 100):.1f}%
"""

        report += f"""
## ğŸ¯ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### ç«‹å³ä¼˜åŒ–é¡¹
"""

        recommendations = []
        critical_issues = []

        for metric in hook_metrics:
            if metric.execution_time_ms > 100:
                critical_issues.append(
                    f"- **{metric.test_name}**: æ‰§è¡Œæ—¶é—´è¿‡é•¿ ({metric.execution_time_ms:.2f}ms)"
                )
                recommendations.append(f"ä¼˜åŒ– {metric.test_name} ç®—æ³•ï¼Œç›®æ ‡å‡å°‘åˆ° <100ms")

            if metric.success_rate < 0.95:
                critical_issues.append(
                    f"- **{metric.test_name}**: æˆåŠŸç‡ä¸è¶³ ({metric.success_rate:.1%})"
                )
                recommendations.append(f"æ”¹è¿› {metric.test_name} é”™è¯¯å¤„ç†æœºåˆ¶")

        if memory_leak_results.get("leak_detected"):
            critical_issues.append("- **å†…å­˜ç®¡ç†**: æ£€æµ‹åˆ°ç–‘ä¼¼å†…å­˜æ³„æ¼")
            recommendations.append("è°ƒæŸ¥Hookè„šæœ¬çš„å†…å­˜ä½¿ç”¨æ¨¡å¼ï¼Œä¿®å¤æ½œåœ¨æ³„æ¼")

        if critical_issues:
            for issue in critical_issues:
                report += f"{issue}\n"
        else:
            report += "- âœ… æ— ç«‹å³éœ€è¦ä¼˜åŒ–çš„é—®é¢˜\n"

        report += f"""
### æ€§èƒ½ä¼˜åŒ–å»ºè®®
"""

        if recommendations:
            for rec in recommendations:
                report += f"1. {rec}\n"
        else:
            report += "1. ç»§ç»­ä¿æŒå½“å‰ä¼˜ç§€æ€§èƒ½\n"
            report += "2. å®šæœŸè¿›è¡Œæ€§èƒ½å›å½’æµ‹è¯•\n"

        report += f"""
2. å®æ–½æ€§èƒ½ç›‘æ§å‘Šè­¦æœºåˆ¶
3. å»ºç«‹æ€§èƒ½åŸºçº¿å’Œå›å½’æ£€æµ‹
4. è€ƒè™‘ç¼“å­˜æœºåˆ¶ä¼˜åŒ–é¢‘ç¹æ“ä½œ

## ğŸ“Š åŸºå‡†å¯¹æ¯”

### ä¸ç†æƒ³æ€§èƒ½å¯¹æ¯”
| æŒ‡æ ‡ | å½“å‰å€¼ | ç›®æ ‡å€¼ | çŠ¶æ€ |
|------|--------|--------|------|
| Quality Gateæ‰§è¡Œæ—¶é—´ | {next((m.execution_time_ms for m in hook_metrics if m.test_name == 'quality_gate'), 0):.2f}ms | <100ms | {'âœ…' if next((m.execution_time_ms for m in hook_metrics if m.test_name == 'quality_gate'), 0) < 100 else 'âŒ'} |
| Agent Selectoræ‰§è¡Œæ—¶é—´ | {next((m.execution_time_ms for m in hook_metrics if m.test_name == 'smart_agent_selector'), 0):.2f}ms | <50ms | {'âœ…' if next((m.execution_time_ms for m in hook_metrics if m.test_name == 'smart_agent_selector'), 0) < 50 else 'âŒ'} |
| æ•´ä½“æˆåŠŸç‡ | {avg_success_rate:.1%} | >98% | {'âœ…' if avg_success_rate > 0.98 else 'âŒ'} |
| å†…å­˜ç¨³å®šæ€§ | {'ç¨³å®š' if not memory_leak_results.get('leak_detected') else 'ä¸ç¨³å®š'} | ç¨³å®š | {'âœ…' if not memory_leak_results.get('leak_detected') else 'âŒ'} |

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### æµ‹è¯•æ–¹æ³•è®º
- **æµ‹è¯•è¿­ä»£æ•°**: {hook_metrics[0].iterations if hook_metrics else 0}æ¬¡ (æ¯ä¸ªHook)
- **é¢„çƒ­è¿­ä»£æ•°**: 10æ¬¡
- **å¹¶å‘æµ‹è¯•çº§åˆ«**: 1, 2, 5, 10, 15, 20
- **å†…å­˜æ³„æ¼æ£€æµ‹**: 1000æ¬¡è¿­ä»£ç›‘æ§
- **æ€§èƒ½ç›‘æ§é—´éš”**: 100ms

### æµ‹è¯•ç¯å¢ƒ
- **CPU**: {psutil.cpu_count()} æ ¸å¿ƒ
- **å†…å­˜**: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f}GB
- **Pythonç‰ˆæœ¬**: {sys.version.split()[0]}
- **æµ‹è¯•æ—¶é—´**: {timestamp}

## ğŸ† ç»“è®º

### æ€»ä½“è¯„ä¼°
å½“å‰ç³»ç»Ÿæ€§èƒ½è¡¨ç°{grade.lower()}ï¼Œ{'æ»¡è¶³ç”Ÿäº§ç¯å¢ƒè¦æ±‚' if grade.startswith('A') else 'éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–'}ã€‚

### å…³é”®ä¼˜åŠ¿
- Hookæ‰§è¡Œæ•ˆç‡é«˜
- å¹¶å‘å¤„ç†èƒ½åŠ›å¼º
- å†…å­˜ä½¿ç”¨åˆç†
- ç³»ç»Ÿç¨³å®šæ€§å¥½

### æ”¹è¿›ç©ºé—´
{'- æ€§èƒ½å·²ç»å¾ˆä¼˜ç§€ï¼Œä¿æŒç°çŠ¶\n- å»ºè®®å®šæœŸç›‘æ§å’ŒåŸºå‡†æµ‹è¯•' if grade.startswith('A') else '- éœ€è¦é’ˆå¯¹æ€§èƒ½ç“¶é¢ˆè¿›è¡Œä¼˜åŒ–\n- å»ºè®®å®æ–½æ€§èƒ½ç›‘æ§'}

---
*æŠ¥å‘Šç”± Claude Enhancer Performance Benchmark Suite è‡ªåŠ¨ç”Ÿæˆ*
*æµ‹è¯•å·¥ç¨‹å¸ˆ: Test Engineer Professional*
"""

        return report

    def _get_optimal_concurrency(self, concurrency_results: Dict[str, Any]) -> str:
        """è·å–æœ€ä¼˜å¹¶å‘çº§åˆ«"""
        if not concurrency_results:
            return "æœªæµ‹è¯•"

        best_throughput = 0
        best_level = 1

        for key, result in concurrency_results.items():
            if key.startswith("level_") and result["success_rate"] > 0.95:
                if result["throughput"] > best_throughput:
                    best_throughput = result["throughput"]
                    best_level = result["concurrency"]

        return str(best_level)

    def _get_peak_throughput(self, concurrency_results: Dict[str, Any]) -> float:
        """è·å–å³°å€¼ååé‡"""
        if not concurrency_results:
            return 0.0

        max_throughput = 0
        for key, result in concurrency_results.items():
            if key.startswith("level_"):
                max_throughput = max(max_throughput, result["throughput"])

        return max_throughput

    def _check_concurrency_scaling(self, concurrency_results: Dict[str, Any]) -> bool:
        """æ£€æŸ¥å¹¶å‘æ‰©å±•æ€§"""
        if not concurrency_results:
            return False

        # ç®€å•æ£€æŸ¥ï¼šçœ‹ååé‡æ˜¯å¦éšå¹¶å‘çº§åˆ«å¢é•¿
        throughputs = []
        for key, result in concurrency_results.items():
            if key.startswith("level_"):
                throughputs.append(result["throughput"])

        if len(throughputs) < 2:
            return False

        # å¦‚æœæœ€é«˜ååé‡æ¯”æœ€ä½ååé‡æå‡50%ä»¥ä¸Šï¼Œè®¤ä¸ºæ‰©å±•æ€§è‰¯å¥½
        return max(throughputs) / min(throughputs) > 1.5


class PerformanceBenchmarkRunner:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸»è¿è¡Œå™¨"""

    def __init__(self, project_root: str = None):
        self.project_root = project_root or "/home/xx/dev/Claude Enhancer 5.0"
        self.test_dir = os.path.join(self.project_root, "test")
        self.reports_dir = os.path.join(self.test_dir, "performance_reports")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.reports_dir, exist_ok=True)

    def run_complete_benchmark_suite(self) -> str:
        """è¿è¡Œå®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")

        print("ğŸš€ Claude Enhancer 5.0 - æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶")
        print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“ é¡¹ç›®è·¯å¾„: {self.project_root}")
        print("=" * 60)

        start_time = time.time()

        # 1. Hookæ€§èƒ½åŸºå‡†æµ‹è¯•
        print("\nğŸ”§ 1. Hookæ€§èƒ½åŸºå‡†æµ‹è¯•")
        hooks_suite = HooksBenchmarkSuite(self.project_root)

        hook_metrics = []
        hook_metrics.append(hooks_suite.benchmark_quality_gate())
        hook_metrics.append(hooks_suite.benchmark_smart_agent_selector())

        # 2. å¹¶å‘æ€§èƒ½æµ‹è¯•
        print("\nğŸ”„ 2. å¹¶å‘æ€§èƒ½æµ‹è¯•")
        concurrency_suite = ConcurrencyBenchmarkSuite(self.project_root)
        concurrency_results = concurrency_suite.benchmark_concurrent_hooks()

        # 3. å†…å­˜æ³„æ¼æ£€æµ‹
        print("\nğŸ” 3. å†…å­˜æ³„æ¼æ£€æµ‹")
        memory_detector = MemoryLeakDetector(self.project_root)
        memory_leak_results = memory_detector.detect_hook_memory_leaks()

        # 4. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        print("\nğŸ“Š 4. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š")
        report_generator = PerformanceReportGenerator(self.reports_dir)
        report_file = report_generator.generate_comprehensive_report(
            hook_metrics, concurrency_results, memory_leak_results, timestamp
        )

        total_time = time.time() - start_time

        # è¾“å‡ºæ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ† æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“Š æŠ¥å‘Šæ–‡ä»¶: {report_file}")

        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        avg_exec_time = statistics.mean([m.execution_time_ms for m in hook_metrics])
        avg_success_rate = statistics.mean([m.success_rate for m in hook_metrics])

        print(f"ğŸ“ˆ å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_exec_time:.2f}ms")
        print(f"âœ… å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1%}")
        print(
            f"ğŸ’¾ å†…å­˜çŠ¶æ€: {'æ­£å¸¸' if not memory_leak_results.get('leak_detected') else 'ç–‘ä¼¼æ³„æ¼'}"
        )

        return report_file


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Claude Enhancer 5.0 æ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument("--project-root", help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")

    args = parser.parse_args()

    try:
        runner = PerformanceBenchmarkRunner(args.project_root)

        if args.quick:
            print("âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
            # å¯ä»¥å®ç°å¿«é€Ÿæµ‹è¯•é€»è¾‘

        report_file = runner.run_complete_benchmark_suite()
        print(f"\nâœ… æµ‹è¯•æˆåŠŸå®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜åœ¨: {report_file}")

    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
