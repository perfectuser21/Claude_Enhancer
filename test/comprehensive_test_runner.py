#!/usr/bin/env python3
"""
Claude Enhancer 5.0 - ç»¼åˆæµ‹è¯•æ‰§è¡Œå™¨
ä½œä¸ºtest-engineerè®¾è®¡çš„ç»Ÿä¸€æµ‹è¯•ç®¡ç†å¹³å°

åŠŸèƒ½ç‰¹æ€§:
1. ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æµ‹è¯•æ¡†æ¶
2. æ™ºèƒ½æµ‹è¯•æ‰§è¡Œè®¡åˆ’
3. å¹¶è¡Œæµ‹è¯•æ”¯æŒ
4. æµ‹è¯•ç»“æœèšåˆåˆ†æ
5. ç»¼åˆæµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
6. CI/CDé›†æˆæ”¯æŒ
"""

import os
import sys
import time
import json
import argparse
import subprocess
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import threading
import queue


@dataclass
class TestFrameworkConfig:
    """æµ‹è¯•æ¡†æ¶é…ç½®"""

    name: str
    description: str
    script_path: str
    category: str  # "unit", "integration", "performance", "regression", "recovery"
    priority: int  # 1-5, 1ä¸ºæœ€é«˜ä¼˜å…ˆçº§
    estimated_duration: int  # é¢„ä¼°æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
    dependencies: List[str]  # ä¾èµ–çš„å…¶ä»–æµ‹è¯•æ¡†æ¶
    parallel_safe: bool  # æ˜¯å¦å¯ä»¥å¹¶è¡Œæ‰§è¡Œ


@dataclass
class TestExecutionResult:
    """æµ‹è¯•æ‰§è¡Œç»“æœ"""

    framework_name: str
    success: bool
    duration: float
    output: str
    error_output: str
    report_file: Optional[str] = None
    metrics: Dict[str, Any] = None


class TestOrchestrator:
    """æµ‹è¯•æ‰§è¡Œç¼–æ’å™¨"""

    def __init__(self, project_root: str):
        self.project_root = project_root
        self.test_dir = os.path.join(project_root, "test")
        self.reports_dir = os.path.join(self.test_dir, "comprehensive_reports")
        self.quick_mode = False  # æ·»åŠ quick_modeçŠ¶æ€è¿½è¸ª

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.reports_dir, exist_ok=True)

        # åˆå§‹åŒ–æµ‹è¯•æ¡†æ¶é…ç½®
        self.test_frameworks = self._initialize_test_frameworks()

        # æ‰§è¡ŒçŠ¶æ€è·Ÿè¸ª
        self.execution_queue = queue.Queue()
        self.completed_tests = {}
        self.failed_tests = {}

    def _initialize_test_frameworks(self) -> Dict[str, TestFrameworkConfig]:
        """åˆå§‹åŒ–æµ‹è¯•æ¡†æ¶é…ç½®"""
        frameworks = {
            "document_quality": TestFrameworkConfig(
                name="document_quality",
                description="æ–‡æ¡£è´¨é‡ç®¡ç†ç³»ç»Ÿæµ‹è¯•",
                script_path="test/document_quality_management_test_strategy.py",
                category="unit",
                priority=1,
                estimated_duration=300,  # 5åˆ†é’Ÿ
                dependencies=[],
                parallel_safe=True,
            ),
            "performance_benchmark": TestFrameworkConfig(
                name="performance_benchmark",
                description="æ€§èƒ½åŸºå‡†æµ‹è¯•",
                script_path="test/performance_benchmark_runner.py",
                category="performance",
                priority=2,
                estimated_duration=600,  # 10åˆ†é’Ÿ
                dependencies=[],
                parallel_safe=False,  # æ€§èƒ½æµ‹è¯•éœ€è¦ç‹¬å èµ„æº
            ),
            "regression_test": TestFrameworkConfig(
                name="regression_test",
                description="å›å½’æµ‹è¯•",
                script_path="test/regression_test_framework.py",
                category="regression",
                priority=3,
                estimated_duration=480,  # 8åˆ†é’Ÿ
                dependencies=["performance_benchmark"],  # éœ€è¦æ€§èƒ½åŸºçº¿
                parallel_safe=True,
            ),
            "failure_recovery": TestFrameworkConfig(
                name="failure_recovery",
                description="æ•…éšœæ¢å¤æµ‹è¯•",
                script_path="test/failure_recovery_test_framework.py",
                category="recovery",
                priority=4,
                estimated_duration=900,  # 15åˆ†é’Ÿ
                dependencies=[],
                parallel_safe=False,  # æ•…éšœæ³¨å…¥å¯èƒ½å½±å“å…¶ä»–æµ‹è¯•
            ),
            "shell_integration": TestFrameworkConfig(
                name="shell_integration",
                description="Shellè„šæœ¬é›†æˆæµ‹è¯•",
                script_path="test/run_document_quality_tests.sh",
                category="integration",
                priority=2,
                estimated_duration=180,  # 3åˆ†é’Ÿ
                dependencies=[],
                parallel_safe=True,
            ),
        }

        return frameworks

    def run_all_tests(
        self, parallel: bool = True, quick_mode: bool = False
    ) -> Dict[str, TestExecutionResult]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.quick_mode = quick_mode  # ä¿å­˜quick_modeçŠ¶æ€
        print("ğŸš€ Claude Enhancer 5.0 - ç»¼åˆæµ‹è¯•æ‰§è¡Œå™¨")
        print(f"ğŸ“ é¡¹ç›®è·¯å¾„: {self.project_root}")
        print(f"ğŸ”§ å¹¶è¡Œæ¨¡å¼: {'å¯ç”¨' if parallel else 'ç¦ç”¨'}")
        print(f"âš¡ å¿«é€Ÿæ¨¡å¼: {'å¯ç”¨' if quick_mode else 'ç¦ç”¨'}")
        print("=" * 60)

        start_time = time.time()

        # æ ¹æ®æ¨¡å¼é€‰æ‹©æµ‹è¯•æ¡†æ¶
        selected_frameworks = self._select_frameworks(quick_mode)

        # è®¡ç®—æ‰§è¡Œè®¡åˆ’
        execution_plan = self._create_execution_plan(selected_frameworks, parallel)

        print(f"ğŸ“‹ æµ‹è¯•è®¡åˆ’: {len(execution_plan)} ä¸ªé˜¶æ®µ")
        for i, phase in enumerate(execution_plan, 1):
            framework_names = [f.name for f in phase]
            estimated_time = sum(f.estimated_duration for f in phase)
            print(
                f"  é˜¶æ®µ {i}: {', '.join(framework_names)} (é¢„ä¼°{estimated_time//60}åˆ†{estimated_time%60}ç§’)"
            )

        # æ‰§è¡Œæµ‹è¯•
        results = {}

        for phase_num, phase_frameworks in enumerate(execution_plan, 1):
            print(f"\nğŸ”„ æ‰§è¡Œé˜¶æ®µ {phase_num}/{len(execution_plan)}")

            if len(phase_frameworks) == 1 or not parallel:
                # ä¸²è¡Œæ‰§è¡Œ
                for framework in phase_frameworks:
                    result = self._execute_single_test(framework)
                    results[framework.name] = result
            else:
                # å¹¶è¡Œæ‰§è¡Œ
                phase_results = self._execute_parallel_tests(phase_frameworks)
                results.update(phase_results)

        total_time = time.time() - start_time

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        print(f"\nğŸ“Š ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š...")
        report_file = self._generate_comprehensive_report(results, total_time)

        # è¾“å‡ºæ€»ç»“
        self._print_execution_summary(results, total_time, report_file)

        return results

    def _select_frameworks(self, quick_mode: bool) -> List[TestFrameworkConfig]:
        """æ ¹æ®æ¨¡å¼é€‰æ‹©æµ‹è¯•æ¡†æ¶"""
        if quick_mode:
            # å¿«é€Ÿæ¨¡å¼ï¼šåªè¿è¡Œé«˜ä¼˜å…ˆçº§å’Œå¿«é€Ÿçš„æµ‹è¯•
            return [
                f
                for f in self.test_frameworks.values()
                if f.priority <= 2 and f.estimated_duration <= 300
            ]
        else:
            # å®Œæ•´æ¨¡å¼ï¼šè¿è¡Œæ‰€æœ‰æµ‹è¯•
            return list(self.test_frameworks.values())

    def _create_execution_plan(
        self, frameworks: List[TestFrameworkConfig], parallel: bool
    ) -> List[List[TestFrameworkConfig]]:
        """åˆ›å»ºæµ‹è¯•æ‰§è¡Œè®¡åˆ’"""
        if not parallel:
            # ä¸²è¡Œæ‰§è¡Œï¼šæŒ‰ä¼˜å…ˆçº§æ’åº
            sorted_frameworks = sorted(frameworks, key=lambda f: f.priority)
            return [[f] for f in sorted_frameworks]

        # å¹¶è¡Œæ‰§è¡Œï¼šè€ƒè™‘ä¾èµ–å…³ç³»å’Œå¹¶è¡Œå®‰å…¨æ€§
        plan = []
        remaining = frameworks.copy()
        completed = set()

        while remaining:
            # æ‰¾å‡ºå½“å‰å¯ä»¥æ‰§è¡Œçš„æ¡†æ¶
            ready_frameworks = []

            for framework in remaining:
                # æ£€æŸ¥ä¾èµ–æ˜¯å¦å·²å®Œæˆ
                dependencies_met = all(
                    dep in completed for dep in framework.dependencies
                )

                if dependencies_met:
                    ready_frameworks.append(framework)

            if not ready_frameworks:
                # é¿å…æ­»é”ï¼šå¦‚æœæ²¡æœ‰å¯æ‰§è¡Œçš„æ¡†æ¶ï¼Œå¼ºåˆ¶æ‰§è¡Œä¸€ä¸ª
                ready_frameworks = [remaining[0]]

            # å°†å¯å¹¶è¡Œçš„æ¡†æ¶åˆ†ç»„
            parallel_group = []
            serial_only = []

            for framework in ready_frameworks:
                if framework.parallel_safe:
                    parallel_group.append(framework)
                else:
                    serial_only.append(framework)

            # å…ˆæ‰§è¡Œå¹¶è¡Œå®‰å…¨çš„æµ‹è¯•
            if parallel_group:
                plan.append(parallel_group)
                for f in parallel_group:
                    remaining.remove(f)
                    completed.add(f.name)

            # ç„¶åé€ä¸ªæ‰§è¡Œéå¹¶è¡Œå®‰å…¨çš„æµ‹è¯•
            for framework in serial_only:
                plan.append([framework])
                remaining.remove(framework)
                completed.add(framework.name)

        return plan

    def _execute_single_test(
        self, framework: TestFrameworkConfig
    ) -> TestExecutionResult:
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•æ¡†æ¶"""
        print(f"  ğŸ§ª æ‰§è¡Œ: {framework.description}")

        start_time = time.time()
        script_path = os.path.join(self.project_root, framework.script_path)

        try:
            # æ ¹æ®è„šæœ¬ç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹å¼
            if script_path.endswith(".py"):
                cmd = [sys.executable, script_path, "--project-root", self.project_root]
            elif script_path.endswith(".sh"):
                cmd = ["bash", script_path]
                if self.quick_mode:
                    cmd.append("--quick")
            else:
                raise ValueError(f"Unsupported script type: {script_path}")

            # æ‰§è¡Œæµ‹è¯•
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=framework.estimated_duration * 2,  # è¶…æ—¶æ—¶é—´ä¸ºé¢„ä¼°æ—¶é—´çš„2å€
            )

            duration = time.time() - start_time
            success = result.returncode == 0

            # å°è¯•æå–æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            report_file = self._extract_report_file(result.stdout, framework.name)

            # å°è¯•æå–æ€§èƒ½æŒ‡æ ‡
            metrics = self._extract_metrics(result.stdout)

            test_result = TestExecutionResult(
                framework_name=framework.name,
                success=success,
                duration=duration,
                output=result.stdout,
                error_output=result.stderr,
                report_file=report_file,
                metrics=metrics,
            )

            # è¾“å‡ºç»“æœ
            status_icon = "âœ…" if success else "âŒ"
            print(
                f"    {status_icon} {framework.description}: {'æˆåŠŸ' if success else 'å¤±è´¥'} ({duration:.1f}s)"
            )

            if not success:
                print(f"    ğŸ“‹ é”™è¯¯ä¿¡æ¯: {result.stderr[:200]}...")

            return test_result

        except subprocess.TimeoutExpired:
            duration = time.time() - start_time
            print(f"    â° {framework.description}: è¶…æ—¶ ({duration:.1f}s)")

            return TestExecutionResult(
                framework_name=framework.name,
                success=False,
                duration=duration,
                output="",
                error_output="Test execution timeout",
                report_file=None,
                metrics={},
            )

        except Exception as e:
            duration = time.time() - start_time
            print(f"    âŒ {framework.description}: æ‰§è¡Œå¤±è´¥ - {e}")

            return TestExecutionResult(
                framework_name=framework.name,
                success=False,
                duration=duration,
                output="",
                error_output=str(e),
                report_file=None,
                metrics={},
            )

    def _execute_parallel_tests(
        self, frameworks: List[TestFrameworkConfig]
    ) -> Dict[str, TestExecutionResult]:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæµ‹è¯•æ¡†æ¶"""
        print(f"  ğŸ”„ å¹¶è¡Œæ‰§è¡Œ: {', '.join(f.description for f in frameworks)}")

        results = {}

        with ThreadPoolExecutor(max_workers=len(frameworks)) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_framework = {
                executor.submit(self._execute_single_test, framework): framework
                for framework in frameworks
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_framework):
                framework = future_to_framework[future]
                try:
                    result = future.result()
                    results[framework.name] = result
                except Exception as e:
                    print(f"    âŒ {framework.description}: å¹¶è¡Œæ‰§è¡Œå¤±è´¥ - {e}")
                    results[framework.name] = TestExecutionResult(
                        framework_name=framework.name,
                        success=False,
                        duration=0,
                        output="",
                        error_output=str(e),
                        report_file=None,
                        metrics={},
                    )

        return results

    def _extract_report_file(self, output: str, framework_name: str) -> Optional[str]:
        """ä»è¾“å‡ºä¸­æå–æŠ¥å‘Šæ–‡ä»¶è·¯å¾„"""
        # æŸ¥æ‰¾å¸¸è§çš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„æ¨¡å¼
        patterns = [
            "æŠ¥å‘Šå·²ç”Ÿæˆ:",
            "æŠ¥å‘Šä¿å­˜åœ¨:",
            "Report generated:",
            "Report saved to:",
            f"{framework_name}_report_",
            "report_",
        ]

        lines = output.split("\n")
        for line in lines:
            for pattern in patterns:
                if pattern in line:
                    # å°è¯•æå–æ–‡ä»¶è·¯å¾„
                    parts = line.split()
                    for part in parts:
                        if ".md" in part or ".html" in part or ".txt" in part:
                            return part.strip()

        return None

    def _extract_metrics(self, output: str) -> Dict[str, Any]:
        """ä»è¾“å‡ºä¸­æå–æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}

        lines = output.split("\n")
        for line in lines:
            # æŸ¥æ‰¾æŒ‡æ ‡æ¨¡å¼
            if "å¹³å‡æ‰§è¡Œæ—¶é—´:" in line:
                try:
                    value = line.split(":")[1].strip().replace("ms", "")
                    metrics["avg_execution_time_ms"] = float(value)
                except:
                    pass

            elif "æˆåŠŸç‡:" in line:
                try:
                    value = line.split(":")[1].strip().replace("%", "")
                    metrics["success_rate"] = float(value)
                except:
                    pass

            elif "å†…å­˜ä½¿ç”¨:" in line:
                try:
                    value = line.split(":")[1].strip().replace("MB", "")
                    metrics["memory_usage_mb"] = float(value)
                except:
                    pass

        return metrics

    def _generate_comprehensive_report(
        self, results: Dict[str, TestExecutionResult], total_time: float
    ) -> str:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        report_file = os.path.join(
            self.reports_dir, f"comprehensive_test_report_{timestamp}.md"
        )

        # ç»Ÿè®¡æ•°æ®
        total_tests = len(results)
        successful_tests = sum(1 for r in results.values() if r.success)
        failed_tests = total_tests - successful_tests
        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0

        # æŒ‰ç±»åˆ«åˆ†ç»„
        category_stats = {}
        for framework_name, result in results.items():
            framework = self.test_frameworks.get(framework_name)
            if framework:
                category = framework.category
                if category not in category_stats:
                    category_stats[category] = {
                        "total": 0,
                        "successful": 0,
                        "duration": 0,
                    }

                category_stats[category]["total"] += 1
                if result.success:
                    category_stats[category]["successful"] += 1
                category_stats[category]["duration"] += result.duration

        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = f"""# Claude Enhancer 5.0 - ç»¼åˆæµ‹è¯•æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
**æµ‹è¯•æ‰§è¡Œå™¨**: Comprehensive Test Runner
**é¡¹ç›®è·¯å¾„**: {self.project_root}
**æ‰§è¡Œæ—¶é•¿**: {total_time:.2f}ç§’

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

### æ•´ä½“æµ‹è¯•ç»“æœ
"""

        # è®¡ç®—æ•´ä½“è¯„çº§
        if success_rate >= 95 and failed_tests == 0:
            grade = "A+ (ä¼˜ç§€)"
            grade_emoji = "ğŸŒŸ"
        elif success_rate >= 90:
            grade = "A (è‰¯å¥½)"
            grade_emoji = "âœ…"
        elif success_rate >= 80:
            grade = "B (åŠæ ¼)"
            grade_emoji = "âš ï¸"
        elif success_rate >= 70:
            grade = "C (éœ€æ”¹è¿›)"
            grade_emoji = "âŒ"
        else:
            grade = "D (ä¸åˆæ ¼)"
            grade_emoji = "ğŸš¨"

        report_content += f"""
{grade_emoji} **æ•´ä½“è¯„çº§**: {grade}
ğŸ“ˆ **æˆåŠŸç‡**: {success_rate:.1f}%
âœ… **æˆåŠŸæµ‹è¯•**: {successful_tests}
âŒ **å¤±è´¥æµ‹è¯•**: {failed_tests}
â±ï¸ **æ€»æ‰§è¡Œæ—¶é—´**: {total_time:.1f}ç§’

| æŒ‡æ ‡ | æ•°å€¼ | çŠ¶æ€ |
|------|------|------|
| æ€»æµ‹è¯•æ¡†æ¶ | {total_tests} | - |
| æˆåŠŸæ‰§è¡Œ | {successful_tests} | {'âœ…' if successful_tests == total_tests else 'âš ï¸'} |
| å¤±è´¥æ‰§è¡Œ | {failed_tests} | {'âœ…' if failed_tests == 0 else 'âŒ'} |
| å¹³å‡æ‰§è¡Œæ—¶é—´ | {total_time/total_tests:.1f}ç§’ | {'âœ…' if total_time/total_tests < 300 else 'âš ï¸'} |

## ğŸ§ª æµ‹è¯•æ¡†æ¶ç»“æœ

### è¯¦ç»†æ‰§è¡Œç»“æœ

| æµ‹è¯•æ¡†æ¶ | æè¿° | çŠ¶æ€ | æ‰§è¡Œæ—¶é—´ | æŠ¥å‘Šæ–‡ä»¶ |
|---------|------|------|----------|----------|
"""

        for framework_name, result in results.items():
            framework = self.test_frameworks.get(
                framework_name,
                TestFrameworkConfig(
                    name=framework_name,
                    description="Unknown",
                    script_path="",
                    category="unknown",
                    priority=5,
                    estimated_duration=0,
                    dependencies=[],
                    parallel_safe=True,
                ),
            )

            status_icon = "âœ…" if result.success else "âŒ"
            report_link = f"[æŠ¥å‘Š]({result.report_file})" if result.report_file else "æ— "

            report_content += f"| {framework.description} | {framework.category} | {status_icon} | {result.duration:.1f}s | {report_link} |\n"

        report_content += f"""
### æŒ‰ç±»åˆ«ç»Ÿè®¡

"""

        for category, stats in category_stats.items():
            category_success_rate = (
                (stats["successful"] / stats["total"] * 100)
                if stats["total"] > 0
                else 0
            )
            status_icon = (
                "âœ…"
                if category_success_rate >= 90
                else "âš ï¸"
                if category_success_rate >= 70
                else "âŒ"
            )

            report_content += f"""
#### {category.upper()} æµ‹è¯•
- **æˆåŠŸç‡**: {category_success_rate:.1f}% ({stats['successful']}/{stats['total']}) {status_icon}
- **æ€»è€—æ—¶**: {stats['duration']:.1f}ç§’
- **å¹³å‡è€—æ—¶**: {stats['duration']/stats['total']:.1f}ç§’
"""

        report_content += f"""
## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡åˆ†æ

### æ‰§è¡Œæ—¶é—´åˆ†æ
"""

        # æ€§èƒ½åˆ†æ
        execution_times = [r.duration for r in results.values()]
        if execution_times:
            fastest_test = min(execution_times)
            slowest_test = max(execution_times)
            avg_test_time = sum(execution_times) / len(execution_times)

            report_content += f"""
- **æœ€å¿«æµ‹è¯•**: {fastest_test:.1f}ç§’
- **æœ€æ…¢æµ‹è¯•**: {slowest_test:.1f}ç§’
- **å¹³å‡æ—¶é—´**: {avg_test_time:.1f}ç§’
- **æ—¶é—´æ ‡å‡†å·®**: {(sum((t - avg_test_time)**2 for t in execution_times) / len(execution_times))**0.5:.1f}ç§’
"""

        # æå–æ€§èƒ½æŒ‡æ ‡
        performance_metrics = {}
        for framework_name, result in results.items():
            if result.metrics:
                performance_metrics[framework_name] = result.metrics

        if performance_metrics:
            report_content += f"""
### æ€§èƒ½æŒ‡æ ‡æ±‡æ€»

| æµ‹è¯•æ¡†æ¶ | å¹³å‡æ‰§è¡Œæ—¶é—´ | æˆåŠŸç‡ | å†…å­˜ä½¿ç”¨ |
|---------|-------------|--------|----------|
"""

            for framework_name, metrics in performance_metrics.items():
                exec_time = metrics.get("avg_execution_time_ms", 0)
                success_rate_metric = metrics.get("success_rate", 0)
                memory_usage = metrics.get("memory_usage_mb", 0)

                report_content += f"| {framework_name} | {exec_time:.2f}ms | {success_rate_metric:.1f}% | {memory_usage:.2f}MB |\n"

        report_content += f"""
## âŒ å¤±è´¥åˆ†æ

"""

        failed_results = {
            name: result for name, result in results.items() if not result.success
        }

        if failed_results:
            report_content += "### å¤±è´¥çš„æµ‹è¯•æ¡†æ¶\n\n"

            for framework_name, result in failed_results.items():
                framework = self.test_frameworks.get(framework_name)
                report_content += f"""
#### {framework.description if framework else framework_name}
- **é”™è¯¯ä¿¡æ¯**: {result.error_output[:200]}{'...' if len(result.error_output) > 200 else ''}
- **æ‰§è¡Œæ—¶é—´**: {result.duration:.1f}ç§’
- **å»ºè®®**: {self._get_failure_recommendation(framework_name, result)}
"""
        else:
            report_content += "âœ… **æ— å¤±è´¥æµ‹è¯•** - æ‰€æœ‰æµ‹è¯•æ¡†æ¶éƒ½æ‰§è¡ŒæˆåŠŸ\n"

        report_content += f"""
## ğŸ¯ æ”¹è¿›å»ºè®®

### ç«‹å³å¤„ç†é¡¹
"""

        immediate_actions = []
        long_term_actions = []

        # åˆ†æå¤±è´¥å’Œæ€§èƒ½é—®é¢˜
        for framework_name, result in results.items():
            framework = self.test_frameworks.get(framework_name)

            if not result.success:
                immediate_actions.append(
                    f"ä¿®å¤ {framework.description if framework else framework_name} çš„æ‰§è¡Œé—®é¢˜"
                )

            elif (
                result.duration > framework.estimated_duration * 1.5
                if framework
                else False
            ):
                long_term_actions.append(f"ä¼˜åŒ– {framework.description} çš„æ‰§è¡Œæ€§èƒ½")

        if immediate_actions:
            for action in immediate_actions:
                report_content += f"- {action}\n"
        else:
            report_content += "âœ… **æ— éœ€ç«‹å³å¤„ç†çš„é—®é¢˜**\n"

        report_content += f"""
### é•¿æœŸä¼˜åŒ–å»ºè®®
"""

        if long_term_actions:
            for action in long_term_actions:
                report_content += f"- {action}\n"
        else:
            report_content += "- ç»§ç»­ä¿æŒå½“å‰ä¼˜ç§€çš„æµ‹è¯•æ‰§è¡ŒçŠ¶æ€\n"

        report_content += f"""
- è€ƒè™‘å¢åŠ æ›´å¤šå¹¶è¡Œå®‰å…¨çš„æµ‹è¯•æ¡†æ¶
- å®æ–½æµ‹è¯•ç»“æœç¼“å­˜æœºåˆ¶
- å»ºç«‹æµ‹è¯•æ€§èƒ½å›å½’ç›‘æ§
- æ‰©å±•CI/CDé›†æˆèƒ½åŠ›

## ğŸš€ CI/CD é›†æˆ

### Jenkins Pipeline ç¤ºä¾‹
```groovy
pipeline {{
    agent any
    stages {{
        stage('Comprehensive Tests') {{
            steps {{
                sh 'python test/comprehensive_test_runner.py --quick'
            }}
        }}
    }}
    post {{
        always {{
            publishHTML([
                allowMissing: false,
                alwaysLinkToLastBuild: true,
                keepAll: true,
                reportDir: 'test/comprehensive_reports',
                reportFiles: '*.md',
                reportName: 'Test Report'
            ])
        }}
    }}
}}
```

### GitHub Actions ç¤ºä¾‹
```yaml
name: Comprehensive Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'
    - name: Run comprehensive tests
      run: python test/comprehensive_test_runner.py
    - name: Upload test reports
      uses: actions/upload-artifact@v2
      with:
        name: test-reports
        path: test/comprehensive_reports/
```

## ğŸ† ç»“è®º

### æµ‹è¯•è´¨é‡è¯„ä¼°
{grade_emoji} **æ•´ä½“è¯„çº§**: {grade}

### å…³é”®å‘ç°
"""

        key_findings = []

        if success_rate >= 95:
            key_findings.append("âœ… æµ‹è¯•æ‰§è¡Œè´¨é‡ä¼˜ç§€ï¼Œæ‰€æœ‰æ¡†æ¶è¿è¡Œç¨³å®š")
        elif success_rate >= 90:
            key_findings.append("ğŸ‘ æµ‹è¯•æ‰§è¡Œè´¨é‡è‰¯å¥½ï¼Œå°‘æ•°æ¡†æ¶éœ€è¦å…³æ³¨")
        else:
            key_findings.append("âš ï¸ æµ‹è¯•æ‰§è¡Œè´¨é‡éœ€è¦æ”¹è¿›ï¼Œå¤šä¸ªæ¡†æ¶å­˜åœ¨é—®é¢˜")

        if failed_tests == 0:
            key_findings.append("âœ… æ‰€æœ‰æµ‹è¯•æ¡†æ¶éƒ½æˆåŠŸæ‰§è¡Œ")
        else:
            key_findings.append(f"ğŸ”§ {failed_tests}ä¸ªæµ‹è¯•æ¡†æ¶éœ€è¦ä¿®å¤")

        if (
            total_time
            < sum(f.estimated_duration for f in self.test_frameworks.values()) * 0.8
        ):
            key_findings.append("âš¡ æµ‹è¯•æ‰§è¡Œæ•ˆç‡ä¼˜ç§€")
        else:
            key_findings.append("ğŸŒ æµ‹è¯•æ‰§è¡Œæ•ˆç‡éœ€è¦ä¼˜åŒ–")

        for finding in key_findings:
            report_content += f"- {finding}\n"

        report_content += f"""
### éƒ¨ç½²å»ºè®®
"""

        if grade.startswith("A"):
            report_content += "**âœ… æ¨èéƒ¨ç½²**: æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼Œç³»ç»Ÿè´¨é‡ä¼˜ç§€ï¼Œå¯ä»¥å®‰å…¨éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒã€‚\n"
        elif grade.startswith("B"):
            report_content += "**ğŸ‘Œ å¯ä»¥éƒ¨ç½²**: å¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œå»ºè®®ä¿®å¤å¤±è´¥çš„æµ‹è¯•åéƒ¨ç½²ã€‚\n"
        elif grade.startswith("C"):
            report_content += "**âš ï¸ è°¨æ…éƒ¨ç½²**: å­˜åœ¨è¾ƒå¤šé—®é¢˜ï¼Œå»ºè®®å…ˆä¿®å¤å…³é”®é—®é¢˜å†è€ƒè™‘éƒ¨ç½²ã€‚\n"
        else:
            report_content += "**ğŸ›‘ ä¸å»ºè®®éƒ¨ç½²**: æµ‹è¯•å¤±è´¥ç‡è¿‡é«˜ï¼Œéœ€è¦é‡å¤§ä¿®å¤æ‰èƒ½éƒ¨ç½²ã€‚\n"

        report_content += f"""
---
*æŠ¥å‘Šç”± Claude Enhancer Comprehensive Test Runner è‡ªåŠ¨ç”Ÿæˆ*
*æµ‹è¯•å·¥ç¨‹å¸ˆ: Test Engineer Professional*
*ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}*
"""

        # ä¿å­˜æŠ¥å‘Š
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report_content)

        return report_file

    def _get_failure_recommendation(
        self, framework_name: str, result: TestExecutionResult
    ) -> str:
        """è·å–å¤±è´¥å»ºè®®"""
        error_msg = result.error_output.lower()

        if "timeout" in error_msg:
            return "å¢åŠ è¶…æ—¶æ—¶é—´æˆ–ä¼˜åŒ–æµ‹è¯•æ‰§è¡Œæ•ˆç‡"
        elif "permission" in error_msg:
            return "æ£€æŸ¥æ–‡ä»¶æƒé™å’Œæ‰§è¡Œæƒé™è®¾ç½®"
        elif "module" in error_msg or "import" in error_msg:
            return "æ£€æŸ¥Pythonä¾èµ–å’Œæ¨¡å—è·¯å¾„"
        elif "command not found" in error_msg:
            return "æ£€æŸ¥ç³»ç»Ÿä¾èµ–å’Œç¯å¢ƒé…ç½®"
        elif "connection" in error_msg or "network" in error_msg:
            return "æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®"
        else:
            return "æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—ï¼Œè¿›è¡Œå…·ä½“é—®é¢˜æ’æŸ¥"

    def _print_execution_summary(
        self,
        results: Dict[str, TestExecutionResult],
        total_time: float,
        report_file: str,
    ):
        """æ‰“å°æ‰§è¡Œæ‘˜è¦"""
        total_tests = len(results)
        successful_tests = sum(1 for r in results.values() if r.success)
        failed_tests = total_tests - successful_tests
        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0

        print("\n" + "=" * 60)
        print("ğŸ† ç»¼åˆæµ‹è¯•æ‰§è¡Œå®Œæˆ")
        print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’")
        print(f"ğŸ“Š ç»¼åˆæŠ¥å‘Š: {report_file}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"âœ… æˆåŠŸ: {successful_tests}/{total_tests}")

        if failed_tests > 0:
            print(f"âŒ å¤±è´¥: {failed_tests}")
            print("âš ï¸ å»ºè®®: æŸ¥çœ‹æŠ¥å‘Šäº†è§£å¤±è´¥è¯¦æƒ…")
        else:
            print("ğŸŒŸ æ‰€æœ‰æµ‹è¯•æ¡†æ¶éƒ½æˆåŠŸæ‰§è¡Œ!")

        print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Claude Enhancer 5.0 ç»¼åˆæµ‹è¯•æ‰§è¡Œå™¨")
    parser.add_argument("--project-root", help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--parallel", action="store_true", default=True, help="å¯ç”¨å¹¶è¡Œæ‰§è¡Œ")
    parser.add_argument("--no-parallel", action="store_true", help="ç¦ç”¨å¹¶è¡Œæ‰§è¡Œ")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    parser.add_argument("--framework", help="åªè¿è¡ŒæŒ‡å®šçš„æµ‹è¯•æ¡†æ¶")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æµ‹è¯•æ¡†æ¶")

    args = parser.parse_args()

    try:
        project_root = args.project_root or "/home/xx/dev/Claude Enhancer 5.0"
        orchestrator = TestOrchestrator(project_root)

        if args.list:
            print("ğŸ“‹ å¯ç”¨çš„æµ‹è¯•æ¡†æ¶:")
            for name, framework in orchestrator.test_frameworks.items():
                print(
                    f"  - {name}: {framework.description} ({framework.category}, {framework.estimated_duration}s)"
                )
            return

        if args.framework:
            # è¿è¡ŒæŒ‡å®šæ¡†æ¶
            framework = orchestrator.test_frameworks.get(args.framework)
            if not framework:
                print(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•æ¡†æ¶: {args.framework}")
                return

            result = orchestrator._execute_single_test(framework)
            success_icon = "âœ…" if result.success else "âŒ"
            print(
                f"{success_icon} {framework.description}: {'æˆåŠŸ' if result.success else 'å¤±è´¥'}"
            )
            return

        # è¿è¡Œç»¼åˆæµ‹è¯•
        parallel = args.parallel and not args.no_parallel
        results = orchestrator.run_all_tests(parallel=parallel, quick_mode=args.quick)

        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        failed_count = sum(1 for r in results.values() if not r.success)
        sys.exit(0 if failed_count == 0 else 1)

    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
