#!/usr/bin/env python3
"""
Claude Enhancer 5.0 - å›å½’æµ‹è¯•æ¡†æ¶
ä½œä¸ºtest-engineerè®¾è®¡çš„ä¸“ä¸šå›å½’æµ‹è¯•ç³»ç»Ÿ

åŠŸèƒ½ç‰¹æ€§:
1. æ€§èƒ½å›å½’æ£€æµ‹
2. åŠŸèƒ½å›å½’éªŒè¯
3. é…ç½®å˜æ›´å½±å“åˆ†æ
4. åŸºçº¿ç®¡ç†å’Œç‰ˆæœ¬å¯¹æ¯”
5. è‡ªåŠ¨åŒ–å›å½’æŠ¥å‘Š
6. Gité›†æˆçš„å˜æ›´è¿½è¸ª
"""

import os
import sys
import json
import time
import hashlib
import subprocess
import statistics
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import shutil
import tempfile
from datetime import datetime, timedelta


@dataclass
class BaselineMetrics:
    """åŸºçº¿æ€§èƒ½æŒ‡æ ‡"""

    test_name: str
    avg_execution_time_ms: float
    success_rate: float
    memory_usage_mb: float
    cpu_usage_percent: float
    timestamp: float
    version: str
    git_commit: Optional[str] = None


@dataclass
class RegressionResult:
    """å›å½’æµ‹è¯•ç»“æœ"""

    test_name: str
    baseline_value: float
    current_value: float
    change_percent: float
    regression_detected: bool
    severity: str  # "minor", "moderate", "severe", "critical"
    recommendation: str


@dataclass
class ConfigurationChange:
    """é…ç½®å˜æ›´è®°å½•"""

    file_path: str
    change_type: str  # "modified", "added", "deleted"
    old_checksum: Optional[str]
    new_checksum: Optional[str]
    impact_severity: str


class BaselineManager:
    """åŸºçº¿ç®¡ç†å™¨"""

    def __init__(self, project_root: str):
        self.project_root = project_root
        self.baseline_dir = os.path.join(project_root, "test", "baselines")
        self.current_baseline_file = os.path.join(
            self.baseline_dir, "current_baseline.json"
        )
        self.historical_baselines_dir = os.path.join(self.baseline_dir, "historical")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.baseline_dir, exist_ok=True)
        os.makedirs(self.historical_baselines_dir, exist_ok=True)

    def create_baseline(self, metrics: List[BaselineMetrics], version: str) -> str:
        """åˆ›å»ºæ–°çš„æ€§èƒ½åŸºçº¿"""
        timestamp = time.time()
        git_commit = self._get_current_git_commit()

        baseline_data = {
            "created_at": timestamp,
            "created_date": datetime.fromtimestamp(timestamp).isoformat(),
            "version": version,
            "git_commit": git_commit,
            "metrics": [asdict(metric) for metric in metrics],
            "configuration_checksums": self._generate_config_checksums(),
            "critical_files_checksums": self._generate_critical_files_checksums(),
        }

        # ä¿å­˜å½“å‰åŸºçº¿
        with open(self.current_baseline_file, "w") as f:
            json.dump(baseline_data, f, indent=2)

        # ä¿å­˜å†å²åŸºçº¿
        baseline_filename = f"baseline_{version}_{int(timestamp)}.json"
        historical_file = os.path.join(self.historical_baselines_dir, baseline_filename)

        with open(historical_file, "w") as f:
            json.dump(baseline_data, f, indent=2)

        print(f"âœ… åŸºçº¿å·²åˆ›å»º: {version}")
        print(f"ğŸ“ åŸºçº¿æ–‡ä»¶: {self.current_baseline_file}")
        print(f"ğŸ“š å†å²è®°å½•: {historical_file}")

        return self.current_baseline_file

    def load_baseline(self, version: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """åŠ è½½åŸºçº¿æ•°æ®"""
        if version:
            pass  # Auto-fixed empty block
            # åŠ è½½æŒ‡å®šç‰ˆæœ¬çš„åŸºçº¿
            baseline_files = list(
                Path(self.historical_baselines_dir).glob(f"baseline_{version}_*.json")
            )
            if baseline_files:
                baseline_file = baseline_files[0]  # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°ç‰ˆæœ¬ {version} çš„åŸºçº¿")
                return None
        else:
            pass  # Auto-fixed empty block
            # åŠ è½½å½“å‰åŸºçº¿
            baseline_file = self.current_baseline_file

        if not os.path.exists(baseline_file):
            print(f"âš ï¸ åŸºçº¿æ–‡ä»¶ä¸å­˜åœ¨: {baseline_file}")
            return None

        try:
            with open(baseline_file, "r") as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½åŸºçº¿å¤±è´¥: {e}")
            return None

    def list_baselines(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åŸºçº¿"""
        baselines = []

        # å½“å‰åŸºçº¿
        current = self.load_baseline()
        if current:
            baselines.append(
                {
                    "type": "current",
                    "version": current.get("version", "unknown"),
                    "created_date": current.get("created_date", "unknown"),
                    "git_commit": current.get("git_commit", "unknown"),
                }
            )

        # å†å²åŸºçº¿
        for baseline_file in Path(self.historical_baselines_dir).glob(
            "baseline_*.json"
        ):
            try:
                with open(baseline_file, "r") as f:
                    baseline_data = json.load(f)
                    baselines.append(
                        {
                            "type": "historical",
                            "version": baseline_data.get("version", "unknown"),
                            "created_date": baseline_data.get(
                                "created_date", "unknown"
                            ),
                            "git_commit": baseline_data.get("git_commit", "unknown"),
                            "file": str(baseline_file),
                        }
                    )
            except Exception:
                continue

        return sorted(baselines, key=lambda x: x.get("created_date", ""), reverse=True)

    def _get_current_git_commit(self) -> Optional[str]:
        """è·å–å½“å‰Gitæäº¤å“ˆå¸Œ"""
        try:
            result = subprocess.run(
                ["git", "rev-parse", "HEAD"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception:
            pass
        return None

    def _generate_config_checksums(self) -> Dict[str, str]:
        """ç”Ÿæˆé…ç½®æ–‡ä»¶æ ¡éªŒå’Œ"""
        config_files = [
            ".claude/settings.json",
            ".claude/config.yaml",
            ".claude/hooks/config.yaml",
            "test/test_config.yaml",
        ]

        checksums = {}
        for config_file in config_files:
            file_path = os.path.join(self.project_root, config_file)
            if os.path.exists(file_path):
                checksums[config_file] = self._calculate_file_checksum(file_path)

        return checksums

    def _generate_critical_files_checksums(self) -> Dict[str, str]:
        """ç”Ÿæˆå…³é”®æ–‡ä»¶æ ¡éªŒå’Œ"""
        critical_files = [
            ".claude/hooks/quality_gate.sh",
            ".claude/hooks/smart_agent_selector.sh",
            ".claude/core/lazy_orchestrator.py",
            ".claude/core/engine.py",
        ]

        checksums = {}
        for critical_file in critical_files:
            file_path = os.path.join(self.project_root, critical_file)
            if os.path.exists(file_path):
                checksums[critical_file] = self._calculate_file_checksum(file_path)

        return checksums

    def _calculate_file_checksum(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶MD5æ ¡éªŒå’Œ"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return "error"


class PerformanceRegressionDetector:
    """æ€§èƒ½å›å½’æ£€æµ‹å™¨"""

    def __init__(self, project_root: str):
        self.project_root = project_root
        self.baseline_manager = BaselineManager(project_root)

        # å›å½’é˜ˆå€¼é…ç½®
        self.regression_thresholds = {
            "minor": 5.0,  # 5% æ€§èƒ½ä¸‹é™
            "moderate": 15.0,  # 15% æ€§èƒ½ä¸‹é™
            "severe": 30.0,  # 30% æ€§èƒ½ä¸‹é™
            "critical": 50.0,  # 50% æ€§èƒ½ä¸‹é™
        }

    def detect_performance_regression(
        self, current_metrics: List[BaselineMetrics]
    ) -> List[RegressionResult]:
        """æ£€æµ‹æ€§èƒ½å›å½’"""
        print("ğŸ” å¼€å§‹æ€§èƒ½å›å½’æ£€æµ‹...")

        baseline_data = self.baseline_manager.load_baseline()
        if not baseline_data:
            print("âš ï¸ æ— åŸºçº¿æ•°æ®ï¼Œæ— æ³•è¿›è¡Œå›å½’æ£€æµ‹")
            return []

        baseline_metrics = {
            metric["test_name"]: metric for metric in baseline_data.get("metrics", [])
        }

        regression_results = []

        for current_metric in current_metrics:
            test_name = current_metric.test_name
            baseline_metric = baseline_metrics.get(test_name)

            if not baseline_metric:
                print(f"âš ï¸ æµ‹è¯• {test_name} æ— åŸºçº¿æ•°æ®ï¼Œè·³è¿‡å›å½’æ£€æµ‹")
                continue

            # æ£€æµ‹æ‰§è¡Œæ—¶é—´å›å½’
            time_regression = self._detect_metric_regression(
                test_name + "_execution_time",
                baseline_metric["avg_execution_time_ms"],
                current_metric.avg_execution_time_ms,
                "æ‰§è¡Œæ—¶é—´",
            )
            if time_regression:
                regression_results.append(time_regression)

            # æ£€æµ‹æˆåŠŸç‡å›å½’
            success_regression = self._detect_metric_regression(
                test_name + "_success_rate",
                baseline_metric["success_rate"],
                current_metric.success_rate,
                "æˆåŠŸç‡",
                is_success_rate=True,
            )
            if success_regression:
                regression_results.append(success_regression)

            # æ£€æµ‹å†…å­˜ä½¿ç”¨å›å½’
            memory_regression = self._detect_metric_regression(
                test_name + "_memory_usage",
                baseline_metric["memory_usage_mb"],
                current_metric.memory_usage_mb,
                "å†…å­˜ä½¿ç”¨",
            )
            if memory_regression:
                regression_results.append(memory_regression)

        return regression_results

    def _detect_metric_regression(
        self,
        test_name: str,
        baseline_value: float,
        current_value: float,
        metric_type: str,
        is_success_rate: bool = False,
    ) -> Optional[RegressionResult]:
        """æ£€æµ‹å•é¡¹æŒ‡æ ‡å›å½’"""
        if baseline_value == 0:
            return None

        if is_success_rate:
            pass  # Auto-fixed empty block
            # æˆåŠŸç‡ä¸‹é™æ£€æµ‹
            change_percent = (baseline_value - current_value) / baseline_value * 100
            regression_detected = change_percent > 1.0  # æˆåŠŸç‡ä¸‹é™è¶…è¿‡1%
        else:
            pass  # Auto-fixed empty block
            # æ€§èƒ½æŒ‡æ ‡æ¶åŒ–æ£€æµ‹ï¼ˆæ—¶é—´å¢åŠ ã€å†…å­˜å¢åŠ ï¼‰
            change_percent = (current_value - baseline_value) / baseline_value * 100
            regression_detected = change_percent > self.regression_thresholds["minor"]

        if not regression_detected:
            return None

        # ç¡®å®šä¸¥é‡ç¨‹åº¦
        severity = self._determine_severity(abs(change_percent), is_success_rate)

        # ç”Ÿæˆå»ºè®®
        recommendation = self._generate_recommendation(
            metric_type, severity, change_percent
        )

        return RegressionResult(
            test_name=test_name,
            baseline_value=baseline_value,
            current_value=current_value,
            change_percent=change_percent,
            regression_detected=True,
            severity=severity,
            recommendation=recommendation,
        )

    def _determine_severity(
        self, change_percent: float, is_success_rate: bool = False
    ) -> str:
        """ç¡®å®šå›å½’ä¸¥é‡ç¨‹åº¦"""
        if is_success_rate:
            pass  # Auto-fixed empty block
            # æˆåŠŸç‡å›å½’ä¸¥é‡ç¨‹åº¦
            if change_percent > 10:
                return "critical"
            elif change_percent > 5:
                return "severe"
            elif change_percent > 2:
                return "moderate"
            else:
                return "minor"
        else:
            pass  # Auto-fixed empty block
            # æ€§èƒ½å›å½’ä¸¥é‡ç¨‹åº¦
            if change_percent > self.regression_thresholds["critical"]:
                return "critical"
            elif change_percent > self.regression_thresholds["severe"]:
                return "severe"
            elif change_percent > self.regression_thresholds["moderate"]:
                return "moderate"
            else:
                return "minor"

    def _generate_recommendation(
        self, metric_type: str, severity: str, change_percent: float
    ) -> str:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = {
            "æ‰§è¡Œæ—¶é—´": {
                "critical": "ç«‹å³åœæ­¢éƒ¨ç½²ï¼æ‰§è¡Œæ—¶é—´ä¸¥é‡æ¶åŒ–ï¼Œéœ€è¦ç´§æ€¥ä¼˜åŒ–ç®—æ³•",
                "severe": "éœ€è¦ç«‹å³ä¼˜åŒ–ï¼Œè€ƒè™‘ç®—æ³•é‡æ„æˆ–ç¼“å­˜æœºåˆ¶",
                "moderate": "å»ºè®®ä¼˜åŒ–æ€§èƒ½ï¼Œæ£€æŸ¥æœ€è¿‘çš„ä»£ç å˜æ›´",
                "minor": "è½»å¾®æ€§èƒ½ä¸‹é™ï¼Œå»ºè®®ç›‘æ§è¶‹åŠ¿",
            },
            "æˆåŠŸç‡": {
                "critical": "ç«‹å³å›æ»šï¼æˆåŠŸç‡ä¸¥é‡ä¸‹é™ï¼Œç³»ç»Ÿå¯é æ€§å—æŸ",
                "severe": "éœ€è¦ç«‹å³ä¿®å¤ï¼Œæ£€æŸ¥é”™è¯¯å¤„ç†é€»è¾‘",
                "moderate": "éœ€è¦è°ƒæŸ¥å¤±è´¥åŸå› ï¼Œæ”¹è¿›é”™è¯¯å¤„ç†",
                "minor": "å»ºè®®æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹å’Œè¾¹ç•Œæ¡ä»¶",
            },
            "å†…å­˜ä½¿ç”¨": {
                "critical": "ä¸¥é‡å†…å­˜æ³„æ¼ï¼ç«‹å³è°ƒæŸ¥å†…å­˜ç®¡ç†é—®é¢˜",
                "severe": "éœ€è¦ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼",
                "moderate": "å»ºè®®ä¼˜åŒ–å†…å­˜ä½¿ç”¨æ•ˆç‡",
                "minor": "è½»å¾®å†…å­˜å¢é•¿ï¼Œå»ºè®®æŒç»­ç›‘æ§",
            },
        }

        return recommendations.get(metric_type, {}).get(
            severity, f"{metric_type}å‡ºç°{severity}çº§åˆ«å›å½’ï¼Œå˜åŒ–{change_percent:.1f}%"
        )


class ConfigurationChangeDetector:
    """é…ç½®å˜æ›´æ£€æµ‹å™¨"""

    def __init__(self, project_root: str):
        self.project_root = project_root
        self.baseline_manager = BaselineManager(project_root)

    def detect_configuration_changes(self) -> List[ConfigurationChange]:
        """æ£€æµ‹é…ç½®å˜æ›´"""
        print("ğŸ”§ æ£€æµ‹é…ç½®æ–‡ä»¶å˜æ›´...")

        baseline_data = self.baseline_manager.load_baseline()
        if not baseline_data:
            print("âš ï¸ æ— åŸºçº¿æ•°æ®ï¼Œæ— æ³•æ£€æµ‹é…ç½®å˜æ›´")
            return []

        baseline_checksums = baseline_data.get("configuration_checksums", {})
        current_checksums = self.baseline_manager._generate_config_checksums()

        changes = []

        # æ£€æŸ¥æ‰€æœ‰é…ç½®æ–‡ä»¶
        all_config_files = set(baseline_checksums.keys()) | set(
            current_checksums.keys()
        )

        for config_file in all_config_files:
            baseline_checksum = baseline_checksums.get(config_file)
            current_checksum = current_checksums.get(config_file)

            if not baseline_checksum and current_checksum:
                pass  # Auto-fixed empty block
                # æ–°å¢é…ç½®æ–‡ä»¶
                changes.append(
                    ConfigurationChange(
                        file_path=config_file,
                        change_type="added",
                        old_checksum=None,
                        new_checksum=current_checksum,
                        impact_severity=self._assess_config_impact(
                            config_file, "added"
                        ),
                    )
                )

            elif baseline_checksum and not current_checksum:
                pass  # Auto-fixed empty block
                # åˆ é™¤é…ç½®æ–‡ä»¶
                changes.append(
                    ConfigurationChange(
                        file_path=config_file,
                        change_type="deleted",
                        old_checksum=baseline_checksum,
                        new_checksum=None,
                        impact_severity=self._assess_config_impact(
                            config_file, "deleted"
                        ),
                    )
                )

            elif baseline_checksum != current_checksum:
                pass  # Auto-fixed empty block
                # ä¿®æ”¹é…ç½®æ–‡ä»¶
                changes.append(
                    ConfigurationChange(
                        file_path=config_file,
                        change_type="modified",
                        old_checksum=baseline_checksum,
                        new_checksum=current_checksum,
                        impact_severity=self._assess_config_impact(
                            config_file, "modified"
                        ),
                    )
                )

        return changes

    def _assess_config_impact(self, config_file: str, change_type: str) -> str:
        """è¯„ä¼°é…ç½®å˜æ›´å½±å“"""
        critical_configs = [".claude/settings.json", ".claude/config.yaml"]
        important_configs = [".claude/hooks/config.yaml"]

        if config_file in critical_configs:
            if change_type == "deleted":
                return "critical"
            else:
                return "high"
        elif config_file in important_configs:
            return "medium"
        else:
            return "low"


class FunctionalRegressionTester:
    """åŠŸèƒ½å›å½’æµ‹è¯•å™¨"""

    def __init__(self, project_root: str):
        self.project_root = project_root
        self.hooks_dir = os.path.join(project_root, ".claude", "hooks")
        self.baseline_manager = BaselineManager(project_root)

    def test_functional_regression(self) -> List[Dict[str, Any]]:
        """æµ‹è¯•åŠŸèƒ½å›å½’"""
        print("ğŸ§ª å¼€å§‹åŠŸèƒ½å›å½’æµ‹è¯•...")

        baseline_data = self.baseline_manager.load_baseline()
        if not baseline_data:
            print("âš ï¸ æ— åŸºçº¿æ•°æ®ï¼Œæ‰§è¡ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•")
            return self._run_basic_functional_tests()

        baseline_checksums = baseline_data.get("critical_files_checksums", {})
        current_checksums = self.baseline_manager._generate_critical_files_checksums()

        regression_results = []

        # æ£€æŸ¥å…³é”®æ–‡ä»¶å˜æ›´
        for file_path, baseline_checksum in baseline_checksums.items():
            current_checksum = current_checksums.get(file_path)

            if current_checksum != baseline_checksum:
                print(f"ğŸ” æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´: {file_path}")
                # å¯¹å˜æ›´çš„æ–‡ä»¶è¿›è¡ŒåŠŸèƒ½æµ‹è¯•
                functional_result = self._test_file_functionality(file_path)
                regression_results.append(
                    {
                        "file_path": file_path,
                        "change_detected": True,
                        "functional_test_result": functional_result,
                    }
                )
            else:
                regression_results.append(
                    {
                        "file_path": file_path,
                        "change_detected": False,
                        "functional_test_result": {
                            "status": "skipped",
                            "reason": "no_changes",
                        },
                    }
                )

        return regression_results

    def _test_file_functionality(self, file_path: str) -> Dict[str, Any]:
        """æµ‹è¯•æ–‡ä»¶åŠŸèƒ½"""
        full_path = os.path.join(self.project_root, file_path)

        if file_path.endswith(".sh"):
            return self._test_shell_script(full_path)
        elif file_path.endswith(".py"):
            return self._test_python_module(full_path)
        else:
            return {"status": "skipped", "reason": "unsupported_file_type"}

    def _test_shell_script(self, script_path: str) -> Dict[str, Any]:
        """æµ‹è¯•Shellè„šæœ¬åŠŸèƒ½"""
        test_cases = [
            '{"prompt": "test functionality"}',
            '{"prompt": "implement feature"}',
            '{"prompt": ""}',  # è¾¹ç•Œæ¡ä»¶æµ‹è¯•
        ]

        results = []
        for test_input in test_cases:
            try:
                result = subprocess.run(
                    [script_path],
                    input=test_input,
                    text=True,
                    capture_output=True,
                    timeout=10,
                )

                results.append(
                    {
                        "input": test_input,
                        "return_code": result.returncode,
                        "success": result.returncode == 0,
                        "stdout": result.stdout[:100],  # é™åˆ¶è¾“å‡ºé•¿åº¦
                        "stderr": result.stderr[:100],
                    }
                )

            except subprocess.TimeoutExpired:
                results.append(
                    {
                        "input": test_input,
                        "return_code": -1,
                        "success": False,
                        "error": "timeout",
                    }
                )
            except Exception as e:
                results.append(
                    {
                        "input": test_input,
                        "return_code": -1,
                        "success": False,
                        "error": str(e),
                    }
                )

        success_count = sum(1 for r in results if r["success"])
        success_rate = success_count / len(results)

        return {
            "status": "completed",
            "test_cases": len(results),
            "success_count": success_count,
            "success_rate": success_rate,
            "results": results,
        }

    def _test_python_module(self, module_path: str) -> Dict[str, Any]:
        """æµ‹è¯•Pythonæ¨¡å—åŠŸèƒ½"""
        try:
            pass  # Auto-fixed empty block
            # ç®€å•çš„å¯¼å…¥æµ‹è¯•
            import importlib.util

            spec = importlib.util.spec_from_file_location("test_module", module_path)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)

            return {"status": "completed", "success": True, "message": "æ¨¡å—å¯¼å…¥æˆåŠŸ"}

        except Exception as e:
            return {"status": "failed", "success": False, "error": str(e)}

    def _run_basic_functional_tests(self) -> List[Dict[str, Any]]:
        """è¿è¡ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•"""
        critical_files = [
            ".claude/hooks/quality_gate.sh",
            ".claude/hooks/smart_agent_selector.sh",
            ".claude/core/lazy_orchestrator.py",
        ]

        results = []
        for file_path in critical_files:
            full_path = os.path.join(self.project_root, file_path)
            if os.path.exists(full_path):
                functional_result = self._test_file_functionality(file_path)
                results.append(
                    {
                        "file_path": file_path,
                        "change_detected": False,
                        "functional_test_result": functional_result,
                    }
                )

        return results


class RegressionReportGenerator:
    """å›å½’æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

    def generate_regression_report(
        self,
        performance_regressions: List[RegressionResult],
        config_changes: List[ConfigurationChange],
        functional_results: List[Dict[str, Any]],
        timestamp: str,
    ) -> str:
        """ç”Ÿæˆå›å½’æµ‹è¯•æŠ¥å‘Š"""
        report_file = self.output_dir / f"regression_report_{timestamp}.md"

        with open(report_file, "w", encoding="utf-8") as f:
            f.write(
                self._generate_regression_markdown(
                    performance_regressions,
                    config_changes,
                    functional_results,
                    timestamp,
                )
            )

        print(f"ğŸ“Š å›å½’æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return str(report_file)

    def _generate_regression_markdown(
        self,
        performance_regressions: List[RegressionResult],
        config_changes: List[ConfigurationChange],
        functional_results: List[Dict[str, Any]],
        timestamp: str,
    ) -> str:
        """ç”Ÿæˆå›å½’æµ‹è¯•MarkdownæŠ¥å‘Š"""
        # ç»Ÿè®¡æ•°æ®
        critical_regressions = len(
            [r for r in performance_regressions if r.severity == "critical"]
        )
        severe_regressions = len(
            [r for r in performance_regressions if r.severity == "severe"]
        )
        total_regressions = len(performance_regressions)

        critical_config_changes = len(
            [c for c in config_changes if c.impact_severity == "critical"]
        )
        high_impact_changes = len(
            [c for c in config_changes if c.impact_severity == "high"]
        )

        functional_failures = len(
            [
                f
                for f in functional_results
                if not f.get("functional_test_result", {}).get("success", True)
            ]
        )

        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if (
            critical_regressions > 0
            or critical_config_changes > 0
            or functional_failures > 0
        ):
            overall_status = "ğŸš¨ CRITICAL"
            overall_color = "red"
        elif severe_regressions > 0 or high_impact_changes > 0:
            overall_status = "âš ï¸ WARNING"
            overall_color = "orange"
        elif total_regressions > 0 or len(config_changes) > 0:
            overall_status = "ğŸ“‹ REVIEW"
            overall_color = "yellow"
        else:
            overall_status = "âœ… PASS"
            overall_color = "green"

        report = f"""# Claude Enhancer 5.0 - å›å½’æµ‹è¯•æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {timestamp}
**æ•´ä½“çŠ¶æ€**: {overall_status}
**ç³»ç»Ÿç¯å¢ƒ**: {os.uname().sysname} {os.uname().release}

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

| æµ‹è¯•ç±»å‹ | æ£€æµ‹é¡¹ç›® | é—®é¢˜æ•°é‡ | çŠ¶æ€ |
|---------|----------|----------|------|
| æ€§èƒ½å›å½’ | {len(performance_regressions)} é¡¹æŒ‡æ ‡ | {critical_regressions} ä¸¥é‡ + {severe_regressions} é‡è¦ | {'ğŸš¨' if critical_regressions > 0 else 'âš ï¸' if severe_regressions > 0 else 'âœ…'} |
| é…ç½®å˜æ›´ | {len(config_changes)} ä¸ªæ–‡ä»¶ | {critical_config_changes} å…³é”® + {high_impact_changes} é‡è¦ | {'ğŸš¨' if critical_config_changes > 0 else 'âš ï¸' if high_impact_changes > 0 else 'âœ…'} |
| åŠŸèƒ½å›å½’ | {len(functional_results)} ä¸ªç»„ä»¶ | {functional_failures} ä¸ªå¤±è´¥ | {'ğŸš¨' if functional_failures > 0 else 'âœ…'} |

## ğŸ”¥ æ€§èƒ½å›å½’åˆ†æ

"""

        if performance_regressions:
            report += """### æ£€æµ‹åˆ°çš„æ€§èƒ½å›å½’

| æµ‹è¯•é¡¹ç›® | åŸºçº¿å€¼ | å½“å‰å€¼ | å˜åŒ– | ä¸¥é‡ç¨‹åº¦ | å»ºè®® |
|---------|--------|--------|------|----------|------|
"""
            for regression in performance_regressions:
                severity_icon = {
                    "critical": "ğŸš¨",
                    "severe": "âš ï¸",
                    "moderate": "ğŸ“‹",
                    "minor": "â„¹ï¸",
                }.get(regression.severity, "â“")

                if "execution_time" in regression.test_name:
                    unit = "ms"
                elif "success_rate" in regression.test_name:
                    unit = "%"
                    regression.baseline_value *= 100
                    regression.current_value *= 100
                else:
                    unit = "MB"

                report += f"| {regression.test_name} | {regression.baseline_value:.2f}{unit} | {regression.current_value:.2f}{unit} | {regression.change_percent:+.1f}% | {severity_icon} {regression.severity} | {regression.recommendation} |\n"

        else:
            report += "âœ… **æ— æ€§èƒ½å›å½’æ£€æµ‹åˆ°**\n"

        report += f"""
## ğŸ”§ é…ç½®å˜æ›´åˆ†æ

"""

        if config_changes:
            report += """### æ£€æµ‹åˆ°çš„é…ç½®å˜æ›´

| æ–‡ä»¶è·¯å¾„ | å˜æ›´ç±»å‹ | å½±å“ç¨‹åº¦ | å»ºè®® |
|---------|----------|----------|------|
"""
            for change in config_changes:
                impact_icon = {
                    "critical": "ğŸš¨",
                    "high": "âš ï¸",
                    "medium": "ğŸ“‹",
                    "low": "â„¹ï¸",
                }.get(change.impact_severity, "â“")

                change_icon = {"added": "â•", "modified": "ğŸ“", "deleted": "ğŸ—‘ï¸"}.get(
                    change.change_type, "â“"
                )

                recommendation = self._get_config_change_recommendation(change)

                report += f"| {change.file_path} | {change_icon} {change.change_type} | {impact_icon} {change.impact_severity} | {recommendation} |\n"

        else:
            report += "âœ… **æ— é…ç½®å˜æ›´æ£€æµ‹åˆ°**\n"

        report += f"""
## ğŸ§ª åŠŸèƒ½å›å½’æµ‹è¯•

"""

        if functional_results:
            for result in functional_results:
                file_path = result["file_path"]
                change_detected = result["change_detected"]
                test_result = result["functional_test_result"]

                status_icon = "ğŸ”" if change_detected else "âœ…"
                test_status = test_result.get("status", "unknown")

                report += f"""### {file_path}
- **æ–‡ä»¶å˜æ›´**: {'æ˜¯' if change_detected else 'å¦'} {status_icon}
- **æµ‹è¯•çŠ¶æ€**: {test_status}
"""

                if test_status == "completed":
                    success_rate = test_result.get("success_rate", 0)
                    test_cases = test_result.get("test_cases", 0)
                    report += f"- **æµ‹è¯•ç”¨ä¾‹**: {test_cases} ä¸ª\n"
                    report += f"- **æˆåŠŸç‡**: {success_rate:.1%}\n"

                    if success_rate < 1.0:
                        report += "- **âš ï¸ å­˜åœ¨åŠŸèƒ½é—®é¢˜ï¼Œéœ€è¦è°ƒæŸ¥**\n"

                elif test_status == "failed":
                    error = test_result.get("error", "unknown")
                    report += f"- **âŒ æµ‹è¯•å¤±è´¥**: {error}\n"

                report += "\n"

        else:
            report += "â„¹ï¸ **æ— åŠŸèƒ½æµ‹è¯•ç»“æœ**\n"

        report += f"""
## ğŸ¯ è¡ŒåŠ¨å»ºè®®

### ç«‹å³å¤„ç†é¡¹
"""

        immediate_actions = []

        # ä¸¥é‡æ€§èƒ½å›å½’
        for regression in performance_regressions:
            if regression.severity in ["critical", "severe"]:
                immediate_actions.append(
                    f"- **{regression.test_name}**: {regression.recommendation}"
                )

        # å…³é”®é…ç½®å˜æ›´
        for change in config_changes:
            if change.impact_severity in ["critical", "high"]:
                immediate_actions.append(
                    f"- **{change.file_path}**: {self._get_config_change_recommendation(change)}"
                )

        # åŠŸèƒ½å¤±è´¥
        for result in functional_results:
            test_result = result["functional_test_result"]
            if (
                test_result.get("status") == "failed"
                or test_result.get("success_rate", 1) < 0.8
            ):
                immediate_actions.append(f"- **{result['file_path']}**: ä¿®å¤åŠŸèƒ½é—®é¢˜")

        if immediate_actions:
            for action in immediate_actions:
                report += f"{action}\n"
        else:
            report += "âœ… **æ— éœ€ç«‹å³å¤„ç†çš„é—®é¢˜**\n"

        report += f"""
### ç›‘æ§å»ºè®®
1. æŒç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡ï¼Œå»ºç«‹è‡ªåŠ¨åŒ–å‘Šè­¦
2. å®šæœŸæ›´æ–°å›å½’æµ‹è¯•åŸºçº¿
3. å®æ–½é…ç½®å˜æ›´å®¡æ‰¹æµç¨‹
4. å¢å¼ºåŠŸèƒ½æµ‹è¯•è¦†ç›–ç‡

### è´¨é‡ä¿è¯
1. æ‰€æœ‰æ€§èƒ½å›å½’éƒ½éœ€è¦è¯¦ç»†åˆ†æ
2. å…³é”®é…ç½®å˜æ›´éœ€è¦å›¢é˜Ÿè¯„å®¡
3. åŠŸèƒ½å¤±è´¥å¿…é¡»åœ¨éƒ¨ç½²å‰ä¿®å¤
4. å»ºç«‹æ€§èƒ½é¢„ç®—å’Œé˜ˆå€¼ç®¡ç†

## ğŸ“ˆ è¶‹åŠ¿åˆ†æ

### æ€§èƒ½è¶‹åŠ¿
- **æ‰§è¡Œæ—¶é—´**: {'ä¸Šå‡è¶‹åŠ¿ âš ï¸' if any(r.change_percent > 0 for r in performance_regressions if 'execution_time' in r.test_name) else 'ç¨³å®š âœ…'}
- **æˆåŠŸç‡**: {'ä¸‹é™è¶‹åŠ¿ âš ï¸' if any(r.change_percent > 0 for r in performance_regressions if 'success_rate' in r.test_name) else 'ç¨³å®š âœ…'}
- **å†…å­˜ä½¿ç”¨**: {'å¢é•¿è¶‹åŠ¿ âš ï¸' if any(r.change_percent > 0 for r in performance_regressions if 'memory' in r.test_name) else 'ç¨³å®š âœ…'}

## ğŸ† ç»“è®º

### å›å½’æµ‹è¯•è¯„ä¼°
{overall_status}

### éƒ¨ç½²å»ºè®®
"""

        if overall_status.startswith("ğŸš¨"):
            report += "**ğŸ›‘ ä¸å»ºè®®éƒ¨ç½²**ï¼šå‘ç°ä¸¥é‡å›å½’é—®é¢˜ï¼Œéœ€è¦ç«‹å³ä¿®å¤åå†éƒ¨ç½²ã€‚\n"
        elif overall_status.startswith("âš ï¸"):
            report += "**âš ï¸ è°¨æ…éƒ¨ç½²**ï¼šå­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå»ºè®®ä¿®å¤åéƒ¨ç½²ï¼Œæˆ–åœ¨ä½é£é™©ç¯å¢ƒå…ˆè¡ŒéªŒè¯ã€‚\n"
        elif overall_status.startswith("ğŸ“‹"):
            report += "**ğŸ“‹ è¯„ä¼°åéƒ¨ç½²**ï¼šæœ‰è½»å¾®å˜æ›´ï¼Œå»ºè®®è¯„ä¼°å½±å“åå†³å®šæ˜¯å¦éƒ¨ç½²ã€‚\n"
        else:
            report += "**âœ… å¯ä»¥éƒ¨ç½²**ï¼šæ— å›å½’é—®é¢˜æ£€æµ‹åˆ°ï¼Œç³»ç»ŸçŠ¶æ€è‰¯å¥½ã€‚\n"

        report += f"""
---
*æŠ¥å‘Šç”± Claude Enhancer Regression Test Framework è‡ªåŠ¨ç”Ÿæˆ*
*æµ‹è¯•å·¥ç¨‹å¸ˆ: Test Engineer Professional*
*ç”Ÿæˆæ—¶é—´: {timestamp}*
"""

        return report

    def _get_config_change_recommendation(self, change: ConfigurationChange) -> str:
        """è·å–é…ç½®å˜æ›´å»ºè®®"""
        recommendations = {
            ("critical", "deleted"): "ç«‹å³æ¢å¤é…ç½®æ–‡ä»¶ï¼Œå¯èƒ½å¯¼è‡´ç³»ç»Ÿæ— æ³•æ­£å¸¸å·¥ä½œ",
            ("critical", "modified"): "ä»”ç»†å®¡æŸ¥é…ç½®å˜æ›´ï¼Œç¡®ä¿ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½",
            ("critical", "added"): "éªŒè¯æ–°é…ç½®çš„å¿…è¦æ€§å’Œå®‰å…¨æ€§",
            ("high", "deleted"): "ç¡®è®¤åˆ é™¤çš„å¿…è¦æ€§ï¼Œè¯„ä¼°å¯¹åŠŸèƒ½çš„å½±å“",
            ("high", "modified"): "å®¡æŸ¥é…ç½®å˜æ›´ï¼Œç¡®ä¿ç¬¦åˆç³»ç»Ÿè¦æ±‚",
            ("high", "added"): "éªŒè¯æ–°é…ç½®çš„æœ‰æ•ˆæ€§",
            ("medium", "deleted"): "è¯„ä¼°åˆ é™¤å¯¹ç³»ç»Ÿçš„å½±å“",
            ("medium", "modified"): "ç¡®è®¤é…ç½®å˜æ›´çš„åˆç†æ€§",
            ("medium", "added"): "éªŒè¯æ–°é…ç½®æ–‡ä»¶",
            ("low", "deleted"): "è®°å½•å˜æ›´åŸå› ",
            ("low", "modified"): "è®°å½•é…ç½®å˜æ›´",
            ("low", "added"): "æ–‡æ¡£åŒ–æ–°é…ç½®",
        }

        return recommendations.get(
            (change.impact_severity, change.change_type), f"è¯„ä¼°{change.change_type}é…ç½®çš„å½±å“"
        )


class RegressionTestFramework:
    """å›å½’æµ‹è¯•æ¡†æ¶ä¸»ç±»"""

    def __init__(self, project_root: str = None):
        self.project_root = project_root or "/home/xx/dev/Claude Enhancer 5.0"
        self.test_dir = os.path.join(self.project_root, "test")
        self.reports_dir = os.path.join(self.test_dir, "regression_reports")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.reports_dir, exist_ok=True)

        # åˆå§‹åŒ–ç»„ä»¶
        self.baseline_manager = BaselineManager(self.project_root)
        self.performance_detector = PerformanceRegressionDetector(self.project_root)
        self.config_detector = ConfigurationChangeDetector(self.project_root)
        self.functional_tester = FunctionalRegressionTester(self.project_root)
        self.report_generator = RegressionReportGenerator(self.reports_dir)

    def run_complete_regression_test(
        self, current_metrics: List[BaselineMetrics] = None
    ) -> str:
        """è¿è¡Œå®Œæ•´å›å½’æµ‹è¯•"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        print("ğŸ”„ Claude Enhancer 5.0 - å›å½’æµ‹è¯•æ¡†æ¶")
        print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“ é¡¹ç›®è·¯å¾„: {self.project_root}")
        print("=" * 60)

        start_time = time.time()

        # 1. æ€§èƒ½å›å½’æ£€æµ‹
        print("\nğŸ“Š 1. æ€§èƒ½å›å½’æ£€æµ‹")
        if current_metrics:
            performance_regressions = (
                self.performance_detector.detect_performance_regression(current_metrics)
            )
        else:
            print("âš ï¸ æ— å½“å‰æ€§èƒ½æŒ‡æ ‡ï¼Œè·³è¿‡æ€§èƒ½å›å½’æ£€æµ‹")
            performance_regressions = []

        # 2. é…ç½®å˜æ›´æ£€æµ‹
        print("\nğŸ”§ 2. é…ç½®å˜æ›´æ£€æµ‹")
        config_changes = self.config_detector.detect_configuration_changes()

        # 3. åŠŸèƒ½å›å½’æµ‹è¯•
        print("\nğŸ§ª 3. åŠŸèƒ½å›å½’æµ‹è¯•")
        functional_results = self.functional_tester.test_functional_regression()

        # 4. ç”Ÿæˆå›å½’æµ‹è¯•æŠ¥å‘Š
        print("\nğŸ“Š 4. ç”Ÿæˆå›å½’æµ‹è¯•æŠ¥å‘Š")
        report_file = self.report_generator.generate_regression_report(
            performance_regressions, config_changes, functional_results, timestamp
        )

        total_time = time.time() - start_time

        # è¾“å‡ºæ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ† å›å½’æµ‹è¯•å®Œæˆ")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“Š æŠ¥å‘Šæ–‡ä»¶: {report_file}")

        # æ˜¾ç¤ºå…³é”®ç»“æœ
        critical_issues = len(
            [r for r in performance_regressions if r.severity == "critical"]
        )
        critical_configs = len(
            [c for c in config_changes if c.impact_severity == "critical"]
        )
        functional_failures = len(
            [
                f
                for f in functional_results
                if not f.get("functional_test_result", {}).get("success", True)
            ]
        )

        print(f"ğŸš¨ ä¸¥é‡é—®é¢˜: {critical_issues + critical_configs + functional_failures}")
        print(
            f"ğŸ“‹ æ€»é—®é¢˜æ•°: {len(performance_regressions) + len(config_changes) + functional_failures}"
        )

        if critical_issues + critical_configs + functional_failures > 0:
            print("âš ï¸ å»ºè®®ï¼šå‘ç°ä¸¥é‡é—®é¢˜ï¼Œä¸å»ºè®®éƒ¨ç½²")
        else:
            print("âœ… å»ºè®®ï¼šå›å½’æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥éƒ¨ç½²")

        return report_file

    def create_baseline_from_current_state(self, version: str = "5.1") -> str:
        """ä»å½“å‰çŠ¶æ€åˆ›å»ºåŸºçº¿"""
        print(f"ğŸ“ åˆ›å»ºç‰ˆæœ¬ {version} çš„æ€§èƒ½åŸºçº¿...")

        # è¿™é‡Œåº”è¯¥è¿è¡Œæ€§èƒ½æµ‹è¯•è·å–å½“å‰æŒ‡æ ‡
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºä¸€äº›æ¨¡æ‹ŸæŒ‡æ ‡
        current_metrics = [
            BaselineMetrics(
                test_name="quality_gate",
                avg_execution_time_ms=45.0,
                success_rate=0.99,
                memory_usage_mb=8.5,
                cpu_usage_percent=2.1,
                timestamp=time.time(),
                version=version,
            ),
            BaselineMetrics(
                test_name="smart_agent_selector",
                avg_execution_time_ms=28.0,
                success_rate=0.995,
                memory_usage_mb=6.2,
                cpu_usage_percent=1.8,
                timestamp=time.time(),
                version=version,
            ),
        ]

        return self.baseline_manager.create_baseline(current_metrics, version)

    def list_available_baselines(self):
        """åˆ—å‡ºå¯ç”¨çš„åŸºçº¿"""
        baselines = self.baseline_manager.list_baselines()

        print("ğŸ“š å¯ç”¨çš„æ€§èƒ½åŸºçº¿:")
        for baseline in baselines:
            print(
                f"  {baseline['type']}: {baseline['version']} "
                f"({baseline['created_date']}) "
                f"[{baseline.get('git_commit', 'unknown')[:8]}]"
            )


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Claude Enhancer 5.0 å›å½’æµ‹è¯•æ¡†æ¶")
    parser.add_argument("--project-root", help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--create-baseline", metavar="VERSION", help="åˆ›å»ºæ–°çš„æ€§èƒ½åŸºçº¿")
    parser.add_argument("--list-baselines", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨åŸºçº¿")

    args = parser.parse_args()

    try:
        framework = RegressionTestFramework(args.project_root)

        if args.create_baseline:
            baseline_file = framework.create_baseline_from_current_state(
                args.create_baseline
            )
            print(f"âœ… åŸºçº¿åˆ›å»ºæˆåŠŸ: {baseline_file}")

        elif args.list_baselines:
            framework.list_available_baselines()

        else:
            report_file = framework.run_complete_regression_test()
            print(f"\nâœ… å›å½’æµ‹è¯•å®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜åœ¨: {report_file}")

    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
