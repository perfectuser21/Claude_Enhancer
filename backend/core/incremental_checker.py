"""
Incremental Document Checker
å¢é‡æ–‡æ¡£æ£€æŸ¥å™¨ - æ™ºèƒ½å·®å¼‚æ£€æµ‹å’Œæœ€å°åŒ–æ£€æŸ¥èŒƒå›´
"""

import asyncio
import json
import logging
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
from dataclasses import dataclass
import hashlib

logger = logging.getLogger(__name__)


@dataclass
class FileChange:
    """æ–‡ä»¶å˜æ›´ä¿¡æ¯"""

    path: str
    change_type: str  # 'added', 'modified', 'deleted', 'renamed'
    old_path: Optional[str] = None  # for renamed files
    lines_changed: List[int] = None
    size_delta: int = 0


@dataclass
class IncrementalResult:
    """å¢é‡æ£€æŸ¥ç»“æœ"""

    total_files: int
    skipped_files: int
    checked_files: int
    processing_time: float
    cache_hit_rate: float
    performance_gain: float  # ç›¸æ¯”å…¨é‡æ£€æŸ¥çš„æ€§èƒ½æå‡


class IncrementalChecker:
    """å¢é‡æ–‡æ¡£æ£€æŸ¥å™¨ - æ™ºèƒ½è¯†åˆ«å˜æ›´èŒƒå›´ï¼Œæœ€å°åŒ–æ£€æŸ¥å·¥ä½œé‡"""

    def __init__(self, repo_root: str):
        self.repo_root = Path(repo_root)
        self.state_file = self.repo_root / ".claude" / "incremental_state.json"
        self.last_check_state = self._load_state()

    def _load_state(self) -> Dict:
        """åŠ è½½ä¸Šæ¬¡æ£€æŸ¥çŠ¶æ€"""
        if self.state_file.exists():
            try:
                with open(self.state_file, "r") as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load incremental state: {e}")

        return {
            "last_commit": None,
            "file_hashes": {},
            "last_check_time": None,
            "branch": None,
        }

    def _save_state(self, state: Dict):
        """ä¿å­˜æ£€æŸ¥çŠ¶æ€"""
        self.state_file.parent.mkdir(parents=True, exist_ok=True)
        try:
            with open(self.state_file, "w") as f:
                json.dump(state, f, indent=2, default=str)
        except Exception as e:
            logger.error(f"Failed to save incremental state: {e}")

    def get_changed_files_since_last_check(self) -> List[FileChange]:
        """è·å–è‡ªä¸Šæ¬¡æ£€æŸ¥ä»¥æ¥çš„å˜æ›´æ–‡ä»¶"""
        changes = []

        try:
            # è·å–å½“å‰GitçŠ¶æ€
            current_commit = self._get_current_commit()
            current_branch = self._get_current_branch()

            # å¦‚æœæ˜¯é¦–æ¬¡æ£€æŸ¥æˆ–åˆ‡æ¢åˆ†æ”¯ï¼Œè¿”å›æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
            if not self.last_check_state.get(
                "last_commit"
            ) or current_branch != self.last_check_state.get("branch"):
                logger.info("First check or branch change, checking all files")
                return self._get_all_doc_files()

            # Git diffè·å–å˜æ›´
            last_commit = self.last_check_state["last_commit"]
            git_changes = self._git_diff_files(last_commit, current_commit)

            # è§£æå˜æ›´
            for change in git_changes:
                if self._is_document_file(change["path"]):
                    changes.append(
                        FileChange(
                            path=change["path"],
                            change_type=change["type"],
                            old_path=change.get("old_path"),
                            lines_changed=change.get("lines_changed", []),
                        )
                    )

            # é¢å¤–æ£€æŸ¥ï¼šæ–‡ä»¶å†…å®¹å“ˆå¸Œå˜æ›´ï¼ˆæ•è·éGitç®¡ç†çš„å˜æ›´ï¼‰
            hash_changes = self._detect_hash_changes()
            changes.extend(hash_changes)

        except Exception as e:
            logger.error(f"Failed to detect changes: {e}")
            # fallback: æ£€æŸ¥æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
            return self._get_all_doc_files()

        logger.info(f"Detected {len(changes)} changed files")
        return changes

    def get_dependent_files(self, changed_files: List[str]) -> Set[str]:
        """è·å–ä¾èµ–æ–‡ä»¶ - å˜æ›´å¯èƒ½å½±å“çš„å…¶ä»–æ–‡ä»¶"""
        dependent_files = set()

        for file_path in changed_files:
            # 1. æ£€æŸ¥å¼•ç”¨å…³ç³»
            references = self._find_file_references(file_path)
            dependent_files.update(references)

            # 2. æ£€æŸ¥ç›®å½•å…³è”
            dir_related = self._find_directory_related_files(file_path)
            dependent_files.update(dir_related)

            # 3. æ£€æŸ¥æ¨¡æ¿/é…ç½®æ–‡ä»¶å½±å“
            template_affected = self._find_template_affected_files(file_path)
            dependent_files.update(template_affected)

        # ç§»é™¤è‡ªèº«
        dependent_files.discard(file_path)

        logger.info(f"Found {len(dependent_files)} dependent files")
        return dependent_files

    def optimize_check_order(self, files_to_check: List[str]) -> List[str]:
        """ä¼˜åŒ–æ£€æŸ¥é¡ºåº - å¿«é€Ÿå¤±è´¥ï¼Œç¼“å­˜å‹å¥½"""
        file_info = []

        for file_path in files_to_check:
            info = {
                "path": file_path,
                "size": self._get_file_size(file_path),
                "complexity": self._estimate_complexity(file_path),
                "cache_probability": self._estimate_cache_hit_probability(file_path),
            }
            file_info.append(info)

        # æ’åºç­–ç•¥ï¼š
        # 1. ç¼“å­˜å‘½ä¸­ç‡é«˜çš„ä¼˜å…ˆï¼ˆå¿«é€Ÿå®Œæˆï¼‰
        # 2. å°æ–‡ä»¶ä¼˜å…ˆï¼ˆå¿«é€Ÿåé¦ˆï¼‰
        # 3. å¤æ‚åº¦ä½çš„ä¼˜å…ˆï¼ˆå¿«é€Ÿå®Œæˆï¼‰
        file_info.sort(
            key=lambda x: (
                -x["cache_probability"],  # ç¼“å­˜å‘½ä¸­ç‡é«˜çš„ä¼˜å…ˆ
                x["size"],  # å°æ–‡ä»¶ä¼˜å…ˆ
                x["complexity"],  # ä½å¤æ‚åº¦ä¼˜å…ˆ
            )
        )

        optimized_order = [info["path"] for info in file_info]
        logger.info(f"Optimized check order for {len(optimized_order)} files")
        return optimized_order

    def should_skip_check(self, file_path: str, check_type: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è·³è¿‡æ£€æŸ¥"""
        # 1. æ–‡ä»¶å¤§å°è¿‡æ»¤
        file_size = self._get_file_size(file_path)
        if file_size > 1024 * 1024:  # 1MB
            logger.debug(f"Skipping large file: {file_path} ({file_size} bytes)")
            return True

        # 2. æ–‡ä»¶ç±»å‹è¿‡æ»¤
        if not self._is_checkable_file(file_path, check_type):
            return True

        # 3. æ—¶é—´æˆ³æ£€æŸ¥ï¼ˆæ–‡ä»¶æœªä¿®æ”¹ï¼‰
        if not self._has_file_changed_recently(file_path):
            logger.debug(f"Skipping unchanged file: {file_path}")
            return True

        # 4. ä¸´æ—¶æ–‡ä»¶è¿‡æ»¤
        if self._is_temp_file(file_path):
            return True

        return False

    def create_incremental_context(self, changed_files: List[str]) -> Dict:
        """åˆ›å»ºå¢é‡æ£€æŸ¥ä¸Šä¸‹æ–‡ - ä¸ºæ£€æŸ¥å™¨æä¾›ä¼˜åŒ–ä¿¡æ¯"""
        return {
            "changed_files": changed_files,
            "change_hotspots": self._identify_change_hotspots(changed_files),
            "risk_level": self._assess_change_risk(changed_files),
            "suggested_focus": self._suggest_check_focus(changed_files),
            "parallel_groups": self._create_parallel_groups(changed_files),
            "estimated_time": self._estimate_check_time(changed_files),
        }

    def _get_current_commit(self) -> str:
        """è·å–å½“å‰Gitæäº¤"""
        try:
            result = subprocess.run(
                ["git", "rev-parse", "HEAD"],
                cwd=self.repo_root,
                capture_output=True,
                text=True,
                check=True,
            )
            return result.stdout.strip()
        except subprocess.CalledProcessError:
            return ""

    def _get_current_branch(self) -> str:
        """è·å–å½“å‰Gitåˆ†æ”¯"""
        try:
            result = subprocess.run(
                ["git", "rev-parse", "--abbrev-ref", "HEAD"],
                cwd=self.repo_root,
                capture_output=True,
                text=True,
                check=True,
            )
            return result.stdout.strip()
        except subprocess.CalledProcessError:
            return ""

    def _git_diff_files(self, from_commit: str, to_commit: str) -> List[Dict]:
        """Git diffè·å–æ–‡ä»¶å˜æ›´"""
        changes = []

        try:
            # è·å–å˜æ›´æ–‡ä»¶åˆ—è¡¨
            result = subprocess.run(
                ["git", "diff", "--name-status", f"{from_commit}..{to_commit}"],
                cwd=self.repo_root,
                capture_output=True,
                text=True,
                check=True,
            )

            for line in result.stdout.strip().split("\n"):
                if not line:
                    continue

                parts = line.split("\t")
                if len(parts) >= 2:
                    status = parts[0]
                    file_path = parts[1]

                    change_type = {
                        "A": "added",
                        "M": "modified",
                        "D": "deleted",
                        "R": "renamed",
                        "C": "copied",
                    }.get(status[0], "modified")

                    change = {"path": file_path, "type": change_type}

                    # é‡å‘½åæ–‡ä»¶çš„æ—§è·¯å¾„
                    if status.startswith("R") and len(parts) >= 3:
                        change["old_path"] = parts[2]

                    changes.append(change)

        except subprocess.CalledProcessError as e:
            logger.error(f"Git diff failed: {e}")

        return changes

    def _detect_hash_changes(self) -> List[FileChange]:
        """æ£€æµ‹æ–‡ä»¶å“ˆå¸Œå˜æ›´"""
        changes = []
        current_hashes = {}

        # è®¡ç®—å½“å‰æ–‡ä»¶å“ˆå¸Œ
        doc_files = self._get_all_doc_files()
        for file_change in doc_files:
            file_path = file_change.path
            if Path(file_path).exists():
                current_hash = self._calculate_file_hash(file_path)
                current_hashes[file_path] = current_hash

                # ä¸ä¸Šæ¬¡å¯¹æ¯”
                last_hash = self.last_check_state.get("file_hashes", {}).get(file_path)
                if last_hash and last_hash != current_hash:
                    changes.append(FileChange(path=file_path, change_type="modified"))

        return changes

    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        try:
            with open(file_path, "rb") as f:
                return hashlib.sha256(f.read()).hexdigest()
        except Exception:
            return ""

    def _get_all_doc_files(self) -> List[FileChange]:
        """è·å–æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶"""
        doc_files = []
        doc_extensions = {".md", ".txt", ".rst", ".adoc", ".org"}

        for ext in doc_extensions:
            for file_path in self.repo_root.rglob(f"*{ext}"):
                if self._is_checkable_file(str(file_path), "all"):
                    doc_files.append(
                        FileChange(
                            path=str(file_path.relative_to(self.repo_root)),
                            change_type="existing",
                        )
                    )

        return doc_files

    def _is_document_file(self, file_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æ¡£æ–‡ä»¶"""
        doc_extensions = {".md", ".txt", ".rst", ".adoc", ".org"}
        return Path(file_path).suffix.lower() in doc_extensions

    def _is_checkable_file(self, file_path: str, check_type: str) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å¯æ£€æŸ¥"""
        path_obj = Path(file_path)

        # è·³è¿‡éšè—æ–‡ä»¶å’Œç›®å½•
        if any(part.startswith(".") for part in path_obj.parts):
            if not any(part in [".claude", ".github"] for part in path_obj.parts):
                return False

        # è·³è¿‡ç‰¹å®šç›®å½•
        skip_dirs = {"node_modules", "__pycache__", ".git", "build", "dist"}
        if any(part in skip_dirs for part in path_obj.parts):
            return False

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        return self._is_document_file(file_path)

    def _find_file_references(self, file_path: str) -> Set[str]:
        """æŸ¥æ‰¾æ–‡ä»¶å¼•ç”¨å…³ç³»"""
        references = set()

        try:
            # æœç´¢åŒ…å«è¯¥æ–‡ä»¶è·¯å¾„çš„å…¶ä»–æ–‡ä»¶
            result = subprocess.run(
                ["grep", "-r", "-l", Path(file_path).name, str(self.repo_root)],
                capture_output=True,
                text=True,
            )

            for line in result.stdout.strip().split("\n"):
                if line and line != file_path and self._is_document_file(line):
                    relative_path = str(Path(line).relative_to(self.repo_root))
                    references.add(relative_path)

        except Exception as e:
            logger.debug(f"Failed to find references for {file_path}: {e}")

        return references

    def _find_directory_related_files(self, file_path: str) -> Set[str]:
        """æŸ¥æ‰¾ç›®å½•ç›¸å…³æ–‡ä»¶"""
        related = set()
        file_dir = Path(file_path).parent

        # åŒç›®å½•ä¸‹çš„å…¶ä»–æ–‡æ¡£æ–‡ä»¶
        for sibling in file_dir.glob("*.md"):
            if sibling.name != Path(file_path).name:
                related.add(str(sibling.relative_to(self.repo_root)))

        # æ£€æŸ¥æ˜¯å¦æœ‰READMEæ–‡ä»¶éœ€è¦æ›´æ–°
        readme_files = ["README.md", "readme.md", "README.txt"]
        for readme in readme_files:
            readme_path = file_dir / readme
            if readme_path.exists():
                related.add(str(readme_path.relative_to(self.repo_root)))

        return related

    def _find_template_affected_files(self, file_path: str) -> Set[str]:
        """æŸ¥æ‰¾æ¨¡æ¿å½±å“çš„æ–‡ä»¶"""
        affected = set()

        # å¦‚æœæ˜¯é…ç½®æ–‡ä»¶æˆ–æ¨¡æ¿æ–‡ä»¶
        template_indicators = ["template", "config", ".claude", "settings"]
        if any(indicator in file_path.lower() for indicator in template_indicators):
            # å¯èƒ½å½±å“æ•´ä¸ªé¡¹ç›®çš„æ–‡æ¡£
            for doc_file in self._get_all_doc_files():
                affected.add(doc_file.path)

        return affected

    def _get_file_size(self, file_path: str) -> int:
        """è·å–æ–‡ä»¶å¤§å°"""
        try:
            return Path(file_path).stat().st_size
        except Exception:
            return 0

    def _estimate_complexity(self, file_path: str) -> int:
        """ä¼°ç®—æ–‡ä»¶å¤æ‚åº¦"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # ç®€å•çš„å¤æ‚åº¦æŒ‡æ ‡
            complexity = 0
            complexity += len(content.split("\n"))  # è¡Œæ•°
            complexity += content.count("[")  # é“¾æ¥æ•°é‡
            complexity += content.count("#")  # æ ‡é¢˜æ•°é‡
            complexity += content.count("```")  # ä»£ç å—æ•°é‡

            return complexity

        except Exception:
            return 100  # é»˜è®¤ä¸­ç­‰å¤æ‚åº¦

    def _estimate_cache_hit_probability(self, file_path: str) -> float:
        """ä¼°ç®—ç¼“å­˜å‘½ä¸­æ¦‚ç‡"""
        # åŸºäºæ–‡ä»¶ä¿®æ”¹æ—¶é—´å’Œå†å²æ¨¡å¼
        try:
            file_stat = Path(file_path).stat()
            hours_since_modified = (
                datetime.now().timestamp() - file_stat.st_mtime
            ) / 3600

            # æ–‡ä»¶è¶Šæ—§ï¼Œç¼“å­˜å‘½ä¸­ç‡è¶Šé«˜
            if hours_since_modified > 24:
                return 0.9
            elif hours_since_modified > 1:
                return 0.7
            else:
                return 0.3

        except Exception:
            return 0.5

    def _has_file_changed_recently(self, file_path: str, hours: int = 1) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ€è¿‘ä¿®æ”¹è¿‡"""
        try:
            file_stat = Path(file_path).stat()
            hours_since_modified = (
                datetime.now().timestamp() - file_stat.st_mtime
            ) / 3600
            return hours_since_modified < hours
        except Exception:
            return True  # å®‰å…¨èµ·è§ï¼Œè®¤ä¸ºå·²ä¿®æ”¹

    def _is_temp_file(self, file_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶æ–‡ä»¶"""
        temp_patterns = [".tmp", ".temp", ".bak", ".swp", "~"]
        return any(pattern in file_path for pattern in temp_patterns)

    def _identify_change_hotspots(self, changed_files: List[str]) -> List[str]:
        """è¯†åˆ«å˜æ›´çƒ­ç‚¹ç›®å½•"""
        directories = {}
        for file_path in changed_files:
            dir_path = str(Path(file_path).parent)
            directories[dir_path] = directories.get(dir_path, 0) + 1

        # è¿”å›å˜æ›´æ–‡ä»¶æœ€å¤šçš„ç›®å½•
        hotspots = sorted(directories.items(), key=lambda x: x[1], reverse=True)
        return [dir_path for dir_path, count in hotspots[:5]]

    def _assess_change_risk(self, changed_files: List[str]) -> str:
        """è¯„ä¼°å˜æ›´é£é™©çº§åˆ«"""
        if len(changed_files) > 50:
            return "high"
        elif len(changed_files) > 10:
            return "medium"
        else:
            return "low"

    def _suggest_check_focus(self, changed_files: List[str]) -> List[str]:
        """å»ºè®®æ£€æŸ¥é‡ç‚¹"""
        focus_areas = []

        # æ ¹æ®æ–‡ä»¶ç±»å‹å»ºè®®
        has_readme = any("readme" in f.lower() for f in changed_files)
        has_docs = any("doc" in f.lower() for f in changed_files)
        has_api = any("api" in f.lower() for f in changed_files)

        if has_readme:
            focus_areas.append("é¡¹ç›®ä»‹ç»å’Œä½¿ç”¨è¯´æ˜")
        if has_docs:
            focus_areas.append("æŠ€æœ¯æ–‡æ¡£å®Œæ•´æ€§")
        if has_api:
            focus_areas.append("APIæ–‡æ¡£å‡†ç¡®æ€§")

        return focus_areas

    def _create_parallel_groups(self, changed_files: List[str]) -> List[List[str]]:
        """åˆ›å»ºå¹¶è¡Œå¤„ç†ç»„"""
        # æŒ‰ç›®å½•åˆ†ç»„ï¼Œé¿å…æ–‡ä»¶ç³»ç»Ÿç«äº‰
        groups = {}
        for file_path in changed_files:
            dir_path = str(Path(file_path).parent)
            if dir_path not in groups:
                groups[dir_path] = []
            groups[dir_path].append(file_path)

        return list(groups.values())

    def _estimate_check_time(self, changed_files: List[str]) -> float:
        """ä¼°ç®—æ£€æŸ¥æ—¶é—´"""
        base_time_per_file = 0.5  # ç§’
        total_size = sum(self._get_file_size(f) for f in changed_files)
        complexity_factor = min(total_size / (1024 * 100), 5)  # æœ€å¤š5å€å¤æ‚åº¦

        return len(changed_files) * base_time_per_file * complexity_factor

    async def update_state_after_check(self, checked_files: List[str]):
        """æ£€æŸ¥å®Œæˆåæ›´æ–°çŠ¶æ€"""
        new_state = {
            "last_commit": self._get_current_commit(),
            "last_check_time": datetime.now().isoformat(),
            "branch": self._get_current_branch(),
            "file_hashes": {},
        }

        # æ›´æ–°æ–‡ä»¶å“ˆå¸Œ
        for file_path in checked_files:
            if Path(file_path).exists():
                new_state["file_hashes"][file_path] = self._calculate_file_hash(
                    file_path
                )

        # ä¿ç•™æœªæ£€æŸ¥æ–‡ä»¶çš„å“ˆå¸Œ
        for file_path, file_hash in self.last_check_state.get(
            "file_hashes", {}
        ).items():
            if file_path not in new_state["file_hashes"]:
                new_state["file_hashes"][file_path] = file_hash

        self._save_state(new_state)
        self.last_check_state = new_state

        logger.info(f"Updated incremental state for {len(checked_files)} files")


# ä½¿ç”¨ç¤ºä¾‹
async def test_incremental_checker():
    """å¢é‡æ£€æŸ¥å™¨æµ‹è¯•"""
    checker = IncrementalChecker("/home/xx/dev/Claude Enhancer 5.0")

    print("ğŸ” æ£€æµ‹æ–‡ä»¶å˜æ›´...")
    changed_files = checker.get_changed_files_since_last_check()
    print(f"å˜æ›´æ–‡ä»¶: {len(changed_files)}")

    if changed_files:
        file_paths = [change.path for change in changed_files]

        print("ğŸ”— åˆ†æä¾èµ–å…³ç³»...")
        dependent_files = checker.get_dependent_files(file_paths)
        print(f"ä¾èµ–æ–‡ä»¶: {len(dependent_files)}")

        all_files = list(set(file_paths + list(dependent_files)))

        print("âš¡ ä¼˜åŒ–æ£€æŸ¥é¡ºåº...")
        optimized_files = checker.optimize_check_order(all_files)

        print("ğŸ“Š åˆ›å»ºå¢é‡ä¸Šä¸‹æ–‡...")
        context = checker.create_incremental_context(file_paths)
        print(f"æ£€æŸ¥ä¸Šä¸‹æ–‡: {context}")

        # æ¨¡æ‹Ÿæ£€æŸ¥å®Œæˆ
        await checker.update_state_after_check(optimized_files)


if __name__ == "__main__":
    asyncio.run(test_incremental_checker())
