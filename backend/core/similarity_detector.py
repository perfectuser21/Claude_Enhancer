"""
High-Performance Similarity Detection
é«˜æ€§èƒ½ç›¸ä¼¼åº¦æ£€æµ‹å™¨ - ä¼˜åŒ–çš„æ–‡æ¡£ç›¸ä¼¼åº¦æ£€æµ‹ç®—æ³•
"""

import asyncio
import hashlib
import logging
import time
from typing import Dict, List, Tuple, Set, Optional
from dataclasses import dataclass
from pathlib import Path
import numpy as np
from collections import defaultdict, Counter
import re
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
import mmap

logger = logging.getLogger(__name__)


@dataclass
class SimilarityResult:
    """ç›¸ä¼¼åº¦æ£€æµ‹ç»“æœ"""

    file1: str
    file2: str
    similarity_score: float
    similarity_type: str  # 'exact', 'high', 'medium', 'low'
    common_sections: List[str]
    processing_time: float


@dataclass
class DocumentFingerprint:
    """æ–‡æ¡£æŒ‡çº¹"""

    file_path: str
    content_hash: str
    structure_hash: str
    keywords: Set[str]
    ngrams: Set[str]
    length: int
    sections: List[str]


class HighPerformanceSimilarityDetector:
    """é«˜æ€§èƒ½ç›¸ä¼¼åº¦æ£€æµ‹å™¨ - å¤šå±‚æ¬¡ã€å¤šç®—æ³•çš„æ–‡æ¡£ç›¸ä¼¼åº¦æ£€æµ‹"""

    def __init__(self):
        self.similarity_thresholds = {
            "exact": 0.95,
            "high": 0.80,
            "medium": 0.60,
            "low": 0.40,
        }

        # ç¼“å­˜å’Œä¼˜åŒ–
        self.fingerprint_cache = {}
        self.similarity_cache = {}
        self.max_cache_size = 10000

        # å¹¶è¡Œå¤„ç†é…ç½®
        self.max_workers = min(mp.cpu_count(), 8)

    async def detect_similarities_batch(
        self, file_paths: List[str]
    ) -> List[SimilarityResult]:
        """æ‰¹é‡ç›¸ä¼¼åº¦æ£€æµ‹ - é«˜æ€§èƒ½ç‰ˆæœ¬"""
        start_time = time.time()

        if len(file_paths) < 2:
            return []

        print(f"ğŸ” å¼€å§‹ç›¸ä¼¼åº¦æ£€æµ‹ - {len(file_paths)} ä¸ªæ–‡ä»¶")

        # 1. å¿«é€Ÿé¢„è¿‡æ»¤ - æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„
        size_groups = self._group_files_by_size(file_paths)
        candidate_pairs = []

        for group in size_groups:
            if len(group) >= 2:
                # åªåœ¨ç›¸ä¼¼å¤§å°çš„æ–‡ä»¶é—´æ£€æµ‹
                candidate_pairs.extend(self._generate_pairs(group))

        print(f"ğŸ“Š é¢„è¿‡æ»¤åå€™é€‰å¯¹: {len(candidate_pairs)}")

        # 2. è®¡ç®—æ–‡æ¡£æŒ‡çº¹ - å¹¶è¡Œå¤„ç†
        fingerprints = await self._compute_fingerprints_parallel(file_paths)
        fingerprint_map = {fp.file_path: fp for fp in fingerprints}

        # 3. å¤šå±‚æ¬¡ç›¸ä¼¼åº¦æ£€æµ‹
        results = []

        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º
            batch_size = 100
            for i in range(0, len(candidate_pairs), batch_size):
                batch = candidate_pairs[i : i + batch_size]

                futures = []
                for file1, file2 in batch:
                    if file1 in fingerprint_map and file2 in fingerprint_map:
                        future = executor.submit(
                            self._compute_similarity_fast,
                            fingerprint_map[file1],
                            fingerprint_map[file2],
                        )
                        futures.append((future, file1, file2))

                # æ”¶é›†ç»“æœ
                for future, file1, file2 in futures:
                    try:
                        similarity = future.result(timeout=5)
                        if (
                            similarity
                            and similarity.similarity_score
                            > self.similarity_thresholds["low"]
                        ):
                            results.append(similarity)
                    except Exception as e:
                        logger.warning(
                            f"Similarity computation failed for {file1} vs {file2}: {e}"
                        )

        processing_time = time.time() - start_time
        print(f"âœ… ç›¸ä¼¼åº¦æ£€æµ‹å®Œæˆ - è€—æ—¶: {processing_time:.2f}s, å‘ç° {len(results)} ä¸ªç›¸ä¼¼å¯¹")

        return results

    def _group_files_by_size(self, file_paths: List[str]) -> List[List[str]]:
        """æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„ - ç›¸ä¼¼å¤§å°çš„æ–‡ä»¶æ›´å¯èƒ½ç›¸ä¼¼"""
        size_groups = defaultdict(list)

        for file_path in file_paths:
            try:
                size = Path(file_path).stat().st_size
                # æŒ‰å¤§å°èŒƒå›´åˆ†ç»„
                size_bucket = self._get_size_bucket(size)
                size_groups[size_bucket].append(file_path)
            except Exception as e:
                logger.warning(f"Failed to get size for {file_path}: {e}")

        # åªè¿”å›æœ‰å¤šä¸ªæ–‡ä»¶çš„ç»„
        return [group for group in size_groups.values() if len(group) >= 2]

    def _get_size_bucket(self, size: int) -> str:
        """è·å–æ–‡ä»¶å¤§å°åˆ†æ¡¶"""
        if size < 1024:  # < 1KB
            return "tiny"
        elif size < 10 * 1024:  # < 10KB
            return "small"
        elif size < 100 * 1024:  # < 100KB
            return "medium"
        elif size < 1024 * 1024:  # < 1MB
            return "large"
        else:
            return "huge"

    def _generate_pairs(self, files: List[str]) -> List[Tuple[str, str]]:
        """ç”Ÿæˆæ–‡ä»¶å¯¹"""
        pairs = []
        for i in range(len(files)):
            for j in range(i + 1, len(files)):
                pairs.append((files[i], files[j]))
        return pairs

    async def _compute_fingerprints_parallel(
        self, file_paths: List[str]
    ) -> List[DocumentFingerprint]:
        """å¹¶è¡Œè®¡ç®—æ–‡æ¡£æŒ‡çº¹"""
        fingerprints = []

        # æ£€æŸ¥ç¼“å­˜
        uncached_files = []
        for file_path in file_paths:
            if file_path in self.fingerprint_cache:
                fingerprints.append(self.fingerprint_cache[file_path])
            else:
                uncached_files.append(file_path)

        if not uncached_files:
            return fingerprints

        print(f"ğŸ“‹ è®¡ç®—æ–‡æ¡£æŒ‡çº¹ - {len(uncached_files)} ä¸ªæ–°æ–‡ä»¶")

        # å¹¶è¡Œè®¡ç®—æŒ‡çº¹
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self._compute_fingerprint, file_path)
                for file_path in uncached_files
            ]

            for future in futures:
                try:
                    fingerprint = future.result(timeout=10)
                    if fingerprint:
                        fingerprints.append(fingerprint)
                        # ç¼“å­˜æŒ‡çº¹
                        self._cache_fingerprint(fingerprint)
                except Exception as e:
                    logger.error(f"Fingerprint computation failed: {e}")

        return fingerprints

    def _compute_fingerprint(self, file_path: str) -> Optional[DocumentFingerprint]:
        """è®¡ç®—å•ä¸ªæ–‡æ¡£æŒ‡çº¹"""
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                # ä½¿ç”¨å†…å­˜æ˜ å°„æé«˜å¤§æ–‡ä»¶è¯»å–æ€§èƒ½
                content = f.read()

            if not content.strip():
                return None

            # 1. å†…å®¹å“ˆå¸Œ
            content_hash = hashlib.md5(content.encode()).hexdigest()

            # 2. ç»“æ„å“ˆå¸Œ (æ ‡é¢˜ã€æ®µè½ç»“æ„)
            structure = self._extract_structure(content)
            structure_hash = hashlib.md5(str(structure).encode()).hexdigest()

            # 3. å…³é”®è¯æå–
            keywords = self._extract_keywords(content)

            # 4. N-gramç‰¹å¾
            ngrams = self._extract_ngrams(content, n=3)

            # 5. æ–‡æ¡£æ®µè½
            sections = self._extract_sections(content)

            return DocumentFingerprint(
                file_path=file_path,
                content_hash=content_hash,
                structure_hash=structure_hash,
                keywords=keywords,
                ngrams=ngrams,
                length=len(content),
                sections=sections,
            )

        except Exception as e:
            logger.error(f"Failed to compute fingerprint for {file_path}: {e}")
            return None

    def _compute_similarity_fast(
        self, fp1: DocumentFingerprint, fp2: DocumentFingerprint
    ) -> Optional[SimilarityResult]:
        """å¿«é€Ÿç›¸ä¼¼åº¦è®¡ç®—"""
        start_time = time.time()

        # 1. å¿«é€Ÿæ’é™¤ï¼šå®Œå…¨ç›¸åŒ
        if fp1.content_hash == fp2.content_hash:
            return SimilarityResult(
                file1=fp1.file_path,
                file2=fp2.file_path,
                similarity_score=1.0,
                similarity_type="exact",
                common_sections=[],
                processing_time=time.time() - start_time,
            )

        # 2. å¿«é€Ÿæ’é™¤ï¼šé•¿åº¦å·®å¼‚è¿‡å¤§
        if abs(fp1.length - fp2.length) / max(fp1.length, fp2.length) > 0.8:
            return None

        # 3. å¤šç»´åº¦ç›¸ä¼¼åº¦è®¡ç®—
        similarities = {}

        # ç»“æ„ç›¸ä¼¼åº¦
        similarities["structure"] = (
            1.0 if fp1.structure_hash == fp2.structure_hash else 0.0
        )

        # å…³é”®è¯ç›¸ä¼¼åº¦ (Jaccard)
        similarities["keywords"] = self._jaccard_similarity(fp1.keywords, fp2.keywords)

        # N-gramç›¸ä¼¼åº¦
        similarities["ngrams"] = self._jaccard_similarity(fp1.ngrams, fp2.ngrams)

        # æ®µè½ç›¸ä¼¼åº¦
        similarities["sections"] = self._section_similarity(fp1.sections, fp2.sections)

        # 4. åŠ æƒå¹³å‡
        weights = {"structure": 0.2, "keywords": 0.3, "ngrams": 0.3, "sections": 0.2}

        overall_similarity = sum(
            similarities[dim] * weight for dim, weight in weights.items()
        )

        # 5. ç¡®å®šç›¸ä¼¼åº¦ç±»å‹
        similarity_type = "low"
        for stype, threshold in sorted(
            self.similarity_thresholds.items(), key=lambda x: x[1], reverse=True
        ):
            if overall_similarity >= threshold:
                similarity_type = stype
                break

        # 6. æ‰¾å‡ºå…±åŒæ®µè½
        common_sections = self._find_common_sections(fp1.sections, fp2.sections)

        return SimilarityResult(
            file1=fp1.file_path,
            file2=fp2.file_path,
            similarity_score=overall_similarity,
            similarity_type=similarity_type,
            common_sections=common_sections,
            processing_time=time.time() - start_time,
        )

    def _extract_structure(self, content: str) -> List[str]:
        """æå–æ–‡æ¡£ç»“æ„"""
        structure = []

        # Markdownæ ‡é¢˜
        lines = content.split("\n")
        for line in lines:
            line = line.strip()
            if line.startswith("#"):
                level = len(line) - len(line.lstrip("#"))
                structure.append(f"H{level}")
            elif line.startswith("-") or line.startswith("*"):
                structure.append("LIST")
            elif line.startswith("```"):
                structure.append("CODE")
            elif line.startswith(">"):
                structure.append("QUOTE")

        return structure

    def _extract_keywords(self, content: str) -> Set[str]:
        """æå–å…³é”®è¯"""
        # ç®€åŒ–çš„å…³é”®è¯æå–
        words = re.findall(r"\b[a-zA-Z]{3,}\b", content.lower())

        # è¿‡æ»¤åœç”¨è¯
        stop_words = {
            "the",
            "and",
            "for",
            "are",
            "but",
            "not",
            "you",
            "all",
            "can",
            "had",
            "her",
            "was",
            "one",
            "our",
            "out",
            "day",
            "get",
            "has",
            "him",
            "his",
            "how",
            "man",
            "new",
            "now",
            "old",
            "see",
            "two",
            "way",
            "who",
            "boy",
            "did",
            "its",
            "let",
            "put",
            "say",
            "she",
            "too",
            "use",
        }

        keywords = {word for word in words if word not in stop_words and len(word) > 3}

        # åªä¿ç•™æœ€å¸¸è§çš„å…³é”®è¯
        word_counts = Counter(words)
        top_keywords = {word for word, count in word_counts.most_common(50)}

        return keywords.intersection(top_keywords)

    def _extract_ngrams(self, content: str, n: int = 3) -> Set[str]:
        """æå–N-gramç‰¹å¾"""
        # æ¸…ç†å†…å®¹
        content = re.sub(r"[^\w\s]", " ", content.lower())
        words = content.split()

        ngrams = set()
        for i in range(len(words) - n + 1):
            ngram = " ".join(words[i : i + n])
            ngrams.add(ngram)

        # é™åˆ¶æ•°é‡ï¼Œé¿å…å†…å­˜æº¢å‡º
        return set(list(ngrams)[:1000])

    def _extract_sections(self, content: str) -> List[str]:
        """æå–æ–‡æ¡£æ®µè½"""
        # æŒ‰ç©ºè¡Œåˆ†æ®µ
        sections = []
        current_section = []

        for line in content.split("\n"):
            line = line.strip()
            if line:
                current_section.append(line)
            else:
                if current_section:
                    sections.append(" ".join(current_section))
                    current_section = []

        if current_section:
            sections.append(" ".join(current_section))

        # è¿‡æ»¤è¿‡çŸ­çš„æ®µè½
        return [section for section in sections if len(section) > 50]

    def _jaccard_similarity(self, set1: Set, set2: Set) -> float:
        """Jaccardç›¸ä¼¼åº¦"""
        if not set1 and not set2:
            return 1.0

        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))

        return intersection / union if union > 0 else 0.0

    def _section_similarity(self, sections1: List[str], sections2: List[str]) -> float:
        """æ®µè½ç›¸ä¼¼åº¦"""
        if not sections1 and not sections2:
            return 1.0

        if not sections1 or not sections2:
            return 0.0

        # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„æ®µè½å¯¹
        max_similarities = []

        for s1 in sections1:
            max_sim = 0.0
            for s2 in sections2:
                sim = self._text_similarity(s1, s2)
                max_sim = max(max_sim, sim)
            max_similarities.append(max_sim)

        return (
            sum(max_similarities) / len(max_similarities) if max_similarities else 0.0
        )

    def _text_similarity(self, text1: str, text2: str) -> float:
        """æ–‡æœ¬ç›¸ä¼¼åº¦ (ç®€åŒ–ç‰ˆ)"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        return self._jaccard_similarity(words1, words2)

    def _find_common_sections(
        self, sections1: List[str], sections2: List[str]
    ) -> List[str]:
        """æ‰¾å‡ºå…±åŒæ®µè½"""
        common = []

        for s1 in sections1:
            for s2 in sections2:
                similarity = self._text_similarity(s1, s2)
                if similarity > 0.8:
                    common.append(s1[:100] + "..." if len(s1) > 100 else s1)

        return common[:5]  # æœ€å¤šè¿”å›5ä¸ªå…±åŒæ®µè½

    def _cache_fingerprint(self, fingerprint: DocumentFingerprint):
        """ç¼“å­˜æ–‡æ¡£æŒ‡çº¹"""
        if len(self.fingerprint_cache) >= self.max_cache_size:
            # ç®€å•çš„LRU: ç§»é™¤ä¸€åŠç¼“å­˜
            items = list(self.fingerprint_cache.items())
            self.fingerprint_cache = dict(items[len(items) // 2 :])

        self.fingerprint_cache[fingerprint.file_path] = fingerprint

    def get_performance_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return {
            "fingerprint_cache_size": len(self.fingerprint_cache),
            "similarity_cache_size": len(self.similarity_cache),
            "max_workers": self.max_workers,
            "similarity_thresholds": self.similarity_thresholds,
        }

    async def cleanup_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        self.fingerprint_cache.clear()
        self.similarity_cache.clear()
        logger.info("Similarity detection cache cleared")


# ä½¿ç”¨ç¤ºä¾‹å’Œæ€§èƒ½æµ‹è¯•
async def test_similarity_detection():
    """ç›¸ä¼¼åº¦æ£€æµ‹æ€§èƒ½æµ‹è¯•"""
    detector = HighPerformanceSimilarityDetector()

    # æ¨¡æ‹Ÿæ–‡ä»¶åˆ—è¡¨
    test_files = [
        "README.md",
        "docs/API.md",
        "docs/GUIDE.md",
        "docs/TUTORIAL.md",
        "CHANGELOG.md",
        "CONTRIBUTING.md",
    ]

    print("ğŸš€ å¼€å§‹ç›¸ä¼¼åº¦æ£€æµ‹æµ‹è¯•...")

    start_time = time.time()
    results = await detector.detect_similarities_batch(test_files)
    duration = time.time() - start_time

    print(f"â±ï¸  æ£€æµ‹è€—æ—¶: {duration:.2f}s")
    print(f"ğŸ“Š å‘ç°ç›¸ä¼¼å¯¹: {len(results)}")

    for result in results:
        print(
            f"ğŸ“„ {result.file1} vs {result.file2}: "
            f"{result.similarity_score:.2f} ({result.similarity_type})"
        )

    # æ€§èƒ½ç»Ÿè®¡
    stats = detector.get_performance_stats()
    print(f"ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡: {stats}")


# ä¼˜åŒ–çš„ç›¸ä¼¼åº¦æ£€æµ‹Hooké›†æˆ
class SimilarityCheckHook:
    """ç›¸ä¼¼åº¦æ£€æŸ¥Hook - é›†æˆåˆ°è´¨é‡æ£€æŸ¥æµç¨‹"""

    def __init__(self, max_similarity: float = 0.8):
        self.detector = HighPerformanceSimilarityDetector()
        self.max_similarity = max_similarity

    async def check_pre_commit(
        self, changed_files: List[str]
    ) -> Tuple[bool, List[str]]:
        """Pre-commitç›¸ä¼¼åº¦æ£€æŸ¥ - å¿«é€Ÿç‰ˆæœ¬"""
        if len(changed_files) < 2:
            return True, []

        # åªæ£€æŸ¥å˜æ›´æ–‡ä»¶ä¹‹é—´çš„ç›¸ä¼¼åº¦
        results = await self.detector.detect_similarities_batch(changed_files)

        issues = []
        for result in results:
            if result.similarity_score > self.max_similarity:
                issues.append(
                    f"High similarity detected: {result.file1} vs {result.file2} "
                    f"({result.similarity_score:.2f})"
                )

        return len(issues) == 0, issues

    async def check_ci_deep(self, all_files: List[str]) -> Tuple[bool, List[str]]:
        """CIæ·±åº¦ç›¸ä¼¼åº¦æ£€æŸ¥"""
        results = await self.detector.detect_similarities_batch(all_files)

        issues = []
        duplicates = []

        for result in results:
            if result.similarity_type == "exact":
                duplicates.append(f"Duplicate files: {result.file1} vs {result.file2}")
            elif result.similarity_score > self.max_similarity:
                issues.append(
                    f"High similarity: {result.file1} vs {result.file2} "
                    f"({result.similarity_score:.2f}) - Common sections: "
                    f"{len(result.common_sections)}"
                )

        all_issues = duplicates + issues
        return len(all_issues) == 0, all_issues


if __name__ == "__main__":
    asyncio.run(test_similarity_detection())
