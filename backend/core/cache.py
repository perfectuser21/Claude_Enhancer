"""
Performance Optimization: Redis Cache Layer
Redisç¼“å­˜å±‚ - é«˜æ€§èƒ½ç¼“å­˜ç®¡ç†ç³»ç»Ÿ
"""

import redis.asyncio as redis
import json
import pickle
import asyncio
import logging
from typing import Any, Optional, Dict, List, Union
from datetime import datetime, timedelta
from functools import wraps
from contextlib import asynccontextmanager
import hashlib
import zlib
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class CacheStrategy(Enum):
    """ç¼“å­˜ç­–ç•¥"""

    LRU = "lru"
    LFU = "lfu"
    TTL = "ttl"
    WRITE_THROUGH = "write_through"
    WRITE_BACK = "write_back"
    WRITE_AROUND = "write_around"


@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""

    host: str = "localhost"
    port: int = 6379
    password: Optional[str] = None
    db: int = 0
    pool_size: int = 50
    timeout: int = 5
    retry_on_timeout: bool = True
    decode_responses: bool = False
    compression_enabled: bool = True
    compression_threshold: int = 1024  # 1KB
    default_ttl: int = 300  # 5åˆ†é’Ÿ
    max_connections: int = 100


class CacheManager:
    """Redisç¼“å­˜ç®¡ç†å™¨ - ä¼ä¸šçº§é«˜æ€§èƒ½ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, config: CacheConfig):
        self.config = config
        self.redis_pool = None
        self.local_cache = {}  # æœ¬åœ°L1ç¼“å­˜
        self.local_cache_ttl = {}
        self.stats = {"hits": 0, "misses": 0, "sets": 0, "deletes": 0, "errors": 0}
        self._lock = asyncio.Lock()

    async def initialize(self):
        """åˆå§‹åŒ–Redisè¿æ¥æ± """
        try:
            self.redis_pool = redis.ConnectionPool(
                host=self.config.host,
                port=self.config.port,
                password=self.config.password,
                db=self.config.db,
                max_connections=self.config.max_connections,
                retry_on_timeout=self.config.retry_on_timeout,
                decode_responses=self.config.decode_responses,
            )

            # æµ‹è¯•è¿æ¥
            async with self._get_connection() as conn:
                await conn.ping()

            logger.info(f"âœ… Redisç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ - {self.config.host}:{self.config.port}")

            # å¯åŠ¨åå°ä»»åŠ¡
            asyncio.create_task(self._cleanup_local_cache())
            asyncio.create_task(self._periodic_stats_log())

        except Exception as e:
            logger.error(f"âŒ Redisè¿æ¥å¤±è´¥: {e}")
            raise

    @asynccontextmanager
    async def _get_connection(self):
        """è·å–Redisè¿æ¥"""
        conn = redis.Redis(connection_pool=self.redis_pool)
        try:
            yield conn
        finally:
            await conn.close()

    def _serialize_value(self, value: Any) -> bytes:
        """åºåˆ—åŒ–å€¼"""
        # ä½¿ç”¨pickleåºåˆ—åŒ–ï¼Œæ”¯æŒå¤æ‚å¯¹è±¡
        serialized = pickle.dumps(value)

        # å‹ç¼©å¤§æ•°æ®
        if (
            self.config.compression_enabled
            and len(serialized) > self.config.compression_threshold
        ):
            serialized = zlib.compress(serialized)
            # æ·»åŠ å‹ç¼©æ ‡è®°
            serialized = b"COMPRESSED:" + serialized

        return serialized

    def _deserialize_value(self, data: bytes) -> Any:
        """ååºåˆ—åŒ–å€¼"""
        if data.startswith(b"COMPRESSED:"):
            # è§£å‹ç¼©
            data = zlib.decompress(data[11:])

        return pickle.loads(data)

    def _generate_key(self, namespace: str, key: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"perfect21:{namespace}:{key}"

    async def get(self, namespace: str, key: str, default: Any = None) -> Any:
        """è·å–ç¼“å­˜å€¼"""
        cache_key = self._generate_key(namespace, key)

        # 1. æ£€æŸ¥æœ¬åœ°ç¼“å­˜ (L1)
        if cache_key in self.local_cache:
            if datetime.now() < self.local_cache_ttl.get(cache_key, datetime.min):
                self.stats["hits"] += 1
                logger.debug(f"ğŸ¯ L1ç¼“å­˜å‘½ä¸­: {cache_key}")
                return self.local_cache[cache_key]
            else:
                # L1ç¼“å­˜è¿‡æœŸ
                self._remove_from_local_cache(cache_key)

        # 2. æ£€æŸ¥Redisç¼“å­˜ (L2)
        try:
            async with self._get_connection() as conn:
                data = await conn.get(cache_key)
                if data:
                    value = self._deserialize_value(data)

                    # æ›´æ–°æœ¬åœ°ç¼“å­˜
                    self._update_local_cache(cache_key, value)

                    self.stats["hits"] += 1
                    logger.debug(f"ğŸ¯ L2ç¼“å­˜å‘½ä¸­: {cache_key}")
                    return value

        except Exception as e:
            logger.error(f"âŒ Redisè·å–å¤±è´¥: {e}")
            self.stats["errors"] += 1

        self.stats["misses"] += 1
        logger.debug(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {cache_key}")
        return default

    async def set(
        self, namespace: str, key: str, value: Any, ttl: Optional[int] = None
    ) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        cache_key = self._generate_key(namespace, key)
        ttl = ttl or self.config.default_ttl

        try:
            # åºåˆ—åŒ–å€¼
            serialized_value = self._serialize_value(value)

            # è®¾ç½®Redisç¼“å­˜
            async with self._get_connection() as conn:
                await conn.setex(cache_key, ttl, serialized_value)

            # æ›´æ–°æœ¬åœ°ç¼“å­˜
            self._update_local_cache(cache_key, value, ttl=min(ttl, 60))  # æœ¬åœ°ç¼“å­˜æœ€å¤š1åˆ†é’Ÿ

            self.stats["sets"] += 1
            logger.debug(f"âœ… ç¼“å­˜è®¾ç½®æˆåŠŸ: {cache_key} (TTL: {ttl}s)")
            return True

        except Exception as e:
            logger.error(f"âŒ Redisè®¾ç½®å¤±è´¥: {e}")
            self.stats["errors"] += 1
            return False

    async def delete(self, namespace: str, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜å€¼"""
        cache_key = self._generate_key(namespace, key)

        try:
            # åˆ é™¤Redisç¼“å­˜
            async with self._get_connection() as conn:
                result = await conn.delete(cache_key)

            # åˆ é™¤æœ¬åœ°ç¼“å­˜
            self._remove_from_local_cache(cache_key)

            self.stats["deletes"] += 1
            logger.debug(f"ğŸ—‘ï¸ ç¼“å­˜åˆ é™¤: {cache_key}")
            return bool(result)

        except Exception as e:
            logger.error(f"âŒ Redisåˆ é™¤å¤±è´¥: {e}")
            self.stats["errors"] += 1
            return False

    async def delete_pattern(self, namespace: str, pattern: str) -> int:
        """æ‰¹é‡åˆ é™¤ç¼“å­˜ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰"""
        full_pattern = self._generate_key(namespace, pattern)

        try:
            async with self._get_connection() as conn:
                keys = await conn.keys(full_pattern)
                if keys:
                    deleted = await conn.delete(*keys)

                    # åˆ é™¤æœ¬åœ°ç¼“å­˜ä¸­åŒ¹é…çš„é”®
                    for key in keys:
                        self._remove_from_local_cache(
                            key.decode() if isinstance(key, bytes) else key
                        )

                    logger.info(f"ğŸ—‘ï¸ æ‰¹é‡åˆ é™¤ç¼“å­˜: {len(keys)}ä¸ªé”®")
                    return deleted

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åˆ é™¤å¤±è´¥: {e}")
            self.stats["errors"] += 1

        return 0

    async def exists(self, namespace: str, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        cache_key = self._generate_key(namespace, key)

        # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        if cache_key in self.local_cache:
            if datetime.now() < self.local_cache_ttl.get(cache_key, datetime.min):
                return True

        # æ£€æŸ¥Redisç¼“å­˜
        try:
            async with self._get_connection() as conn:
                return bool(await conn.exists(cache_key))
        except Exception as e:
            logger.error(f"âŒ Redisæ£€æŸ¥å­˜åœ¨å¤±è´¥: {e}")
            return False

    async def increment(
        self, namespace: str, key: str, amount: int = 1
    ) -> Optional[int]:
        """åŸå­é€’å¢æ“ä½œ"""
        cache_key = self._generate_key(namespace, key)

        try:
            async with self._get_connection() as conn:
                result = await conn.incrby(cache_key, amount)
                return result
        except Exception as e:
            logger.error(f"âŒ Redisé€’å¢å¤±è´¥: {e}")
            return None

    async def get_multiple(self, namespace: str, keys: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡è·å–ç¼“å­˜"""
        cache_keys = [self._generate_key(namespace, key) for key in keys]
        result = {}

        try:
            async with self._get_connection() as conn:
                values = await conn.mget(cache_keys)

                for i, (original_key, cache_key, value) in enumerate(
                    zip(keys, cache_keys, values)
                ):
                    if value:
                        try:
                            result[original_key] = self._deserialize_value(value)
                            # æ›´æ–°æœ¬åœ°ç¼“å­˜
                            self._update_local_cache(cache_key, result[original_key])
                        except Exception as e:
                            logger.error(f"âŒ ååºåˆ—åŒ–å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡è·å–å¤±è´¥: {e}")

        return result

    async def set_multiple(
        self, namespace: str, mapping: Dict[str, Any], ttl: Optional[int] = None
    ) -> bool:
        """æ‰¹é‡è®¾ç½®ç¼“å­˜"""
        ttl = ttl or self.config.default_ttl

        try:
            async with self._get_connection() as conn:
                pipe = conn.pipeline()

                for key, value in mapping.items():
                    cache_key = self._generate_key(namespace, key)
                    serialized_value = self._serialize_value(value)
                    pipe.setex(cache_key, ttl, serialized_value)

                    # æ›´æ–°æœ¬åœ°ç¼“å­˜
                    self._update_local_cache(cache_key, value, ttl=min(ttl, 60))

                await pipe.execute()

                self.stats["sets"] += len(mapping)
                logger.debug(f"âœ… æ‰¹é‡è®¾ç½®ç¼“å­˜: {len(mapping)}ä¸ªé”®")
                return True

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡è®¾ç½®å¤±è´¥: {e}")
            return False

    def _update_local_cache(self, key: str, value: Any, ttl: int = 60):
        """æ›´æ–°æœ¬åœ°ç¼“å­˜"""
        self.local_cache[key] = value
        self.local_cache_ttl[key] = datetime.now() + timedelta(seconds=ttl)

        # é™åˆ¶æœ¬åœ°ç¼“å­˜å¤§å°
        if len(self.local_cache) > 1000:
            # ç§»é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = min(
                self.local_cache_ttl.keys(), key=lambda k: self.local_cache_ttl[k]
            )
            self._remove_from_local_cache(oldest_key)

    def _remove_from_local_cache(self, key: str):
        """ä»æœ¬åœ°ç¼“å­˜ä¸­ç§»é™¤"""
        self.local_cache.pop(key, None)
        self.local_cache_ttl.pop(key, None)

    async def _cleanup_local_cache(self):
        """å®šæœŸæ¸…ç†æœ¬åœ°ç¼“å­˜"""
        while True:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡

                now = datetime.now()
                expired_keys = [
                    key for key, expiry in self.local_cache_ttl.items() if now >= expiry
                ]

                for key in expired_keys:
                    self._remove_from_local_cache(key)

                if expired_keys:
                    logger.debug(f"ğŸ§¹ æ¸…ç†æœ¬åœ°ç¼“å­˜: {len(expired_keys)}ä¸ªè¿‡æœŸé”®")

            except Exception as e:
                logger.error(f"âŒ æœ¬åœ°ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

    async def _periodic_stats_log(self):
        """å®šæœŸè®°å½•ç¼“å­˜ç»Ÿè®¡"""
        while True:
            try:
                await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿè®°å½•ä¸€æ¬¡

                total_operations = sum(self.stats.values())
                if total_operations > 0:
                    hit_rate = (
                        self.stats["hits"] / (self.stats["hits"] + self.stats["misses"])
                    ) * 100
                    logger.info(
                        f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡ - "
                        f"å‘½ä¸­ç‡: {hit_rate:.2f}%, "
                        f"å‘½ä¸­: {self.stats['hits']}, "
                        f"æœªå‘½ä¸­: {self.stats['misses']}, "
                        f"è®¾ç½®: {self.stats['sets']}, "
                        f"åˆ é™¤: {self.stats['deletes']}, "
                        f"é”™è¯¯: {self.stats['errors']}"
                    )

            except Exception as e:
                logger.error(f"âŒ ç»Ÿè®¡è®°å½•å¤±è´¥: {e}")

    async def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_operations = self.stats["hits"] + self.stats["misses"]
        hit_rate = (
            (self.stats["hits"] / total_operations * 100) if total_operations > 0 else 0
        )

        return {
            **self.stats,
            "hit_rate": hit_rate,
            "local_cache_size": len(self.local_cache),
            "redis_connected": await self._check_redis_connection(),
        }

    async def _check_redis_connection(self) -> bool:
        """æ£€æŸ¥Redisè¿æ¥çŠ¶æ€"""
        try:
            async with self._get_connection() as conn:
                await conn.ping()
                return True
        except:
            return False

    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        return await self._check_redis_connection()

    async def ready_check(self) -> bool:
        """å°±ç»ªæ£€æŸ¥"""
        return await self._check_redis_connection()

    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.redis_pool:
            await self.redis_pool.disconnect()
            logger.info("ğŸ“´ Redisè¿æ¥æ± å·²å…³é—­")


def cache_result(namespace: str, ttl: int = 300, key_func: Optional[callable] = None):
    """ç¼“å­˜è£…é¥°å™¨ - è‡ªåŠ¨ç¼“å­˜å‡½æ•°ç»“æœ"""

    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                # åŸºäºå‡½æ•°åå’Œå‚æ•°ç”Ÿæˆé”®
                key_parts = [func.__name__]
                key_parts.extend(str(arg) for arg in args)
                key_parts.extend(f"{k}={v}" for k, v in sorted(kwargs.items()))
                cache_key = hashlib.md5(":".join(key_parts).encode()).hexdigest()

            # è·å–ç¼“å­˜ç®¡ç†å™¨å®ä¾‹ï¼ˆéœ€è¦ä»åº”ç”¨çŠ¶æ€ä¸­è·å–ï¼‰
            cache_manager = getattr(wrapper, "_cache_manager", None)
            if not cache_manager:
                # å¦‚æœæ²¡æœ‰ç¼“å­˜ç®¡ç†å™¨ï¼Œç›´æ¥æ‰§è¡Œå‡½æ•°
                return await func(*args, **kwargs)

            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = await cache_manager.get(namespace, cache_key)
            if cached_result is not None:
                return cached_result

            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = await func(*args, **kwargs)
            await cache_manager.set(namespace, cache_key, result, ttl)

            return result

        return wrapper

    return decorator


# ç¼“å­˜è£…é¥°å™¨ä½¿ç”¨ç¤ºä¾‹
@cache_result("user_profile", ttl=600, key_func=lambda user_id: f"user_{user_id}")
async def get_user_profile(user_id: int):
    """è·å–ç”¨æˆ·èµ„æ–™ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    # å®é™…çš„æ•°æ®åº“æŸ¥è¯¢é€»è¾‘
    pass


@cache_result("permissions", ttl=300)
async def get_user_permissions(user_id: int, resource: str):
    """è·å–ç”¨æˆ·æƒé™ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    # å®é™…çš„æƒé™æŸ¥è¯¢é€»è¾‘
    pass
