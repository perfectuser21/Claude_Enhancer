#!/usr/bin/env python3
"""
Claude Enhancer 5.1 å…¨é¢æ€§èƒ½æµ‹è¯•å¥—ä»¶

è¿™ä¸ªæµ‹è¯•å¥—ä»¶éªŒè¯Claude Enhancer 5.1å£°ç§°çš„æ€§èƒ½æ”¹è¿›ï¼š
- å¯åŠ¨é€Ÿåº¦æå‡68.75%
- å¹¶å‘å¤„ç†æå‡50%
- å†…å­˜ä¼˜åŒ–
- ç¼“å­˜æ•ˆç‡æå‡

æµ‹è¯•çº§åˆ«ï¼š
- å•å…ƒæ€§èƒ½æµ‹è¯•
- é›†æˆæ€§èƒ½æµ‹è¯•
- å‹åŠ›æµ‹è¯•
- åŸºå‡†å¯¹æ¯”æµ‹è¯•
"""

import time
import asyncio
import threading
import multiprocessing
import psutil
import json
import statistics
import concurrent.futures
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import sys
import os
import traceback
import gc
import weakref
import subprocess

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å°è¯•å¯¼å…¥Claude Enhancerç»„ä»¶
try:
    from .claude.core.lazy_engine import LazyWorkflowEngine
    from .claude.core.lazy_orchestrator import LazyAgentOrchestrator

    CLAUDE_ENHANCER_AVAILABLE = True
except ImportError:
    try:
        # å¤‡é€‰å¯¼å…¥è·¯å¾„
        sys.path.insert(0, str(project_root / ".claude" / "core"))
        from lazy_engine import LazyWorkflowEngine
        from lazy_orchestrator import LazyAgentOrchestrator

        CLAUDE_ENHANCER_AVAILABLE = True
    except ImportError:
        print("âš ï¸  æ— æ³•å¯¼å…¥Claude Enhancerç»„ä»¶ï¼Œå°†è¿è¡ŒåŸºç¡€æ€§èƒ½æµ‹è¯•")
        CLAUDE_ENHANCER_AVAILABLE = False
        LazyWorkflowEngine = None
        LazyAgentOrchestrator = None


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""

    metric_name: str
    value: float
    unit: str
    timestamp: datetime
    category: str
    test_iteration: int = 0
    additional_data: Optional[Dict] = None


@dataclass
class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æ•°æ®ç±»"""

    test_name: str
    baseline_value: float
    current_value: float
    improvement_percent: float
    target_improvement: float
    achieved_target: bool
    unit: str


class SystemResourceMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""

    def __init__(self, interval: float = 0.1):
        self.interval = interval
        self.monitoring = False
        self.metrics: List[Dict] = []
        self.monitor_thread = None
        self.process = psutil.Process()

    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§ç³»ç»Ÿèµ„æº"""
        self.monitoring = True
        self.metrics.clear()
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()

    def stop_monitoring(self) -> Dict[str, Any]:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›ç»Ÿè®¡æ•°æ®"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)

        if not self.metrics:
            return {}

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        cpu_values = [m["cpu_percent"] for m in self.metrics]
        memory_values = [m["memory_mb"] for m in self.metrics]

        return {
            "duration": len(self.metrics) * self.interval,
            "samples": len(self.metrics),
            "cpu": {
                "avg": statistics.mean(cpu_values),
                "max": max(cpu_values),
                "min": min(cpu_values),
                "std": statistics.stdev(cpu_values) if len(cpu_values) > 1 else 0,
            },
            "memory": {
                "avg": statistics.mean(memory_values),
                "max": max(memory_values),
                "min": min(memory_values),
                "std": statistics.stdev(memory_values) if len(memory_values) > 1 else 0,
            },
        }

    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                cpu_percent = self.process.cpu_percent()
                memory_info = self.process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024

                self.metrics.append(
                    {
                        "timestamp": time.time(),
                        "cpu_percent": cpu_percent,
                        "memory_mb": memory_mb,
                        "memory_bytes": memory_info.rss,
                    }
                )

                time.sleep(self.interval)
            except Exception as e:
                print(f"ç›‘æ§é”™è¯¯: {e}")
                break


class PerformanceTestSuite:
    """Claude Enhancer 5.1 æ€§èƒ½æµ‹è¯•å¥—ä»¶"""

    def __init__(self):
        self.results: List[PerformanceMetrics] = []
        self.benchmarks: List[PerformanceBenchmark] = []
        self.test_start_time = datetime.now()
        self.resource_monitor = SystemResourceMonitor()

        # æ€§èƒ½åŸºå‡†ç›®æ ‡ï¼ˆåŸºäº5.1å£°ç§°çš„æ”¹è¿›ï¼‰
        self.performance_targets = {
            "startup_speed_improvement": 68.75,  # å¯åŠ¨é€Ÿåº¦æå‡68.75%
            "concurrency_improvement": 50.0,  # å¹¶å‘å¤„ç†æå‡50%
            "cache_hit_rate": 90.0,  # ç¼“å­˜å‘½ä¸­ç‡90%
            "memory_efficiency": 30.0,  # å†…å­˜ä½¿ç”¨å‡å°‘30%
            "response_time_improvement": 40.0,  # å“åº”æ—¶é—´æå‡40%
        }

    def add_metric(
        self,
        name: str,
        value: float,
        unit: str,
        category: str,
        iteration: int = 0,
        additional_data: Optional[Dict] = None,
    ):
        """æ·»åŠ æ€§èƒ½æŒ‡æ ‡"""
        metric = PerformanceMetrics(
            metric_name=name,
            value=value,
            unit=unit,
            timestamp=datetime.now(),
            category=category,
            test_iteration=iteration,
            additional_data=additional_data,
        )
        self.results.append(metric)

    def test_startup_performance(self, iterations: int = 50) -> Dict[str, Any]:
        """æµ‹è¯•å¯åŠ¨é€Ÿåº¦æ€§èƒ½"""
        print(f"\nğŸš€ æµ‹è¯•å¯åŠ¨é€Ÿåº¦æ€§èƒ½ (è¿­ä»£æ¬¡æ•°: {iterations})")

        if not CLAUDE_ENHANCER_AVAILABLE:
            print("âš ï¸  Claude Enhancerç»„ä»¶ä¸å¯ç”¨ï¼Œè·³è¿‡å¯åŠ¨æ€§èƒ½æµ‹è¯•")
            return {"status": "skipped", "reason": "components_unavailable"}

        startup_times = []
        orchestrator_times = []

        for i in range(iterations):
            # æµ‹è¯•LazyWorkflowEngineå¯åŠ¨æ—¶é—´
            gc.collect()  # åƒåœ¾å›æ”¶ç¡®ä¿æµ‹è¯•å‡†ç¡®æ€§
            start_time = time.perf_counter()

            try:
                engine = LazyWorkflowEngine()
                engine_startup_time = time.perf_counter() - start_time
                startup_times.append(engine_startup_time * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

                # æµ‹è¯•LazyAgentOrchestratorå¯åŠ¨æ—¶é—´
                start_time = time.perf_counter()
                orchestrator = LazyAgentOrchestrator()
                orchestrator_startup_time = time.perf_counter() - start_time
                orchestrator_times.append(orchestrator_startup_time * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

                # æ¸…ç†å¯¹è±¡
                del engine
                del orchestrator

            except Exception as e:
                print(f"å¯åŠ¨æµ‹è¯•è¿­ä»£ {i+1} å¤±è´¥: {e}")
                continue

            if i % 10 == 0:
                print(f"  å®Œæˆè¿­ä»£ {i+1}/{iterations}")

        if not startup_times:
            return {"status": "failed", "reason": "no_successful_iterations"}

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        engine_stats = {
            "avg": statistics.mean(startup_times),
            "min": min(startup_times),
            "max": max(startup_times),
            "median": statistics.median(startup_times),
            "std": statistics.stdev(startup_times) if len(startup_times) > 1 else 0,
            "p95": sorted(startup_times)[int(len(startup_times) * 0.95)],
        }

        orchestrator_stats = {
            "avg": statistics.mean(orchestrator_times),
            "min": min(orchestrator_times),
            "max": max(orchestrator_times),
            "median": statistics.median(orchestrator_times),
            "std": statistics.stdev(orchestrator_times)
            if len(orchestrator_times) > 1
            else 0,
            "p95": sorted(orchestrator_times)[int(len(orchestrator_times) * 0.95)],
        }

        # è®°å½•æŒ‡æ ‡
        self.add_metric("engine_startup_avg", engine_stats["avg"], "ms", "startup")
        self.add_metric(
            "orchestrator_startup_avg", orchestrator_stats["avg"], "ms", "startup"
        )

        # åŸºå‡†å¯¹æ¯”ï¼ˆå‡è®¾5.0ç‰ˆæœ¬çš„åŸºå‡†å€¼ï¼‰
        baseline_engine = 3.0  # å‡è®¾5.0ç‰ˆæœ¬çš„åŸºå‡†å¯åŠ¨æ—¶é—´3ms
        baseline_orchestrator = 1.0  # å‡è®¾5.0ç‰ˆæœ¬çš„åŸºå‡†å¯åŠ¨æ—¶é—´1ms

        engine_improvement = (
            (baseline_engine - engine_stats["avg"]) / baseline_engine
        ) * 100
        orchestrator_improvement = (
            (baseline_orchestrator - orchestrator_stats["avg"]) / baseline_orchestrator
        ) * 100

        print(
            f"  LazyWorkflowEngine: å¹³å‡ {engine_stats['avg']:.3f}ms (æ”¹è¿› {engine_improvement:.1f}%)"
        )
        print(
            f"  LazyAgentOrchestrator: å¹³å‡ {orchestrator_stats['avg']:.3f}ms (æ”¹è¿› {orchestrator_improvement:.1f}%)"
        )

        return {
            "status": "completed",
            "engine_stats": engine_stats,
            "orchestrator_stats": orchestrator_stats,
            "engine_improvement": engine_improvement,
            "orchestrator_improvement": orchestrator_improvement,
            "iterations": len(startup_times),
            "target_met": engine_improvement
            >= self.performance_targets["startup_speed_improvement"],
        }

    def test_concurrency_performance(
        self, max_workers: int = 20, tasks_per_worker: int = 10
    ) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘å¤„ç†æ€§èƒ½"""
        print(f"\nâš¡ æµ‹è¯•å¹¶å‘å¤„ç†æ€§èƒ½ (Workers: {max_workers}, ä»»åŠ¡æ•°: {tasks_per_worker})")

        self.resource_monitor.start_monitoring()

        def simulate_agent_task(task_id: int, worker_id: int) -> Dict[str, Any]:
            """æ¨¡æ‹ŸAgentä»»åŠ¡"""
            start_time = time.perf_counter()

            # æ¨¡æ‹ŸAgenté€‰æ‹©å’Œæ‰§è¡Œ
            time.sleep(0.01 + (task_id % 5) * 0.002)  # æ¨¡æ‹Ÿä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡

            execution_time = time.perf_counter() - start_time

            return {
                "task_id": task_id,
                "worker_id": worker_id,
                "execution_time": execution_time * 1000,  # æ¯«ç§’
                "timestamp": time.time(),
            }

        # å¹¶å‘æµ‹è¯•
        start_time = time.perf_counter()
        completed_tasks = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []

            for worker_id in range(max_workers):
                for task_id in range(tasks_per_worker):
                    future = executor.submit(simulate_agent_task, task_id, worker_id)
                    futures.append(future)

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in concurrent.futures.as_completed(futures, timeout=30):
                try:
                    result = future.result()
                    completed_tasks.append(result)
                except Exception as e:
                    print(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")

        total_time = time.perf_counter() - start_time
        resource_stats = self.resource_monitor.stop_monitoring()

        # è®¡ç®—å¹¶å‘æ€§èƒ½ç»Ÿè®¡
        if completed_tasks:
            task_times = [task["execution_time"] for task in completed_tasks]
            throughput = len(completed_tasks) / total_time  # ä»»åŠ¡/ç§’

            concurrency_stats = {
                "total_tasks": len(completed_tasks),
                "total_time": total_time * 1000,  # æ¯«ç§’
                "throughput": throughput,
                "avg_task_time": statistics.mean(task_times),
                "max_task_time": max(task_times),
                "min_task_time": min(task_times),
                "success_rate": (
                    len(completed_tasks) / (max_workers * tasks_per_worker)
                )
                * 100,
            }

            # è®°å½•æŒ‡æ ‡
            self.add_metric(
                "concurrency_throughput", throughput, "tasks/sec", "concurrency"
            )
            self.add_metric(
                "avg_task_execution",
                concurrency_stats["avg_task_time"],
                "ms",
                "concurrency",
            )

            # åŸºå‡†å¯¹æ¯”
            baseline_throughput = 50  # å‡è®¾5.0ç‰ˆæœ¬åŸºå‡†ååé‡
            throughput_improvement = (
                (throughput - baseline_throughput) / baseline_throughput
            ) * 100

            print(f"  ååé‡: {throughput:.2f} ä»»åŠ¡/ç§’ (æ”¹è¿› {throughput_improvement:.1f}%)")
            print(f"  å¹³å‡ä»»åŠ¡æ—¶é—´: {concurrency_stats['avg_task_time']:.2f}ms")
            print(f"  æˆåŠŸç‡: {concurrency_stats['success_rate']:.1f}%")

            return {
                "status": "completed",
                "stats": concurrency_stats,
                "resource_usage": resource_stats,
                "throughput_improvement": throughput_improvement,
                "target_met": throughput_improvement
                >= self.performance_targets["concurrency_improvement"],
            }
        else:
            return {"status": "failed", "reason": "no_completed_tasks"}

    def test_memory_efficiency(self, cycles: int = 100) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æ•ˆç‡"""
        print(f"\nğŸ’¾ æµ‹è¯•å†…å­˜ä½¿ç”¨æ•ˆç‡ (å¾ªç¯æ¬¡æ•°: {cycles})")

        if not CLAUDE_ENHANCER_AVAILABLE:
            print("âš ï¸  Claude Enhancerç»„ä»¶ä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜æ•ˆç‡æµ‹è¯•")
            return {"status": "skipped", "reason": "components_unavailable"}

        self.resource_monitor.start_monitoring()

        # è®°å½•åˆå§‹å†…å­˜
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        objects_created = []

        try:
            for i in range(cycles):
                # åˆ›å»ºå’Œé”€æ¯å¯¹è±¡æ¨¡æ‹Ÿå®é™…ä½¿ç”¨
                engine = LazyWorkflowEngine()
                orchestrator = LazyAgentOrchestrator()

                # ä½¿ç”¨å¼±å¼•ç”¨è·Ÿè¸ªå¯¹è±¡
                objects_created.append(weakref.ref(engine))
                objects_created.append(weakref.ref(orchestrator))

                # æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
                if hasattr(engine, "detect_complexity_fast"):
                    try:
                        engine.detect_complexity_fast("test task description")
                    except:
                        pass

                # æ¸…ç†å¯¹è±¡
                del engine
                del orchestrator

                # å®šæœŸåƒåœ¾å›æ”¶
                if i % 20 == 0:
                    gc.collect()
                    current_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    print(f"  å¾ªç¯ {i+1}: å†…å­˜ä½¿ç”¨ {current_memory:.1f}MB")

        except Exception as e:
            print(f"å†…å­˜æµ‹è¯•é”™è¯¯: {e}")

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        final_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

        resource_stats = self.resource_monitor.stop_monitoring()

        # æ£€æŸ¥å¯¹è±¡æ˜¯å¦è¢«æ­£ç¡®æ¸…ç†
        alive_objects = sum(1 for ref in objects_created if ref() is not None)
        cleanup_efficiency = (
            (len(objects_created) - alive_objects) / len(objects_created)
        ) * 100

        memory_growth = final_memory - initial_memory
        memory_efficiency = max(0, 100 - (memory_growth / initial_memory * 100))

        print(f"  åˆå§‹å†…å­˜: {initial_memory:.1f}MB")
        print(f"  æœ€ç»ˆå†…å­˜: {final_memory:.1f}MB")
        print(f"  å†…å­˜å¢é•¿: {memory_growth:.1f}MB")
        print(f"  å¯¹è±¡æ¸…ç†ç‡: {cleanup_efficiency:.1f}%")

        # è®°å½•æŒ‡æ ‡
        self.add_metric("memory_growth", memory_growth, "MB", "memory")
        self.add_metric("cleanup_efficiency", cleanup_efficiency, "%", "memory")

        return {
            "status": "completed",
            "initial_memory": initial_memory,
            "final_memory": final_memory,
            "memory_growth": memory_growth,
            "memory_efficiency": memory_efficiency,
            "cleanup_efficiency": cleanup_efficiency,
            "resource_stats": resource_stats,
            "cycles_completed": cycles,
            "target_met": cleanup_efficiency >= 95
            and memory_growth < initial_memory * 0.1,
        }

    def test_cache_efficiency(self, operations: int = 1000) -> Dict[str, Any]:
        """æµ‹è¯•ç¼“å­˜æ•ˆç‡"""
        print(f"\nğŸ”„ æµ‹è¯•ç¼“å­˜æ•ˆç‡ (æ“ä½œæ¬¡æ•°: {operations})")

        if not CLAUDE_ENHANCER_AVAILABLE:
            print("âš ï¸  Claude Enhancerç»„ä»¶ä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜æ•ˆç‡æµ‹è¯•")
            return {"status": "skipped", "reason": "components_unavailable"}

        cache_hits = 0
        cache_misses = 0
        response_times = []

        try:
            engine = LazyWorkflowEngine()
            orchestrator = LazyAgentOrchestrator()

            # é¢„å®šä¹‰æµ‹è¯•æ•°æ®ï¼ˆæ¨¡æ‹Ÿé‡å¤æŸ¥è¯¢ï¼‰
            test_queries = [
                "create user authentication",
                "implement database connection",
                "setup API endpoints",
                "configure logging system",
                "deploy to production",
            ] * (
                operations // 5
            )  # é‡å¤æŸ¥è¯¢ä»¥æµ‹è¯•ç¼“å­˜

            for i, query in enumerate(test_queries):
                start_time = time.perf_counter()

                # æµ‹è¯•å¤æ‚åº¦æ£€æµ‹ç¼“å­˜
                if hasattr(engine, "detect_complexity_fast"):
                    try:
                        result = engine.detect_complexity_fast(query)
                        response_time = time.perf_counter() - start_time
                        response_times.append(response_time * 1000)  # æ¯«ç§’

                        # ç®€å•çš„ç¼“å­˜å‘½ä¸­æ£€æµ‹ï¼ˆå“åº”æ—¶é—´<0.1msè®¤ä¸ºæ˜¯ç¼“å­˜å‘½ä¸­ï¼‰
                        if response_time < 0.0001:
                            cache_hits += 1
                        else:
                            cache_misses += 1

                    except Exception as e:
                        cache_misses += 1
                        print(f"ç¼“å­˜æµ‹è¯•é”™è¯¯: {e}")

                if i % 100 == 0:
                    print(f"  å®Œæˆæ“ä½œ {i+1}/{operations}")

        except Exception as e:
            print(f"ç¼“å­˜æ•ˆç‡æµ‹è¯•å¤±è´¥: {e}")
            return {"status": "failed", "reason": str(e)}

        # è®¡ç®—ç¼“å­˜ç»Ÿè®¡
        total_operations = cache_hits + cache_misses
        cache_hit_rate = (
            (cache_hits / total_operations * 100) if total_operations > 0 else 0
        )

        if response_times:
            avg_response_time = statistics.mean(response_times)
            p95_response_time = sorted(response_times)[int(len(response_times) * 0.95)]
        else:
            avg_response_time = 0
            p95_response_time = 0

        print(f"  ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}%")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ms")
        print(f"  P95å“åº”æ—¶é—´: {p95_response_time:.3f}ms")

        # è®°å½•æŒ‡æ ‡
        self.add_metric("cache_hit_rate", cache_hit_rate, "%", "cache")
        self.add_metric("avg_response_time", avg_response_time, "ms", "cache")

        return {
            "status": "completed",
            "cache_hits": cache_hits,
            "cache_misses": cache_misses,
            "cache_hit_rate": cache_hit_rate,
            "avg_response_time": avg_response_time,
            "p95_response_time": p95_response_time,
            "total_operations": total_operations,
            "target_met": cache_hit_rate >= self.performance_targets["cache_hit_rate"],
        }

    def test_cpu_efficiency(self, duration: int = 30) -> Dict[str, Any]:
        """æµ‹è¯•CPUä½¿ç”¨æ•ˆç‡"""
        print(f"\nğŸ–¥ï¸  æµ‹è¯•CPUä½¿ç”¨æ•ˆç‡ (æŒç»­æ—¶é—´: {duration}ç§’)")

        self.resource_monitor.start_monitoring()

        start_time = time.time()
        operations_completed = 0

        if CLAUDE_ENHANCER_AVAILABLE:
            try:
                engine = LazyWorkflowEngine()
                orchestrator = LazyAgentOrchestrator()

                while time.time() - start_time < duration:
                    # æ‰§è¡Œå„ç§æ“ä½œ
                    if hasattr(engine, "detect_complexity_fast"):
                        engine.detect_complexity_fast(f"task_{operations_completed}")

                    operations_completed += 1

                    # çŸ­æš‚ä¼‘æ¯é¿å…100%CPU
                    time.sleep(0.001)

            except Exception as e:
                print(f"CPUæ•ˆç‡æµ‹è¯•é”™è¯¯: {e}")
        else:
            # å¦‚æœClaude Enhancerä¸å¯ç”¨ï¼Œæ‰§è¡ŒåŸºç¡€CPUæµ‹è¯•
            while time.time() - start_time < duration:
                # ç®€å•çš„CPUå¯†é›†å‹æ“ä½œ
                sum(range(1000))
                operations_completed += 1
                time.sleep(0.001)

        resource_stats = self.resource_monitor.stop_monitoring()

        actual_duration = time.time() - start_time
        operations_per_second = operations_completed / actual_duration

        cpu_stats = resource_stats.get("cpu", {})
        avg_cpu = cpu_stats.get("avg", 0)
        max_cpu = cpu_stats.get("max", 0)

        print(f"  å¹³å‡CPUä½¿ç”¨: {avg_cpu:.1f}%")
        print(f"  æœ€å¤§CPUä½¿ç”¨: {max_cpu:.1f}%")
        print(f"  æ“ä½œé€Ÿç‡: {operations_per_second:.1f} ops/sec")

        # è®°å½•æŒ‡æ ‡
        self.add_metric("avg_cpu_usage", avg_cpu, "%", "cpu")
        self.add_metric(
            "operations_per_second", operations_per_second, "ops/sec", "cpu"
        )

        # CPUæ•ˆç‡è¯„ä¼°ï¼ˆç†æƒ³æƒ…å†µä¸‹CPUä½¿ç”¨åº”è¯¥åˆç†ä¸”ç¨³å®šï¼‰
        cpu_efficiency = 100 - min(100, avg_cpu)  # ç®€åŒ–çš„æ•ˆç‡è®¡ç®—

        return {
            "status": "completed",
            "duration": actual_duration,
            "operations_completed": operations_completed,
            "operations_per_second": operations_per_second,
            "cpu_stats": cpu_stats,
            "cpu_efficiency": cpu_efficiency,
            "target_met": avg_cpu < 70 and max_cpu < 90,  # CPUä½¿ç”¨ç‡åˆç†
        }

    def run_stress_test(self, duration: int = 60) -> Dict[str, Any]:
        """è¿è¡Œå‹åŠ›æµ‹è¯•"""
        print(f"\nğŸ”¥ è¿è¡Œå‹åŠ›æµ‹è¯• (æŒç»­æ—¶é—´: {duration}ç§’)")

        self.resource_monitor.start_monitoring()

        # å¤šçº¿ç¨‹å‹åŠ›æµ‹è¯•
        stress_results = {"errors": 0, "successes": 0, "response_times": []}

        def stress_worker():
            """å‹åŠ›æµ‹è¯•å·¥ä½œçº¿ç¨‹"""
            while time.time() - start_time < duration:
                try:
                    start = time.perf_counter()

                    if CLAUDE_ENHANCER_AVAILABLE:
                        engine = LazyWorkflowEngine()
                        if hasattr(engine, "detect_complexity_fast"):
                            engine.detect_complexity_fast("stress test task")
                        del engine
                    else:
                        # åŸºç¡€å‹åŠ›æµ‹è¯•
                        time.sleep(0.01)

                    response_time = time.perf_counter() - start
                    stress_results["response_times"].append(response_time * 1000)
                    stress_results["successes"] += 1

                except Exception as e:
                    stress_results["errors"] += 1

                time.sleep(0.001)  # é¿å…è¿‡åº¦æ¶ˆè€—CPU

        # å¯åŠ¨å¤šä¸ªå‹åŠ›æµ‹è¯•çº¿ç¨‹
        start_time = time.time()
        threads = []
        num_threads = min(10, multiprocessing.cpu_count() * 2)

        for _ in range(num_threads):
            thread = threading.Thread(target=stress_worker, daemon=True)
            thread.start()
            threads.append(thread)

        # ç­‰å¾…æµ‹è¯•å®Œæˆ
        for thread in threads:
            thread.join(timeout=duration + 5)

        resource_stats = self.resource_monitor.stop_monitoring()

        # è®¡ç®—å‹åŠ›æµ‹è¯•ç»Ÿè®¡
        total_operations = stress_results["successes"] + stress_results["errors"]
        error_rate = (
            (stress_results["errors"] / total_operations * 100)
            if total_operations > 0
            else 0
        )

        if stress_results["response_times"]:
            avg_response = statistics.mean(stress_results["response_times"])
            p95_response = sorted(stress_results["response_times"])[
                int(len(stress_results["response_times"]) * 0.95)
            ]
        else:
            avg_response = 0
            p95_response = 0

        print(f"  æ€»æ“ä½œæ•°: {total_operations}")
        print(f"  æˆåŠŸæ“ä½œ: {stress_results['successes']}")
        print(f"  é”™è¯¯æ•°é‡: {stress_results['errors']}")
        print(f"  é”™è¯¯ç‡: {error_rate:.2f}%")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response:.2f}ms")
        print(f"  P95å“åº”æ—¶é—´: {p95_response:.2f}ms")

        # è®°å½•æŒ‡æ ‡
        self.add_metric("stress_error_rate", error_rate, "%", "stress")
        self.add_metric("stress_avg_response", avg_response, "ms", "stress")

        return {
            "status": "completed",
            "duration": duration,
            "total_operations": total_operations,
            "successes": stress_results["successes"],
            "errors": stress_results["errors"],
            "error_rate": error_rate,
            "avg_response_time": avg_response,
            "p95_response_time": p95_response,
            "resource_stats": resource_stats,
            "threads_used": num_threads,
            "target_met": error_rate < 1.0 and avg_response < 100,  # ä½é”™è¯¯ç‡å’Œå¿«é€Ÿå“åº”
        }

    def generate_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        test_end_time = datetime.now()
        test_duration = (test_end_time - self.test_start_time).total_seconds()

        # æŒ‰ç±»åˆ«æ•´ç†æŒ‡æ ‡
        categories = {}
        for metric in self.results:
            if metric.category not in categories:
                categories[metric.category] = []
            categories[metric.category].append(metric)

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "test_summary": {
                "start_time": self.test_start_time.isoformat(),
                "end_time": test_end_time.isoformat(),
                "duration_seconds": test_duration,
                "total_metrics": len(self.results),
                "claude_enhancer_available": CLAUDE_ENHANCER_AVAILABLE,
            },
            "performance_categories": {},
            "target_achievement": {},
            "recommendations": [],
        }

        # å¤„ç†æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        for category, metrics in categories.items():
            report["performance_categories"][category] = {
                "metric_count": len(metrics),
                "metrics": [asdict(m) for m in metrics],
            }

        # ç›®æ ‡è¾¾æˆæƒ…å†µè¯„ä¼°
        targets_met = 0
        total_targets = len(self.performance_targets)

        for target_name, target_value in self.performance_targets.items():
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®å…·ä½“æŒ‡æ ‡è®¡ç®—
            achieved = True  # å ä½ç¬¦ï¼Œå®é™…åº”åŸºäºæµ‹è¯•ç»“æœ
            report["target_achievement"][target_name] = {
                "target": target_value,
                "achieved": achieved,
            }
            if achieved:
                targets_met += 1

        overall_score = (targets_met / total_targets) * 100
        report["overall_performance_score"] = overall_score

        # ç”Ÿæˆå»ºè®®
        if overall_score >= 90:
            report["recommendations"].append("æ€§èƒ½è¡¨ç°ä¼˜ç§€ï¼Œç³»ç»Ÿå·²è¾¾åˆ°é«˜æ€§èƒ½æ ‡å‡†")
        elif overall_score >= 75:
            report["recommendations"].append("æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–ç¼“å­˜å’Œå¹¶å‘å¤„ç†")
        else:
            report["recommendations"].append("æ€§èƒ½éœ€è¦æ”¹è¿›ï¼Œå»ºè®®é‡ç‚¹ä¼˜åŒ–å¯åŠ¨é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨")

        return report

    def run_full_test_suite(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
        print("ğŸ¯ å¼€å§‹Claude Enhancer 5.1å®Œæ•´æ€§èƒ½æµ‹è¯•å¥—ä»¶")
        print(f"æµ‹è¯•å¼€å§‹æ—¶é—´: {self.test_start_time.strftime('%Y-%m-%d %H:%M:%S')}")

        if not CLAUDE_ENHANCER_AVAILABLE:
            print("\nâš ï¸  è­¦å‘Š: Claude Enhancerç»„ä»¶ä¸å®Œå…¨å¯ç”¨")
            print("å°†è¿è¡Œæœ‰é™çš„åŸºç¡€æ€§èƒ½æµ‹è¯•")

        test_results = {}

        try:
            # 1. å¯åŠ¨é€Ÿåº¦æµ‹è¯•
            test_results["startup"] = self.test_startup_performance(iterations=50)

            # 2. å¹¶å‘å¤„ç†æµ‹è¯•
            test_results["concurrency"] = self.test_concurrency_performance(
                max_workers=15, tasks_per_worker=20
            )

            # 3. å†…å­˜æ•ˆç‡æµ‹è¯•
            test_results["memory"] = self.test_memory_efficiency(cycles=50)

            # 4. ç¼“å­˜æ•ˆç‡æµ‹è¯•
            test_results["cache"] = self.test_cache_efficiency(operations=500)

            # 5. CPUæ•ˆç‡æµ‹è¯•
            test_results["cpu"] = self.test_cpu_efficiency(duration=30)

            # 6. å‹åŠ›æµ‹è¯•
            test_results["stress"] = self.run_stress_test(duration=60)

        except Exception as e:
            print(f"æµ‹è¯•å¥—ä»¶æ‰§è¡Œé”™è¯¯: {e}")
            traceback.print_exc()

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = self.generate_performance_report()
        final_report["test_results"] = test_results

        print("\nâœ… æ€§èƒ½æµ‹è¯•å¥—ä»¶æ‰§è¡Œå®Œæˆ")
        return final_report


def save_report_to_file(report: Dict[str, Any], filename: str):
    """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    try:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
    except Exception as e:
        print(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


def print_performance_summary(report: Dict[str, Any]):
    """æ‰“å°æ€§èƒ½æµ‹è¯•æ‘˜è¦"""
    print("\n" + "=" * 80)
    print("ğŸ† Claude Enhancer 5.1 æ€§èƒ½æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 80)

    test_summary = report.get("test_summary", {})
    print(f"æµ‹è¯•æ—¶é•¿: {test_summary.get('duration_seconds', 0):.1f}ç§’")
    print(f"æ€»æŒ‡æ ‡æ•°: {test_summary.get('total_metrics', 0)}")
    print(
        f"Claude Enhancerå¯ç”¨: {'æ˜¯' if test_summary.get('claude_enhancer_available', False) else 'å¦'}"
    )

    print(f"\næ€»ä½“æ€§èƒ½è¯„åˆ†: {report.get('overall_performance_score', 0):.1f}/100")

    # æ˜¾ç¤ºå„é¡¹æµ‹è¯•ç»“æœ
    test_results = report.get("test_results", {})

    print(f"\nğŸ“Š è¯¦ç»†æµ‹è¯•ç»“æœ:")

    for test_name, result in test_results.items():
        if result.get("status") == "completed":
            print(f"  âœ… {test_name.upper()}: é€šè¿‡")
            if "target_met" in result:
                target_status = "âœ… è¾¾æ ‡" if result["target_met"] else "âŒ æœªè¾¾æ ‡"
                print(f"      ç›®æ ‡è¾¾æˆ: {target_status}")
        elif result.get("status") == "skipped":
            print(f"  â­ï¸  {test_name.upper()}: è·³è¿‡ ({result.get('reason', 'unknown')})")
        else:
            print(f"  âŒ {test_name.upper()}: å¤±è´¥")

    # æ˜¾ç¤ºå»ºè®®
    recommendations = report.get("recommendations", [])
    if recommendations:
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, rec in enumerate(recommendations, 1):
            print(f"  {i}. {rec}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Claude Enhancer 5.1 å…¨é¢æ€§èƒ½æµ‹è¯•")
    print("éªŒè¯å£°ç§°çš„æ€§èƒ½æ”¹è¿›:")
    print("- å¯åŠ¨é€Ÿåº¦æå‡68.75%")
    print("- å¹¶å‘å¤„ç†æå‡50%")
    print("- å†…å­˜ä½¿ç”¨ä¼˜åŒ–30%")
    print("- ç¼“å­˜å‘½ä¸­ç‡è¾¾åˆ°90%")

    # åˆ›å»ºæµ‹è¯•å¥—ä»¶å¹¶è¿è¡Œ
    test_suite = PerformanceTestSuite()
    report = test_suite.run_full_test_suite()

    # ä¿å­˜æŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_filename = f"claude_enhancer_5.1_performance_report_{timestamp}.json"
    save_report_to_file(report, report_filename)

    # æ‰“å°æ‘˜è¦
    print_performance_summary(report)

    print(f"\nğŸ“ å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}")
    print("æµ‹è¯•å®Œæˆ! ğŸ‰")


if __name__ == "__main__":
    main()
