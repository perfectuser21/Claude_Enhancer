#!/usr/bin/env python3
"""
Perfect21 æ€§èƒ½ä¼˜åŒ–å®ç°
åŸºäºæ€§èƒ½åˆ†æç»“æœï¼Œå®ç°å…·ä½“çš„æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ
"""

import os
import sys
import time
import json
import asyncio
import threading
import functools
import weakref
from typing import Dict, List, Any, Optional, Callable, TypeVar, Generic
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import OrderedDict, defaultdict
from pathlib import Path
import logging
import pickle
import hashlib

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__)))

from modules.logger import log_info, log_error, log_warning

logger = logging.getLogger(__name__)

T = TypeVar('T')

# ============================== ç¼“å­˜ä¼˜åŒ– ==============================

class LRUCache(Generic[T]):
    """é«˜æ€§èƒ½LRUç¼“å­˜å®ç°"""

    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        self.max_size = max_size
        self.ttl = ttl
        self.cache: OrderedDict[str, Dict[str, Any]] = OrderedDict()
        self.access_times: Dict[str, float] = {}
        self.hit_count = 0
        self.miss_count = 0
        self._lock = threading.RLock()

    def _generate_key(self, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = str(args) + str(sorted(kwargs.items()))
        return hashlib.md5(key_data.encode()).hexdigest()

    def _is_expired(self, key: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        if key not in self.access_times:
            return True
        return time.time() - self.access_times[key] > self.ttl

    def get(self, key: str) -> Optional[T]:
        """è·å–ç¼“å­˜å€¼"""
        with self._lock:
            if key in self.cache and not self._is_expired(key):
                # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
                value = self.cache.pop(key)
                self.cache[key] = value
                self.access_times[key] = time.time()
                self.hit_count += 1
                return value['data']

            self.miss_count += 1
            return None

    def put(self, key: str, value: T) -> None:
        """å­˜å‚¨ç¼“å­˜å€¼"""
        with self._lock:
            if key in self.cache:
                self.cache.pop(key)
            elif len(self.cache) >= self.max_size:
                # ç§»é™¤æœ€è€çš„é¡¹
                oldest_key = next(iter(self.cache))
                self.cache.pop(oldest_key)
                self.access_times.pop(oldest_key, None)

            self.cache[key] = {'data': value, 'timestamp': time.time()}
            self.access_times[key] = time.time()

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self.cache.clear()
            self.access_times.clear()

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = (self.hit_count / total_requests * 100) if total_requests > 0 else 0

        return {
            'size': len(self.cache),
            'max_size': self.max_size,
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': f"{hit_rate:.1f}%",
            'utilization': f"{len(self.cache) / self.max_size * 100:.1f}%"
        }

def cached(max_size: int = 1000, ttl: int = 3600):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        cache = LRUCache(max_size=max_size, ttl=ttl)

        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = cache._generate_key(*args, **kwargs)

            # å°è¯•ä»ç¼“å­˜è·å–
            result = cache.get(cache_key)
            if result is not None:
                return result

            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = func(*args, **kwargs)
            cache.put(cache_key, result)
            return result

        # æ·»åŠ ç¼“å­˜ç®¡ç†æ–¹æ³•
        wrapper.cache = cache
        wrapper.clear_cache = cache.clear
        wrapper.cache_stats = cache.get_stats

        return wrapper

    return decorator

# ============================== å·¥ä½œæµç¼“å­˜ä¼˜åŒ– ==============================

class WorkflowTemplateCache:
    """å·¥ä½œæµæ¨¡æ¿ç¼“å­˜"""

    def __init__(self, cache_dir: str = ".perfect21/workflow_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.memory_cache = LRUCache(max_size=100, ttl=1800)  # 30åˆ†é’Ÿ
        self.template_signatures: Dict[str, str] = {}

        # åŠ è½½å·²æœ‰çš„æ¨¡æ¿ç­¾å
        self._load_template_signatures()

    def _load_template_signatures(self):
        """åŠ è½½æ¨¡æ¿ç­¾å"""
        signatures_file = self.cache_dir / "signatures.json"
        if signatures_file.exists():
            try:
                with open(signatures_file, 'r') as f:
                    self.template_signatures = json.load(f)
            except Exception as e:
                logger.error(f"Failed to load template signatures: {e}")

    def _save_template_signatures(self):
        """ä¿å­˜æ¨¡æ¿ç­¾å"""
        signatures_file = self.cache_dir / "signatures.json"
        try:
            with open(signatures_file, 'w') as f:
                json.dump(self.template_signatures, f, indent=2)
        except Exception as e:
            logger.error(f"Failed to save template signatures: {e}")

    def _generate_signature(self, workflow_config: Dict[str, Any]) -> str:
        """ç”Ÿæˆå·¥ä½œæµç­¾å"""
        config_str = json.dumps(workflow_config, sort_keys=True)
        return hashlib.sha256(config_str.encode()).hexdigest()

    def get_workflow_template(self, workflow_config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„å·¥ä½œæµæ¨¡æ¿"""
        signature = self._generate_signature(workflow_config)

        # å…ˆä»å†…å­˜ç¼“å­˜è·å–
        cached_template = self.memory_cache.get(signature)
        if cached_template:
            return cached_template

        # ä»ç£ç›˜ç¼“å­˜è·å–
        cache_file = self.cache_dir / f"{signature}.pickle"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    template = pickle.load(f)

                # åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                self.memory_cache.put(signature, template)
                return template

            except Exception as e:
                logger.error(f"Failed to load cached workflow template: {e}")

        return None

    def cache_workflow_template(self, workflow_config: Dict[str, Any],
                              processed_template: Dict[str, Any]):
        """ç¼“å­˜å·¥ä½œæµæ¨¡æ¿"""
        signature = self._generate_signature(workflow_config)

        # å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜
        self.memory_cache.put(signature, processed_template)

        # å­˜å‚¨åˆ°ç£ç›˜ç¼“å­˜
        cache_file = self.cache_dir / f"{signature}.pickle"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(processed_template, f)

            # æ›´æ–°ç­¾åè®°å½•
            self.template_signatures[signature] = {
                'created_at': datetime.now().isoformat(),
                'workflow_name': workflow_config.get('name', 'Unknown')
            }
            self._save_template_signatures()

        except Exception as e:
            logger.error(f"Failed to cache workflow template: {e}")

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        memory_stats = self.memory_cache.get_stats()

        disk_cache_size = 0
        disk_file_count = 0
        for cache_file in self.cache_dir.glob("*.pickle"):
            disk_cache_size += cache_file.stat().st_size
            disk_file_count += 1

        return {
            'memory_cache': memory_stats,
            'disk_cache': {
                'file_count': disk_file_count,
                'total_size_mb': disk_cache_size / 1024 / 1024,
                'templates_cached': len(self.template_signatures)
            }
        }

# ============================== å¹¶è¡Œå¤„ç†ä¼˜åŒ– ==============================

class SmartThreadPool:
    """æ™ºèƒ½çº¿ç¨‹æ± """

    def __init__(self, min_workers: int = 2, max_workers: int = 8,
                 scale_threshold: float = 0.8):
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.scale_threshold = scale_threshold

        self.current_workers = min_workers
        self.task_queue = asyncio.Queue()
        self.workers: List[asyncio.Task] = []
        self.active_tasks = 0
        self.completed_tasks = 0
        self.failed_tasks = 0

        self._lock = asyncio.Lock()
        self._shutdown = False
        self._monitoring_task: Optional[asyncio.Task] = None

    async def start(self):
        """å¯åŠ¨çº¿ç¨‹æ± """
        # åˆ›å»ºåˆå§‹å·¥ä½œçº¿ç¨‹
        for i in range(self.min_workers):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            self.workers.append(worker)

        # å¯åŠ¨ç›‘æ§ä»»åŠ¡
        self._monitoring_task = asyncio.create_task(self._monitor_load())

        log_info(f"SmartThreadPool started with {self.min_workers} workers")

    async def submit(self, func: Callable, *args, **kwargs) -> Any:
        """æäº¤ä»»åŠ¡"""
        future = asyncio.Future()
        task_item = {
            'func': func,
            'args': args,
            'kwargs': kwargs,
            'future': future,
            'submitted_at': time.time()
        }

        await self.task_queue.put(task_item)
        self.active_tasks += 1

        return await future

    async def _worker(self, worker_name: str):
        """å·¥ä½œçº¿ç¨‹"""
        while not self._shutdown:
            try:
                # è·å–ä»»åŠ¡ï¼ˆå¸¦è¶…æ—¶ï¼‰
                task_item = await asyncio.wait_for(
                    self.task_queue.get(),
                    timeout=1.0
                )

                func = task_item['func']
                args = task_item['args']
                kwargs = task_item['kwargs']
                future = task_item['future']

                try:
                    # æ‰§è¡Œä»»åŠ¡
                    if asyncio.iscoroutinefunction(func):
                        result = await func(*args, **kwargs)
                    else:
                        result = func(*args, **kwargs)

                    future.set_result(result)
                    self.completed_tasks += 1

                except Exception as e:
                    future.set_exception(e)
                    self.failed_tasks += 1

                finally:
                    self.active_tasks -= 1
                    self.task_queue.task_done()

            except asyncio.TimeoutError:
                # è¶…æ—¶ï¼Œç»§ç»­å¾ªç¯
                continue
            except Exception as e:
                logger.error(f"Worker {worker_name} error: {e}")

    async def _monitor_load(self):
        """ç›‘æ§è´Ÿè½½å¹¶åŠ¨æ€è°ƒæ•´å·¥ä½œçº¿ç¨‹æ•°"""
        while not self._shutdown:
            try:
                await asyncio.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡

                queue_size = self.task_queue.qsize()
                worker_count = len(self.workers)

                # è®¡ç®—è´Ÿè½½
                load_ratio = queue_size / worker_count if worker_count > 0 else 0

                async with self._lock:
                    if load_ratio > self.scale_threshold and worker_count < self.max_workers:
                        # å¢åŠ å·¥ä½œçº¿ç¨‹
                        new_worker_id = len(self.workers)
                        worker = asyncio.create_task(self._worker(f"worker-{new_worker_id}"))
                        self.workers.append(worker)
                        log_info(f"Scaled up to {len(self.workers)} workers (load: {load_ratio:.2f})")

                    elif load_ratio < 0.2 and worker_count > self.min_workers:
                        # å‡å°‘å·¥ä½œçº¿ç¨‹
                        if self.workers:
                            worker = self.workers.pop()
                            worker.cancel()
                            log_info(f"Scaled down to {len(self.workers)} workers (load: {load_ratio:.2f})")

            except Exception as e:
                logger.error(f"Load monitoring error: {e}")

    async def shutdown(self):
        """å…³é—­çº¿ç¨‹æ± """
        self._shutdown = True

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await self.task_queue.join()

        # å–æ¶ˆæ‰€æœ‰å·¥ä½œçº¿ç¨‹
        for worker in self.workers:
            worker.cancel()

        # å–æ¶ˆç›‘æ§ä»»åŠ¡
        if self._monitoring_task:
            self._monitoring_task.cancel()

        log_info("SmartThreadPool shutdown completed")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'current_workers': len(self.workers),
            'min_workers': self.min_workers,
            'max_workers': self.max_workers,
            'active_tasks': self.active_tasks,
            'completed_tasks': self.completed_tasks,
            'failed_tasks': self.failed_tasks,
            'queue_size': self.task_queue.qsize(),
            'success_rate': (
                self.completed_tasks / (self.completed_tasks + self.failed_tasks) * 100
                if (self.completed_tasks + self.failed_tasks) > 0 else 100
            )
        }

# ============================== å†…å­˜ä¼˜åŒ– ==============================

class MemoryManager:
    """å†…å­˜ç®¡ç†å™¨"""

    def __init__(self, max_memory_mb: int = 512, cleanup_threshold: float = 0.8):
        self.max_memory_mb = max_memory_mb
        self.cleanup_threshold = cleanup_threshold

        self.object_pools: Dict[str, List[Any]] = defaultdict(list)
        self.weak_references: weakref.WeakValueDictionary = weakref.WeakValueDictionary()

        self._cleanup_interval = 60  # 60ç§’
        self._cleanup_task: Optional[asyncio.Task] = None
        self._memory_stats = {
            'allocations': 0,
            'deallocations': 0,
            'pool_hits': 0,
            'pool_misses': 0
        }

    def start_monitoring(self):
        """å¼€å§‹å†…å­˜ç›‘æ§"""
        self._cleanup_task = asyncio.create_task(self._cleanup_loop())
        log_info("Memory manager started")

    async def _cleanup_loop(self):
        """æ¸…ç†å¾ªç¯"""
        import psutil

        while True:
            try:
                await asyncio.sleep(self._cleanup_interval)

                # æ£€æŸ¥å†…å­˜ä½¿ç”¨
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024

                if memory_mb > self.max_memory_mb * self.cleanup_threshold:
                    await self._perform_cleanup()
                    log_info(f"Memory cleanup performed, usage: {memory_mb:.1f}MB")

            except Exception as e:
                logger.error(f"Memory cleanup error: {e}")

    async def _perform_cleanup(self):
        """æ‰§è¡Œå†…å­˜æ¸…ç†"""
        import gc

        # æ¸…ç†å¯¹è±¡æ± 
        for pool_name, pool in self.object_pools.items():
            if len(pool) > 10:  # ä¿ç•™æœ€å¤š10ä¸ªå¯¹è±¡
                pool[:] = pool[-10:]

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        collected = gc.collect()
        log_info(f"Garbage collection freed {collected} objects")

        # æ¸…ç†å¼±å¼•ç”¨
        expired_refs = []
        for key, ref in self.weak_references.items():
            if ref is None:
                expired_refs.append(key)

        for key in expired_refs:
            del self.weak_references[key]

    def get_object(self, object_type: str, factory: Callable = None):
        """ä»å¯¹è±¡æ± è·å–å¯¹è±¡"""
        pool = self.object_pools[object_type]

        if pool:
            obj = pool.pop()
            self._memory_stats['pool_hits'] += 1
            return obj

        # æ± ä¸­æ²¡æœ‰å¯¹è±¡ï¼Œåˆ›å»ºæ–°çš„
        if factory:
            obj = factory()
            self._memory_stats['pool_misses'] += 1
            self._memory_stats['allocations'] += 1
            return obj

        return None

    def return_object(self, object_type: str, obj: Any):
        """å½’è¿˜å¯¹è±¡åˆ°æ± ä¸­"""
        pool = self.object_pools[object_type]

        # é‡ç½®å¯¹è±¡çŠ¶æ€ï¼ˆå¦‚æœæœ‰resetæ–¹æ³•ï¼‰
        if hasattr(obj, 'reset'):
            obj.reset()

        pool.append(obj)
        self._memory_stats['deallocations'] += 1

    def register_weak_reference(self, key: str, obj: Any):
        """æ³¨å†Œå¼±å¼•ç”¨"""
        self.weak_references[key] = obj

    def get_stats(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ç»Ÿè®¡"""
        import psutil

        process = psutil.Process()
        memory_info = process.memory_info()

        return {
            'current_memory_mb': memory_info.rss / 1024 / 1024,
            'max_memory_mb': self.max_memory_mb,
            'pool_stats': {
                pool_name: len(pool)
                for pool_name, pool in self.object_pools.items()
            },
            'allocation_stats': self._memory_stats,
            'weak_references': len(self.weak_references)
        }

# ============================== I/Oä¼˜åŒ– ==============================

class AsyncFileManager:
    """å¼‚æ­¥æ–‡ä»¶ç®¡ç†å™¨"""

    def __init__(self, max_concurrent_ops: int = 20):
        self.max_concurrent_ops = max_concurrent_ops
        self.semaphore = asyncio.Semaphore(max_concurrent_ops)
        self.operation_stats = {
            'reads': 0,
            'writes': 0,
            'errors': 0,
            'total_bytes': 0
        }

    async def read_file(self, file_path: Path, encoding: str = 'utf-8') -> str:
        """å¼‚æ­¥è¯»å–æ–‡ä»¶"""
        async with self.semaphore:
            try:
                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒI/Oæ“ä½œ
                loop = asyncio.get_event_loop()

                def _read_sync():
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                        self.operation_stats['total_bytes'] += len(content.encode())
                        return content

                content = await loop.run_in_executor(None, _read_sync)
                self.operation_stats['reads'] += 1
                return content

            except Exception as e:
                self.operation_stats['errors'] += 1
                raise e

    async def write_file(self, file_path: Path, content: str,
                        encoding: str = 'utf-8', create_dirs: bool = True) -> None:
        """å¼‚æ­¥å†™å…¥æ–‡ä»¶"""
        async with self.semaphore:
            try:
                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒI/Oæ“ä½œ
                loop = asyncio.get_event_loop()

                def _write_sync():
                    if create_dirs:
                        file_path.parent.mkdir(parents=True, exist_ok=True)

                    with open(file_path, 'w', encoding=encoding) as f:
                        f.write(content)
                        self.operation_stats['total_bytes'] += len(content.encode())

                await loop.run_in_executor(None, _write_sync)
                self.operation_stats['writes'] += 1

            except Exception as e:
                self.operation_stats['errors'] += 1
                raise e

    async def batch_read(self, file_paths: List[Path]) -> Dict[Path, str]:
        """æ‰¹é‡è¯»å–æ–‡ä»¶"""
        tasks = [self.read_file(path) for path in file_paths]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        return {
            path: result for path, result in zip(file_paths, results)
            if not isinstance(result, Exception)
        }

    async def batch_write(self, file_data: Dict[Path, str]) -> None:
        """æ‰¹é‡å†™å…¥æ–‡ä»¶"""
        tasks = [
            self.write_file(path, content)
            for path, content in file_data.items()
        ]
        await asyncio.gather(*tasks, return_exceptions=True)

    def get_stats(self) -> Dict[str, Any]:
        """è·å–I/Oç»Ÿè®¡"""
        total_ops = self.operation_stats['reads'] + self.operation_stats['writes']
        error_rate = (
            self.operation_stats['errors'] / total_ops * 100
            if total_ops > 0 else 0
        )

        return {
            'operations': self.operation_stats,
            'error_rate': f"{error_rate:.2f}%",
            'avg_bytes_per_op': (
                self.operation_stats['total_bytes'] / total_ops
                if total_ops > 0 else 0
            )
        }

# ============================== æ­£åˆ™è¡¨è¾¾å¼ä¼˜åŒ– ==============================

class RegexOptimizer:
    """æ­£åˆ™è¡¨è¾¾å¼ä¼˜åŒ–å™¨"""

    def __init__(self, cache_size: int = 1000):
        self.compiled_patterns = LRUCache(max_size=cache_size, ttl=7200)  # 2å°æ—¶
        self.pattern_stats = defaultdict(lambda: {'uses': 0, 'total_time': 0.0})

    def compile_pattern(self, pattern: str, flags: int = 0):
        """ç¼–è¯‘å¹¶ç¼“å­˜æ­£åˆ™è¡¨è¾¾å¼"""
        import re

        cache_key = f"{pattern}:{flags}"

        # ä»ç¼“å­˜è·å–
        compiled = self.compiled_patterns.get(cache_key)
        if compiled:
            return compiled

        # ç¼–è¯‘å¹¶ç¼“å­˜
        start_time = time.perf_counter()
        compiled = re.compile(pattern, flags)
        compile_time = time.perf_counter() - start_time

        self.compiled_patterns.put(cache_key, compiled)

        # æ›´æ–°ç»Ÿè®¡
        stats = self.pattern_stats[pattern]
        stats['uses'] += 1
        stats['total_time'] += compile_time

        return compiled

    def find_all_optimized(self, pattern: str, text: str, flags: int = 0) -> List[str]:
        """ä¼˜åŒ–çš„findallæ“ä½œ"""
        compiled_pattern = self.compile_pattern(pattern, flags)

        start_time = time.perf_counter()
        matches = compiled_pattern.findall(text)
        search_time = time.perf_counter() - start_time

        # æ›´æ–°ç»Ÿè®¡
        stats = self.pattern_stats[pattern]
        stats['total_time'] += search_time

        return matches

    def get_pattern_stats(self) -> Dict[str, Any]:
        """è·å–æ¨¡å¼ç»Ÿè®¡"""
        return {
            'cache_stats': self.compiled_patterns.get_stats(),
            'pattern_usage': dict(self.pattern_stats),
            'top_patterns': sorted(
                self.pattern_stats.items(),
                key=lambda x: x[1]['uses'],
                reverse=True
            )[:10]
        }

# ============================== æ€§èƒ½ä¼˜åŒ–ç®¡ç†å™¨ ==============================

class PerformanceOptimizationManager:
    """æ€§èƒ½ä¼˜åŒ–ç®¡ç†å™¨"""

    def __init__(self):
        self.workflow_cache = WorkflowTemplateCache()
        self.thread_pool = SmartThreadPool()
        self.memory_manager = MemoryManager()
        self.file_manager = AsyncFileManager()
        self.regex_optimizer = RegexOptimizer()

        self.optimization_stats = {
            'workflow_cache_hits': 0,
            'workflow_cache_misses': 0,
            'parallel_tasks_executed': 0,
            'memory_cleanups': 0,
            'file_operations': 0,
            'regex_optimizations': 0
        }

        self._initialized = False

    async def initialize(self):
        """åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–"""
        if self._initialized:
            return

        try:
            # å¯åŠ¨çº¿ç¨‹æ± 
            await self.thread_pool.start()

            # å¯åŠ¨å†…å­˜ç®¡ç†
            self.memory_manager.start_monitoring()

            self._initialized = True
            log_info("Performance optimization manager initialized")

        except Exception as e:
            log_error(f"Failed to initialize performance optimization: {e}")
            raise

    async def shutdown(self):
        """å…³é—­æ€§èƒ½ä¼˜åŒ–"""
        if not self._initialized:
            return

        try:
            await self.thread_pool.shutdown()
            self._initialized = False
            log_info("Performance optimization manager shutdown")

        except Exception as e:
            log_error(f"Failed to shutdown performance optimization: {e}")

    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """è·å–ç»¼åˆæ€§èƒ½ç»Ÿè®¡"""
        return {
            'workflow_cache': self.workflow_cache.get_cache_stats(),
            'thread_pool': self.thread_pool.get_stats(),
            'memory_manager': self.memory_manager.get_stats(),
            'file_manager': self.file_manager.get_stats(),
            'regex_optimizer': self.regex_optimizer.get_pattern_stats(),
            'optimization_stats': self.optimization_stats
        }

    def generate_optimization_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        stats = self.get_comprehensive_stats()

        # è®¡ç®—æ”¹è¿›æŒ‡æ ‡
        improvements = {}

        # å·¥ä½œæµç¼“å­˜æ”¹è¿›
        workflow_cache_stats = stats['workflow_cache']['memory_cache']
        if 'hit_rate' in workflow_cache_stats:
            hit_rate = float(workflow_cache_stats['hit_rate'].rstrip('%'))
            improvements['workflow_loading'] = f"{hit_rate:.1f}% cache hit rate"

        # çº¿ç¨‹æ± æ•ˆç‡
        thread_stats = stats['thread_pool']
        if 'success_rate' in thread_stats:
            improvements['parallel_execution'] = f"{thread_stats['success_rate']:.1f}% success rate"

        # å†…å­˜ä½¿ç”¨ä¼˜åŒ–
        memory_stats = stats['memory_manager']
        pool_efficiency = sum(memory_stats['pool_stats'].values())
        if pool_efficiency > 0:
            improvements['memory_usage'] = f"{pool_efficiency} objects pooled"

        return {
            'timestamp': datetime.now().isoformat(),
            'overall_status': 'optimized',
            'active_optimizations': len([k for k, v in improvements.items() if v]),
            'improvements': improvements,
            'detailed_stats': stats,
            'recommendations': self._generate_further_optimizations(stats)
        }

    def _generate_further_optimizations(self, stats: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè¿›ä¸€æ­¥çš„ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åˆ†æç¼“å­˜å‘½ä¸­ç‡
        workflow_hit_rate = float(
            stats['workflow_cache']['memory_cache'].get('hit_rate', '0%').rstrip('%')
        )
        if workflow_hit_rate < 80:
            recommendations.append("å¢åŠ å·¥ä½œæµç¼“å­˜å¤§å°ä»¥æé«˜å‘½ä¸­ç‡")

        # åˆ†æçº¿ç¨‹æ± ä½¿ç”¨
        thread_stats = stats['thread_pool']
        if thread_stats['current_workers'] == thread_stats['max_workers']:
            recommendations.append("è€ƒè™‘å¢åŠ æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°")

        # åˆ†æå†…å­˜ä½¿ç”¨
        memory_stats = stats['memory_manager']
        if memory_stats['current_memory_mb'] > memory_stats['max_memory_mb'] * 0.9:
            recommendations.append("å¢åŠ å†…å­˜é™åˆ¶æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")

        # åˆ†æI/Oæ“ä½œ
        io_stats = stats['file_manager']
        error_rate = float(io_stats.get('error_rate', '0%').rstrip('%'))
        if error_rate > 1:
            recommendations.append("æ£€æŸ¥æ–‡ä»¶æ“ä½œé”™è¯¯åŸå› å¹¶ä¼˜åŒ–")

        return recommendations

# ============================== å…¨å±€å®ä¾‹ ==============================

# åˆ›å»ºå…¨å±€æ€§èƒ½ä¼˜åŒ–ç®¡ç†å™¨å®ä¾‹
performance_manager = PerformanceOptimizationManager()

# ä¾¿æ·å‡½æ•°
async def initialize_performance_optimizations():
    """åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–"""
    await performance_manager.initialize()

async def shutdown_performance_optimizations():
    """å…³é—­æ€§èƒ½ä¼˜åŒ–"""
    await performance_manager.shutdown()

def get_performance_stats():
    """è·å–æ€§èƒ½ç»Ÿè®¡"""
    return performance_manager.get_comprehensive_stats()

def get_optimization_report():
    """è·å–ä¼˜åŒ–æŠ¥å‘Š"""
    return performance_manager.generate_optimization_report()

# å¯¼å‡ºä¼˜åŒ–åçš„è£…é¥°å™¨å’Œå·¥å…·
__all__ = [
    'cached',
    'LRUCache',
    'WorkflowTemplateCache',
    'SmartThreadPool',
    'MemoryManager',
    'AsyncFileManager',
    'RegexOptimizer',
    'PerformanceOptimizationManager',
    'performance_manager',
    'initialize_performance_optimizations',
    'shutdown_performance_optimizations',
    'get_performance_stats',
    'get_optimization_report'
]

if __name__ == "__main__":
    import asyncio

    async def demo():
        """æ¼”ç¤ºæ€§èƒ½ä¼˜åŒ–åŠŸèƒ½"""
        print("ğŸš€ Perfect21 æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º")
        print("=" * 40)

        # åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–
        await initialize_performance_optimizations()

        # æ¼”ç¤ºç¼“å­˜åŠŸèƒ½
        @cached(max_size=100, ttl=60)
        def expensive_calculation(n):
            return sum(i * i for i in range(n))

        # æµ‹è¯•ç¼“å­˜
        start_time = time.perf_counter()
        result1 = expensive_calculation(10000)
        first_call_time = time.perf_counter() - start_time

        start_time = time.perf_counter()
        result2 = expensive_calculation(10000)  # åº”è¯¥ä»ç¼“å­˜è·å–
        second_call_time = time.perf_counter() - start_time

        print(f"é¦–æ¬¡è°ƒç”¨: {first_call_time:.4f}s")
        print(f"ç¼“å­˜è°ƒç”¨: {second_call_time:.4f}s")
        print(f"åŠ é€Ÿæ¯”: {first_call_time / second_call_time:.1f}x")

        # è·å–æ€§èƒ½ç»Ÿè®¡
        stats = get_performance_stats()
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"  çº¿ç¨‹æ± : {stats['thread_pool']['current_workers']} å·¥ä½œçº¿ç¨‹")
        print(f"  å†…å­˜ä½¿ç”¨: {stats['memory_manager']['current_memory_mb']:.1f}MB")

        # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        report = get_optimization_report()
        print(f"\nğŸ“ˆ ä¼˜åŒ–æŠ¥å‘Š:")
        print(f"  çŠ¶æ€: {report['overall_status']}")
        print(f"  æ´»è·ƒä¼˜åŒ–: {report['active_optimizations']} é¡¹")

        # å…³é—­æ€§èƒ½ä¼˜åŒ–
        await shutdown_performance_optimizations()

        print("\nâœ… æ¼”ç¤ºå®Œæˆ")

    asyncio.run(demo())