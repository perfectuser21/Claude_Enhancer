#!/usr/bin/env python3
"""
Perfect21æ€§èƒ½ä¼˜åŒ–Phase 1æµ‹è¯•
éªŒè¯LRUç¼“å­˜ã€é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ã€ä¼˜åŒ–agenté€‰æ‹©ç®—æ³•å’Œæ€§èƒ½ç›‘æ§çš„æ•ˆæœ
"""

import os
import sys
import time
import json
import asyncio
import logging
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_dynamic_workflow_generator_performance():
    """æµ‹è¯•åŠ¨æ€å·¥ä½œæµç”Ÿæˆå™¨æ€§èƒ½ä¼˜åŒ–"""
    print("\n" + "="*60)
    print("ğŸš€ æµ‹è¯•åŠ¨æ€å·¥ä½œæµç”Ÿæˆå™¨æ€§èƒ½ä¼˜åŒ–")
    print("="*60)

    try:
        from features.workflow_orchestrator.dynamic_workflow_generator import (
            dynamic_workflow_generator,
            get_workflow_performance_metrics,
            optimize_workflow_performance
        )

        # æµ‹è¯•æ•°æ®
        test_tasks = [
            "å®ç°ç”¨æˆ·è®¤è¯ç³»ç»Ÿï¼ŒåŒ…å«JWT tokenéªŒè¯ã€å¯†ç åŠ å¯†ã€è§’è‰²æƒé™ç®¡ç†",
            "å¼€å‘APIæ¥å£ï¼Œæ”¯æŒRESTfulè§„èŒƒï¼ŒåŒ…å«CRUDæ“ä½œå’Œæ•°æ®éªŒè¯",
            "åˆ›å»ºå‰ç«¯ç•Œé¢ï¼Œä½¿ç”¨Reactå’ŒTypeScriptï¼Œæ”¯æŒå“åº”å¼è®¾è®¡",
            "è®¾è®¡æ•°æ®åº“schemaï¼ŒåŒ…å«ç”¨æˆ·è¡¨ã€æƒé™è¡¨ã€æ—¥å¿—è¡¨çš„å…³è”å…³ç³»",
            "å®ç°å•å…ƒæµ‹è¯•ï¼Œè¦†ç›–ç‡è¦æ±‚90%ä»¥ä¸Šï¼ŒåŒ…å«è¾¹ç•Œæ¡ä»¶æµ‹è¯•",
            "é…ç½®CI/CDæµæ°´çº¿ï¼Œæ”¯æŒè‡ªåŠ¨æµ‹è¯•ã€æ„å»ºå’Œéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ",
            "ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ï¼ŒåŒ…å«æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–ã€ç¼“å­˜ç­–ç•¥ã€å¹¶å‘å¤„ç†",
            "å®ç°å®‰å…¨åŠŸèƒ½ï¼ŒåŒ…å«XSSé˜²æŠ¤ã€SQLæ³¨å…¥é˜²æŠ¤ã€æ•°æ®åŠ å¯†ä¼ è¾“",
            "åˆ›å»ºç›‘æ§ç³»ç»Ÿï¼ŒåŒ…å«æ€§èƒ½æŒ‡æ ‡æ”¶é›†ã€å‘Šè­¦é€šçŸ¥ã€æ—¥å¿—èšåˆåˆ†æ",
            "ç¼–å†™æŠ€æœ¯æ–‡æ¡£ï¼ŒåŒ…å«APIæ–‡æ¡£ã€éƒ¨ç½²æŒ‡å—ã€ç”¨æˆ·æ“ä½œæ‰‹å†Œ"
        ]

        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        print("\nğŸ“Š æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")

        total_start_time = time.perf_counter()
        results = []

        for i, task in enumerate(test_tasks, 1):
            start_time = time.perf_counter()

            # ç”Ÿæˆå·¥ä½œæµ
            workflow = dynamic_workflow_generator.generate_workflow(task)

            execution_time = time.perf_counter() - start_time
            results.append({
                'task_id': i,
                'execution_time': execution_time,
                'workflow_stages': len(workflow.get('stages', [])),
                'total_agents': workflow.get('execution_metadata', {}).get('total_agents', 0)
            })

            print(f"   Task {i}: {execution_time:.3f}s - {len(workflow.get('stages', []))} stages")

        total_time = time.perf_counter() - total_start_time
        avg_time = total_time / len(test_tasks)

        print(f"\nğŸ“ˆ åŸºå‡†æµ‹è¯•ç»“æœ:")
        print(f"   æ€»æ‰§è¡Œæ—¶é—´: {total_time:.3f}s")
        print(f"   å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time:.3f}s")
        print(f"   æœ€å¿«æ‰§è¡Œ: {min(r['execution_time'] for r in results):.3f}s")
        print(f"   æœ€æ…¢æ‰§è¡Œ: {max(r['execution_time'] for r in results):.3f}s")

        # è·å–æ€§èƒ½æŒ‡æ ‡
        performance_metrics = get_workflow_performance_metrics()

        print(f"\nğŸ” ç¼“å­˜æ€§èƒ½:")
        cache_stats = performance_metrics.get('cache_stats', {})
        for cache_name, stats in cache_stats.items():
            print(f"   {cache_name}: {stats.get('hit_rate', 'N/A')} å‘½ä¸­ç‡")

        print(f"\nğŸ§  Agenté€‰æ‹©å™¨æ€§èƒ½:")
        agent_stats = performance_metrics.get('agent_selector_stats', {})
        selection_stats = agent_stats.get('selection_stats', {})
        print(f"   æ€»é€‰æ‹©æ¬¡æ•°: {selection_stats.get('total_selections', 0)}")
        print(f"   ç¼“å­˜å‘½ä¸­: {selection_stats.get('cache_hits', 0)}")
        print(f"   å¿«é€Ÿè·¯å¾„: {selection_stats.get('fast_path_selections', 0)}")

        print(f"\nğŸ“ æ­£åˆ™è¡¨è¾¾å¼ä½¿ç”¨ç»Ÿè®¡:")
        regex_stats = performance_metrics.get('regex_stats', {})
        for pattern, count in regex_stats.items():
            print(f"   {pattern}: {count} æ¬¡ä½¿ç”¨")

        # ä¼˜åŒ–å»ºè®®
        optimization = optimize_workflow_performance()
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        print(f"   çŠ¶æ€: {optimization.get('optimization_status', 'unknown')}")
        for rec in optimization.get('recommendations', []):
            print(f"   - {rec}")

        return {
            'success': True,
            'benchmark_results': results,
            'total_time': total_time,
            'avg_time': avg_time,
            'performance_metrics': performance_metrics,
            'optimization': optimization
        }

    except Exception as e:
        logger.error(f"åŠ¨æ€å·¥ä½œæµç”Ÿæˆå™¨æµ‹è¯•å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}

def test_performance_optimizer():
    """æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨åŠŸèƒ½"""
    print("\n" + "="*60)
    print("âš¡ æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨åŠŸèƒ½")
    print("="*60)

    try:
        from modules.performance_optimizer import (
            performance_optimizer,
            analyze_performance,
            optimize_performance,
            get_optimization_report
        )

        # ç³»ç»Ÿæ€§èƒ½åˆ†æ
        print("\nğŸ” æ‰§è¡Œç³»ç»Ÿæ€§èƒ½åˆ†æ...")
        analysis_start = time.perf_counter()
        analysis = analyze_performance()
        analysis_time = time.perf_counter() - analysis_start

        print(f"   åˆ†æç”¨æ—¶: {analysis_time:.3f}s")
        print(f"   ç³»ç»Ÿå¥åº·åˆ†æ•°: {analysis.get('health_score', 0):.1f}/100")
        print(f"   å‘ç°ç“¶é¢ˆ: {len(analysis.get('bottlenecks', []))} ä¸ª")
        print(f"   ä¼˜åŒ–å»ºè®®: {len(analysis.get('recommendations', []))} æ¡")

        # æ˜¾ç¤ºç“¶é¢ˆä¿¡æ¯
        bottlenecks = analysis.get('bottlenecks', [])
        if bottlenecks:
            print(f"\nğŸš¨ ç³»ç»Ÿç“¶é¢ˆ:")
            for bottleneck in bottlenecks:
                print(f"   {bottleneck['type']} ({bottleneck['severity']}): {bottleneck['impact_description']}")

        # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
        recommendations = analysis.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for rec in recommendations[:5]:  # æ˜¾ç¤ºå‰5ä¸ªå»ºè®®
                print(f"   [{rec['priority']}] {rec['title']}")
                print(f"       {rec['description']}")

        # åº”ç”¨ä¼˜åŒ–
        print(f"\nğŸ› ï¸ åº”ç”¨æ€§èƒ½ä¼˜åŒ–...")
        optimization_start = time.perf_counter()
        optimization_result = optimize_performance(['cache', 'memory'])
        optimization_time = time.perf_counter() - optimization_start

        print(f"   ä¼˜åŒ–ç”¨æ—¶: {optimization_time:.3f}s")
        print(f"   ä¼˜åŒ–æˆåŠŸ: {optimization_result.get('success', False)}")

        if optimization_result.get('success'):
            results = optimization_result.get('results', {})
            for opt_type, result in results.items():
                print(f"   {opt_type}ä¼˜åŒ–: å®Œæˆ")

        # è·å–ä¼˜åŒ–æŠ¥å‘Š
        print(f"\nğŸ“Š ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š...")
        report = get_optimization_report()

        optimization_stats = report.get('optimization_stats', {})
        print(f"   å†å²ä¼˜åŒ–æ¬¡æ•°: {optimization_stats.get('total_optimizations', 0)}")
        print(f"   å¹³å‡ä¼˜åŒ–æ—¶é—´: {optimization_stats.get('avg_optimization_time', 0):.3f}s")

        return {
            'success': True,
            'analysis_time': analysis_time,
            'analysis_result': analysis,
            'optimization_time': optimization_time,
            'optimization_result': optimization_result,
            'report': report
        }

    except Exception as e:
        logger.error(f"æ€§èƒ½ä¼˜åŒ–å™¨æµ‹è¯•å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}

def test_performance_monitor_integration():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§é›†æˆ"""
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æ€§èƒ½ç›‘æ§é›†æˆ")
    print("="*60)

    try:
        from modules.performance_monitor import (
            performance_monitor,
            get_performance_summary
        )

        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        print("\nğŸ¯ å¯åŠ¨æ€§èƒ½ç›‘æ§...")
        performance_monitor.start_monitoring()

        # ç­‰å¾…æ”¶é›†ä¸€äº›æ•°æ®
        print("   æ”¶é›†æ€§èƒ½æ•°æ®...")
        time.sleep(3)

        # è·å–æ€§èƒ½æ‘˜è¦
        summary = get_performance_summary()

        print(f"   å¥åº·åˆ†æ•°: {summary.get('health_score', 0):.1f}/100")
        print(f"   ç›‘æ§æŒ‡æ ‡: {summary.get('total_metrics', 0)} ä¸ª")
        print(f"   æ´»è·ƒå‘Šè­¦: {summary.get('active_alerts', 0)} ä¸ª")
        print(f"   ä¸¥é‡å‘Šè­¦: {summary.get('critical_alerts', 0)} ä¸ª")

        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        key_metrics = summary.get('key_metrics', {})
        if key_metrics:
            print(f"\nğŸ“ˆ å…³é”®æŒ‡æ ‡:")
            for metric_name, metric_data in key_metrics.items():
                status = metric_data.get('status', 'unknown')
                value = metric_data.get('value', 0)
                unit = metric_data.get('unit', '')
                print(f"   {metric_name}: {value}{unit} ({status})")

        # åœæ­¢ç›‘æ§
        performance_monitor.stop_monitoring()

        return {
            'success': True,
            'summary': summary,
            'monitoring_time': 3
        }

    except Exception as e:
        logger.error(f"æ€§èƒ½ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}

def test_lru_cache_effectiveness():
    """æµ‹è¯•LRUç¼“å­˜æ•ˆæœ"""
    print("\n" + "="*60)
    print("ğŸ—„ï¸ æµ‹è¯•LRUç¼“å­˜æ•ˆæœ")
    print("="*60)

    try:
        from features.workflow_orchestrator.dynamic_workflow_generator import LRUCache

        # åˆ›å»ºæµ‹è¯•ç¼“å­˜
        cache = LRUCache(max_size=5)

        print("\nğŸ§ª LRUç¼“å­˜åŸºæœ¬åŠŸèƒ½æµ‹è¯•:")

        # æµ‹è¯•åŸºæœ¬æ“ä½œ
        cache.put("key1", "value1")
        cache.put("key2", "value2")
        cache.put("key3", "value3")

        # æµ‹è¯•å‘½ä¸­
        result1 = cache.get("key1")
        result2 = cache.get("key2")
        result_miss = cache.get("nonexistent")

        print(f"   ç¼“å­˜å‘½ä¸­æµ‹è¯•: {'âœ“' if result1 == 'value1' else 'âœ—'}")
        print(f"   ç¼“å­˜ä¸¢å¤±æµ‹è¯•: {'âœ“' if result_miss is None else 'âœ—'}")

        # æµ‹è¯•LRUæ·˜æ±°
        cache.put("key4", "value4")
        cache.put("key5", "value5")
        cache.put("key6", "value6")  # åº”è¯¥æ·˜æ±°key3ï¼ˆæœ€å°‘ä½¿ç”¨ï¼‰

        evicted = cache.get("key3")  # åº”è¯¥è¿”å›None
        retained = cache.get("key1")  # åº”è¯¥ä»ç„¶å­˜åœ¨

        print(f"   LRUæ·˜æ±°æµ‹è¯•: {'âœ“' if evicted is None and retained == 'value1' else 'âœ—'}")

        # è·å–ç¼“å­˜ç»Ÿè®¡
        stats = cache.get_stats()
        print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
        print(f"   å½“å‰å¤§å°: {stats['size']}/{stats['max_size']}")
        print(f"   å‘½ä¸­ç‡: {stats['hit_rate']}")
        print(f"   åˆ©ç”¨ç‡: {stats['utilization']}")

        # æ€§èƒ½æµ‹è¯•
        print(f"\nâš¡ ç¼“å­˜æ€§èƒ½æµ‹è¯•:")

        # é¢„å¡«å……ç¼“å­˜
        perf_cache = LRUCache(max_size=1000)
        for i in range(500):
            perf_cache.put(f"key_{i}", f"value_{i}")

        # æµ‹è¯•ç¼“å­˜å‘½ä¸­æ€§èƒ½
        start_time = time.perf_counter()
        for i in range(100):
            perf_cache.get(f"key_{i % 500}")
        hit_time = time.perf_counter() - start_time

        # æµ‹è¯•ç¼“å­˜ä¸¢å¤±æ€§èƒ½
        start_time = time.perf_counter()
        for i in range(100):
            perf_cache.get(f"miss_key_{i}")
        miss_time = time.perf_counter() - start_time

        print(f"   100æ¬¡å‘½ä¸­ç”¨æ—¶: {hit_time:.6f}s")
        print(f"   100æ¬¡ä¸¢å¤±ç”¨æ—¶: {miss_time:.6f}s")
        print(f"   å¹³å‡å‘½ä¸­æ—¶é—´: {hit_time/100*1000:.3f}Î¼s")

        return {
            'success': True,
            'basic_tests_passed': True,
            'lru_eviction_works': evicted is None and retained == 'value1',
            'cache_stats': stats,
            'performance': {
                'hit_time_100': hit_time,
                'miss_time_100': miss_time,
                'avg_hit_time_us': hit_time/100*1000000
            }
        }

    except Exception as e:
        logger.error(f"LRUç¼“å­˜æµ‹è¯•å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}

def test_regex_precompilation():
    """æµ‹è¯•æ­£åˆ™è¡¨è¾¾å¼é¢„ç¼–è¯‘æ•ˆæœ"""
    print("\n" + "="*60)
    print("ğŸ” æµ‹è¯•æ­£åˆ™è¡¨è¾¾å¼é¢„ç¼–è¯‘æ•ˆæœ")
    print("="*60)

    try:
        from features.workflow_orchestrator.dynamic_workflow_generator import PrecompiledRegexManager
        import re

        regex_manager = PrecompiledRegexManager()

        # æµ‹è¯•æ–‡æœ¬
        test_text = """
        å®ç°ç”¨æˆ·è®¤è¯ç³»ç»Ÿï¼Œéœ€è¦@backend-architectå’Œ@security-auditoråä½œã€‚
        è¿™æ˜¯ä¸€ä¸ªhigh priorityçš„complexä»»åŠ¡ï¼Œéœ€è¦pythonã€javascriptã€databaseæŠ€èƒ½ã€‚
        é¢„è®¡éœ€è¦2 hourså®Œæˆï¼Œdepends on APIè®¾è®¡ã€‚
        å¯ä»¥parallelæ‰§è¡Œå‰ç«¯å’Œåç«¯å¼€å‘ã€‚
        """

        print("\nğŸ§ª é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼åŠŸèƒ½æµ‹è¯•:")

        # æµ‹è¯•å„ç§æ¨¡å¼
        agent_matches = regex_manager.findall('agent_name', test_text)
        complexity_matches = regex_manager.findall('task_complexity', test_text)
        skill_matches = regex_manager.findall('skill_keywords', test_text)
        time_matches = regex_manager.findall('time_estimates', test_text)

        print(f"   AgentåŒ¹é…: {agent_matches} {'âœ“' if 'backend-architect' in agent_matches else 'âœ—'}")
        print(f"   å¤æ‚åº¦åŒ¹é…: {complexity_matches} {'âœ“' if 'complex' in complexity_matches else 'âœ—'}")
        print(f"   æŠ€èƒ½åŒ¹é…: {skill_matches} {'âœ“' if 'python' in skill_matches else 'âœ—'}")
        print(f"   æ—¶é—´åŒ¹é…: {time_matches} {'âœ“' if '2' in time_matches else 'âœ—'}")

        # æ€§èƒ½å¯¹æ¯”æµ‹è¯•
        print(f"\nâš¡ æ€§èƒ½å¯¹æ¯”æµ‹è¯•:")

        # é¢„ç¼–è¯‘ç‰ˆæœ¬æ€§èƒ½
        start_time = time.perf_counter()
        for _ in range(1000):
            regex_manager.findall('agent_name', test_text)
            regex_manager.findall('skill_keywords', test_text)
        precompiled_time = time.perf_counter() - start_time

        # å³æ—¶ç¼–è¯‘ç‰ˆæœ¬æ€§èƒ½
        agent_pattern = re.compile(r'@([a-zA-Z0-9\-_]+)', re.IGNORECASE)
        skill_pattern = re.compile(r'\b(?:python|javascript|react|vue|node|docker|kubernetes|aws|database|sql|nosql|redis|mongodb)\b', re.IGNORECASE)

        start_time = time.perf_counter()
        for _ in range(1000):
            agent_pattern.findall(test_text)
            skill_pattern.findall(test_text)
        manual_time = time.perf_counter() - start_time

        # æ¯æ¬¡ç¼–è¯‘ç‰ˆæœ¬æ€§èƒ½ï¼ˆæœ€æ…¢ï¼‰
        start_time = time.perf_counter()
        for _ in range(1000):
            re.findall(r'@([a-zA-Z0-9\-_]+)', test_text, re.IGNORECASE)
            re.findall(r'\b(?:python|javascript|react|vue|node|docker|kubernetes|aws|database|sql|nosql|redis|mongodb)\b', test_text, re.IGNORECASE)
        runtime_compile_time = time.perf_counter() - start_time

        print(f"   é¢„ç¼–è¯‘ç®¡ç†å™¨: {precompiled_time:.6f}s")
        print(f"   æ‰‹åŠ¨é¢„ç¼–è¯‘: {manual_time:.6f}s")
        print(f"   è¿è¡Œæ—¶ç¼–è¯‘: {runtime_compile_time:.6f}s")

        speedup_vs_runtime = runtime_compile_time / precompiled_time
        print(f"   é¢„ç¼–è¯‘åŠ é€Ÿæ¯”: {speedup_vs_runtime:.2f}x")

        # è·å–ä½¿ç”¨ç»Ÿè®¡
        usage_stats = regex_manager.get_stats()
        print(f"\nğŸ“Š ä½¿ç”¨ç»Ÿè®¡:")
        for pattern, count in usage_stats.items():
            print(f"   {pattern}: {count} æ¬¡")

        return {
            'success': True,
            'pattern_tests_passed': all([
                'backend-architect' in agent_matches,
                'complex' in complexity_matches,
                'python' in skill_matches,
                '2' in time_matches
            ]),
            'performance': {
                'precompiled_time': precompiled_time,
                'manual_time': manual_time,
                'runtime_compile_time': runtime_compile_time,
                'speedup_vs_runtime': speedup_vs_runtime
            },
            'usage_stats': usage_stats
        }

    except Exception as e:
        logger.error(f"æ­£åˆ™è¡¨è¾¾å¼é¢„ç¼–è¯‘æµ‹è¯•å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}

def test_agent_selection_optimization():
    """æµ‹è¯•Agenté€‰æ‹©ç®—æ³•ä¼˜åŒ–"""
    print("\n" + "="*60)
    print("ğŸ¯ æµ‹è¯•Agenté€‰æ‹©ç®—æ³•ä¼˜åŒ–")
    print("="*60)

    try:
        from features.workflow_orchestrator.dynamic_workflow_generator import (
            OptimizedAgentSelector, AgentCapability, TaskRequirement
        )

        selector = OptimizedAgentSelector()

        print("\nğŸ§ª Agenté€‰æ‹©å™¨åŠŸèƒ½æµ‹è¯•:")

        # æ·»åŠ æµ‹è¯•agents
        test_agents = [
            AgentCapability("backend-dev", "technical", ["python", "database", "api"], 8.0),
            AgentCapability("frontend-dev", "technical", ["javascript", "react", "css"], 7.5),
            AgentCapability("devops-eng", "infrastructure", ["docker", "kubernetes", "aws"], 8.5),
            AgentCapability("qa-tester", "quality", ["testing", "automation"], 7.0),
            AgentCapability("security-expert", "security", ["security", "audit"], 9.0)
        ]

        for agent in test_agents:
            selector.add_agent(agent)

        # æµ‹è¯•é€‰æ‹©é€»è¾‘
        task_req = TaskRequirement(
            description="å®ç°APIæ¥å£",
            domain="technical",
            complexity=7.0,
            required_skills=["python", "api", "database"]
        )

        selected = selector.select_agents(task_req, count=2)
        print(f"   é€‰æ‹©ç»“æœ: {selected}")
        print(f"   é€‰æ‹©æ•°é‡: {len(selected)} {'âœ“' if len(selected) == 2 else 'âœ—'}")

        # éªŒè¯é€‰æ‹©çš„åˆç†æ€§
        expected_agents = ["backend-dev"]  # åº”è¯¥ä¼˜å…ˆé€‰æ‹©backend-dev
        reasonable = any(agent in selected for agent in expected_agents)
        print(f"   é€‰æ‹©åˆç†æ€§: {'âœ“' if reasonable else 'âœ—'}")

        # æ€§èƒ½æµ‹è¯•
        print(f"\nâš¡ é€‰æ‹©å™¨æ€§èƒ½æµ‹è¯•:")

        # å¤§é‡agentæµ‹è¯•
        large_selector = OptimizedAgentSelector()
        for i in range(100):
            agent = AgentCapability(
                f"agent_{i}",
                f"domain_{i % 5}",
                [f"skill_{j}" for j in range(i % 5 + 1)],
                float(i % 10 + 1)
            )
            large_selector.add_agent(agent)

        # æµ‹è¯•é€‰æ‹©æ€§èƒ½
        start_time = time.perf_counter()
        for _ in range(100):
            large_selector.select_agents(task_req, count=3)
        selection_time = time.perf_counter() - start_time

        print(f"   100æ¬¡é€‰æ‹©ç”¨æ—¶: {selection_time:.6f}s")
        print(f"   å¹³å‡é€‰æ‹©æ—¶é—´: {selection_time/100*1000:.3f}ms")

        # è·å–é€‰æ‹©å™¨ç»Ÿè®¡
        stats = selector.get_stats()
        print(f"\nğŸ“Š é€‰æ‹©å™¨ç»Ÿè®¡:")
        print(f"   æ€»agents: {stats.get('total_agents', 0)}")
        print(f"   å¯ç”¨åŸŸ: {len(stats.get('domains', []))}")
        print(f"   å¯ç”¨æŠ€èƒ½: {len(stats.get('skills', []))}")

        selection_stats = stats.get('selection_stats', {})
        print(f"   é€‰æ‹©ç»Ÿè®¡:")
        print(f"     æ€»é€‰æ‹©: {selection_stats.get('total_selections', 0)}")
        print(f"     ç¼“å­˜å‘½ä¸­: {selection_stats.get('cache_hits', 0)}")
        print(f"     å¿«é€Ÿè·¯å¾„: {selection_stats.get('fast_path_selections', 0)}")

        return {
            'success': True,
            'selection_works': len(selected) == 2,
            'selection_reasonable': reasonable,
            'performance': {
                'selection_time_100': selection_time,
                'avg_selection_time_ms': selection_time/100*1000
            },
            'stats': stats
        }

    except Exception as e:
        logger.error(f"Agenté€‰æ‹©ç®—æ³•æµ‹è¯•å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}

def generate_performance_report(test_results: Dict[str, Any]):
    """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“‹ Perfect21æ€§èƒ½ä¼˜åŒ–Phase 1æµ‹è¯•æŠ¥å‘Š")
    print("="*60)

    report = {
        'test_timestamp': datetime.now().isoformat(),
        'test_results': test_results,
        'overall_success': all(result.get('success', False) for result in test_results.values()),
        'summary': {}
    }

    # ç»Ÿè®¡æµ‹è¯•ç»“æœ
    total_tests = len(test_results)
    passed_tests = sum(1 for result in test_results.values() if result.get('success', False))

    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   æµ‹è¯•æ¨¡å—: {total_tests}")
    print(f"   é€šè¿‡æµ‹è¯•: {passed_tests}")
    print(f"   æµ‹è¯•é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%")

    # æ€§èƒ½æ”¹è¿›æ€»ç»“
    print(f"\nğŸš€ æ€§èƒ½æ”¹è¿›æ€»ç»“:")

    # å·¥ä½œæµç”Ÿæˆå™¨æ€§èƒ½
    if 'workflow_generator' in test_results and test_results['workflow_generator'].get('success'):
        wf_result = test_results['workflow_generator']
        avg_time = wf_result.get('avg_time', 0)
        print(f"   å·¥ä½œæµç”Ÿæˆå¹³å‡æ—¶é—´: {avg_time:.3f}s")

        cache_stats = wf_result.get('performance_metrics', {}).get('cache_stats', {})
        for cache_name, stats in cache_stats.items():
            hit_rate = stats.get('hit_rate', 'N/A')
            print(f"   {cache_name}ç¼“å­˜å‘½ä¸­ç‡: {hit_rate}")

    # LRUç¼“å­˜æ€§èƒ½
    if 'lru_cache' in test_results and test_results['lru_cache'].get('success'):
        lru_result = test_results['lru_cache']
        performance = lru_result.get('performance', {})
        avg_hit_time = performance.get('avg_hit_time_us', 0)
        print(f"   LRUç¼“å­˜å¹³å‡å‘½ä¸­æ—¶é—´: {avg_hit_time:.3f}Î¼s")

    # æ­£åˆ™è¡¨è¾¾å¼æ€§èƒ½
    if 'regex_precompilation' in test_results and test_results['regex_precompilation'].get('success'):
        regex_result = test_results['regex_precompilation']
        performance = regex_result.get('performance', {})
        speedup = performance.get('speedup_vs_runtime', 1)
        print(f"   æ­£åˆ™è¡¨è¾¾å¼é¢„ç¼–è¯‘åŠ é€Ÿæ¯”: {speedup:.2f}x")

    # Agenté€‰æ‹©æ€§èƒ½
    if 'agent_selection' in test_results and test_results['agent_selection'].get('success'):
        agent_result = test_results['agent_selection']
        performance = agent_result.get('performance', {})
        avg_selection_time = performance.get('avg_selection_time_ms', 0)
        print(f"   Agenté€‰æ‹©å¹³å‡æ—¶é—´: {avg_selection_time:.3f}ms")

    # ç³»ç»Ÿå¥åº·åˆ†æ•°
    if 'performance_optimizer' in test_results and test_results['performance_optimizer'].get('success'):
        opt_result = test_results['performance_optimizer']
        analysis = opt_result.get('analysis_result', {})
        health_score = analysis.get('health_score', 0)
        print(f"   ç³»ç»Ÿå¥åº·åˆ†æ•°: {health_score:.1f}/100")

    # ä¿å­˜æŠ¥å‘Š
    report_file = f"performance_phase1_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    try:
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        print(f"\nğŸ’¾ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    except Exception as e:
        print(f"\nâŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

    # å»ºè®®
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    if passed_tests == total_tests:
        print("   âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œæ€§èƒ½ä¼˜åŒ–Phase 1å®æ–½æˆåŠŸ")
        print("   ğŸ¯ å»ºè®®ç»§ç»­å®æ–½Phase 2ä¼˜åŒ–")
    else:
        failed_tests = [name for name, result in test_results.items() if not result.get('success')]
        print(f"   âŒ ä»¥ä¸‹æµ‹è¯•å¤±è´¥: {', '.join(failed_tests)}")
        print("   ğŸ”§ å»ºè®®æ£€æŸ¥ç›¸å…³ç»„ä»¶å¹¶ä¿®å¤é—®é¢˜")

    print(f"\nğŸ“ˆ å…³é”®æ€§èƒ½æŒ‡æ ‡:")
    if 'workflow_generator' in test_results:
        wf_metrics = test_results['workflow_generator'].get('performance_metrics', {})
        method_metrics = wf_metrics.get('method_metrics', {})
        for method, metrics in method_metrics.items():
            exec_time = metrics.get('execution_time', 0)
            call_count = metrics.get('call_count', 0)
            print(f"   {method}: {exec_time:.3f}s/è°ƒç”¨ ({call_count}æ¬¡è°ƒç”¨)")

    return report

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ Perfect21æ€§èƒ½ä¼˜åŒ–Phase 1æµ‹è¯•")
    print("æµ‹è¯•é¡¹ç›®:")
    print("1. åŠ¨æ€å·¥ä½œæµç”Ÿæˆå™¨ - LRUç¼“å­˜å’Œç®—æ³•ä¼˜åŒ–")
    print("2. æ€§èƒ½ä¼˜åŒ–å™¨ - æ™ºèƒ½åˆ†æå’Œè‡ªåŠ¨ä¼˜åŒ–")
    print("3. æ€§èƒ½ç›‘æ§ - å®æ—¶ç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†")
    print("4. LRUç¼“å­˜ - é«˜æ•ˆç¼“å­˜ç®¡ç†")
    print("5. æ­£åˆ™è¡¨è¾¾å¼é¢„ç¼–è¯‘ - æ¨¡å¼åŒ¹é…ä¼˜åŒ–")
    print("6. Agenté€‰æ‹©ç®—æ³• - O(log n)å¤æ‚åº¦ä¼˜åŒ–")

    test_results = {}

    # æ‰§è¡Œå„é¡¹æµ‹è¯•
    test_results['workflow_generator'] = test_dynamic_workflow_generator_performance()
    test_results['performance_optimizer'] = test_performance_optimizer()
    test_results['performance_monitor'] = test_performance_monitor_integration()
    test_results['lru_cache'] = test_lru_cache_effectiveness()
    test_results['regex_precompilation'] = test_regex_precompilation()
    test_results['agent_selection'] = test_agent_selection_optimization()

    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    report = generate_performance_report(test_results)

    return report

if __name__ == "__main__":
    try:
        report = main()
        exit_code = 0 if report.get('overall_success', False) else 1
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)