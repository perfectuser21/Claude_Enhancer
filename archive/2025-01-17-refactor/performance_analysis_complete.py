#!/usr/bin/env python3
"""
Perfect21 å®Œæ•´æ€§èƒ½åˆ†æå·¥å…·
ç»¼åˆåˆ†æç³»ç»Ÿçš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬åŸºå‡†æµ‹è¯•ã€èµ„æºä½¿ç”¨åˆ†æå’Œç“¶é¢ˆè¯†åˆ«
"""

import os
import sys
import time
import psutil
import json
import asyncio
import threading
import concurrent.futures
import statistics
import gc
import tracemalloc
import cProfile
import pstats
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
import logging
import re
import subprocess

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__)))

# å¯¼å…¥Perfect21æ¨¡å—
from modules.logger import log_info, log_error, log_warning
from modules.performance_monitor import performance_monitor, PerformanceMetric
from modules.performance_optimizer import performance_optimizer
from features.workflow_orchestrator.orchestrator import WorkflowOrchestrator

logger = logging.getLogger(__name__)

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    test_name: str
    execution_time: float
    memory_usage: float
    cpu_usage: float
    throughput: float
    error_count: int
    success_rate: float
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class PerformanceProfile:
    """æ€§èƒ½å‰–æç»“æœ"""
    component: str
    total_time: float
    function_calls: int
    memory_peak: float
    hot_spots: List[Dict[str, Any]] = field(default_factory=list)
    bottlenecks: List[Dict[str, Any]] = field(default_factory=list)

@dataclass
class ResourceUsage:
    """èµ„æºä½¿ç”¨æƒ…å†µ"""
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    memory_rss: float
    memory_vms: float
    disk_usage: float
    network_io: Dict[str, int]
    file_descriptors: int
    threads: int

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.results: Dict[str, Any] = {}
        self.resource_samples: List[ResourceUsage] = []
        self.benchmark_results: List[BenchmarkResult] = []
        self.profiling_data: Dict[str, PerformanceProfile] = {}

        # æ€§èƒ½ç›‘æ§é…ç½®
        self.sampling_interval = 0.1  # 100ms
        self.monitoring_duration = 60  # 60ç§’

        # åŸºå‡†æµ‹è¯•é…ç½®
        self.benchmark_configs = {
            'workflow_generation': {
                'iterations': 100,
                'timeout': 30
            },
            'agent_selection': {
                'iterations': 1000,
                'timeout': 10
            },
            'parallel_execution': {
                'workers': 4,
                'tasks': 50,
                'timeout': 60
            },
            'file_operations': {
                'file_count': 100,
                'file_size': 1024,  # 1KB
                'timeout': 30
            },
            'regex_performance': {
                'patterns': 50,
                'text_size': 10000,
                'timeout': 15
            }
        }

        log_info("æ€§èƒ½åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    def run_complete_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´æ€§èƒ½åˆ†æ"""
        analysis_start = time.perf_counter()

        log_info("å¼€å§‹å®Œæ•´æ€§èƒ½åˆ†æ")

        try:
            # 1. åŸºå‡†æµ‹è¯•
            log_info("æ‰§è¡ŒåŸºå‡†æµ‹è¯•...")
            benchmark_results = self._run_all_benchmarks()

            # 2. èµ„æºä½¿ç”¨åˆ†æ
            log_info("è¿›è¡Œèµ„æºä½¿ç”¨åˆ†æ...")
            resource_analysis = self._analyze_resource_usage()

            # 3. æ€§èƒ½å‰–æ
            log_info("æ‰§è¡Œæ€§èƒ½å‰–æ...")
            profiling_results = self._run_performance_profiling()

            # 4. ç“¶é¢ˆè¯†åˆ«
            log_info("è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ...")
            bottleneck_analysis = self._identify_bottlenecks()

            # 5. ä¼˜åŒ–å»ºè®®
            log_info("ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
            optimization_recommendations = self._generate_optimization_recommendations()

            # 6. æ•´ä½“è¯„åˆ†
            performance_score = self._calculate_performance_score()

            analysis_time = time.perf_counter() - analysis_start

            self.results = {
                'analysis_metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'analysis_duration': analysis_time,
                    'system_info': self._get_system_info()
                },
                'performance_score': performance_score,
                'benchmark_results': benchmark_results,
                'resource_analysis': resource_analysis,
                'profiling_results': profiling_results,
                'bottleneck_analysis': bottleneck_analysis,
                'optimization_recommendations': optimization_recommendations
            }

            # ä¿å­˜ç»“æœ
            self._save_results()

            log_info(f"æ€§èƒ½åˆ†æå®Œæˆï¼Œæ€»ç”¨æ—¶: {analysis_time:.2f}ç§’")
            return self.results

        except Exception as e:
            log_error(f"æ€§èƒ½åˆ†æå¤±è´¥: {e}")
            return {'error': str(e), 'timestamp': datetime.now().isoformat()}

    def _run_all_benchmarks(self) -> Dict[str, BenchmarkResult]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        benchmark_results = {}

        # 1. å·¥ä½œæµç”Ÿæˆæ€§èƒ½æµ‹è¯•
        benchmark_results['workflow_generation'] = self._benchmark_workflow_generation()

        # 2. Agenté€‰æ‹©é€Ÿåº¦æµ‹è¯•
        benchmark_results['agent_selection'] = self._benchmark_agent_selection()

        # 3. å¹¶è¡Œæ‰§è¡Œæ•ˆç‡æµ‹è¯•
        benchmark_results['parallel_execution'] = self._benchmark_parallel_execution()

        # 4. æ–‡ä»¶æ“ä½œæ€§èƒ½æµ‹è¯•
        benchmark_results['file_operations'] = self._benchmark_file_operations()

        # 5. æ­£åˆ™è¡¨è¾¾å¼æ€§èƒ½æµ‹è¯•
        benchmark_results['regex_performance'] = self._benchmark_regex_performance()

        return benchmark_results

    def _benchmark_workflow_generation(self) -> BenchmarkResult:
        """å·¥ä½œæµç”Ÿæˆæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        config = self.benchmark_configs['workflow_generation']
        iterations = config['iterations']

        execution_times = []
        memory_usage = []
        errors = 0

        # ç›‘æ§èµ„æºä½¿ç”¨
        process = psutil.Process()

        for i in range(iterations):
            try:
                # è®°å½•å¼€å§‹çŠ¶æ€
                start_time = time.perf_counter()
                start_memory = process.memory_info().rss / 1024 / 1024  # MB

                # åˆ›å»ºå·¥ä½œæµç¼–æ’å™¨
                orchestrator = WorkflowOrchestrator()

                # æ¨¡æ‹Ÿå·¥ä½œæµé…ç½®
                workflow_config = {
                    'name': f'Test Workflow {i}',
                    'stages': [
                        {
                            'name': 'analysis',
                            'description': 'Analysis stage',
                            'execution_mode': 'parallel',
                            'depends_on': []
                        },
                        {
                            'name': 'implementation',
                            'description': 'Implementation stage',
                            'execution_mode': 'sequential',
                            'depends_on': ['analysis']
                        }
                    ]
                }

                # åŠ è½½å·¥ä½œæµ
                result = orchestrator.load_workflow(workflow_config)

                if not result['success']:
                    errors += 1
                    continue

                # è®°å½•ç»“æŸçŠ¶æ€
                end_time = time.perf_counter()
                end_memory = process.memory_info().rss / 1024 / 1024  # MB

                execution_times.append(end_time - start_time)
                memory_usage.append(end_memory - start_memory)

            except Exception as e:
                errors += 1
                logger.error(f"Workflow generation benchmark error: {e}")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if execution_times:
            avg_time = statistics.mean(execution_times)
            avg_memory = statistics.mean(memory_usage)
            throughput = iterations / sum(execution_times) if execution_times else 0
            success_rate = (iterations - errors) / iterations * 100
        else:
            avg_time = avg_memory = throughput = success_rate = 0

        return BenchmarkResult(
            test_name='workflow_generation',
            execution_time=avg_time,
            memory_usage=avg_memory,
            cpu_usage=0,  # CPU usage will be calculated separately
            throughput=throughput,
            error_count=errors,
            success_rate=success_rate,
            metadata={
                'iterations': iterations,
                'min_time': min(execution_times) if execution_times else 0,
                'max_time': max(execution_times) if execution_times else 0,
                'p95_time': statistics.quantiles(execution_times, n=20)[18] if len(execution_times) >= 20 else 0
            }
        )

    def _benchmark_agent_selection(self) -> BenchmarkResult:
        """Agenté€‰æ‹©é€Ÿåº¦åŸºå‡†æµ‹è¯•"""
        config = self.benchmark_configs['agent_selection']
        iterations = config['iterations']

        execution_times = []
        errors = 0

        # æ¨¡æ‹ŸAgenté€‰æ‹©é€»è¾‘
        agents = [
            'project-manager', 'business-analyst', 'technical-writer',
            'api-designer', 'backend-architect', 'frontend-specialist',
            'database-specialist', 'security-auditor', 'test-engineer'
        ]

        for i in range(iterations):
            try:
                start_time = time.perf_counter()

                # æ¨¡æ‹Ÿå¤æ‚çš„Agenté€‰æ‹©é€»è¾‘
                task_description = f"Implement feature {i % 100}"
                selected_agents = []

                # åŸºäºä»»åŠ¡æè¿°é€‰æ‹©Agentï¼ˆæ¨¡æ‹Ÿå¤æ‚åŒ¹é…ç®—æ³•ï¼‰
                for agent in agents:
                    score = 0
                    if 'implement' in task_description.lower():
                        if agent in ['backend-architect', 'frontend-specialist']:
                            score += 0.8
                    if 'feature' in task_description.lower():
                        if agent in ['project-manager', 'business-analyst']:
                            score += 0.6

                    if score > 0.5:
                        selected_agents.append(agent)

                end_time = time.perf_counter()
                execution_times.append(end_time - start_time)

            except Exception as e:
                errors += 1
                logger.error(f"Agent selection benchmark error: {e}")

        if execution_times:
            avg_time = statistics.mean(execution_times)
            throughput = iterations / sum(execution_times)
            success_rate = (iterations - errors) / iterations * 100
        else:
            avg_time = throughput = success_rate = 0

        return BenchmarkResult(
            test_name='agent_selection',
            execution_time=avg_time,
            memory_usage=0,
            cpu_usage=0,
            throughput=throughput,
            error_count=errors,
            success_rate=success_rate,
            metadata={
                'iterations': iterations,
                'agents_tested': len(agents),
                'avg_selections_per_iteration': 2.5
            }
        )

    def _benchmark_parallel_execution(self) -> BenchmarkResult:
        """å¹¶è¡Œæ‰§è¡Œæ•ˆç‡åŸºå‡†æµ‹è¯•"""
        config = self.benchmark_configs['parallel_execution']
        workers = config['workers']
        tasks = config['tasks']

        def worker_task(task_id):
            """æ¨¡æ‹Ÿå·¥ä½œä»»åŠ¡"""
            start_time = time.perf_counter()

            # æ¨¡æ‹ŸCPUå¯†é›†å‹ä»»åŠ¡
            result = sum(i * i for i in range(1000))

            # æ¨¡æ‹ŸI/Oæ“ä½œ
            time.sleep(0.01)

            execution_time = time.perf_counter() - start_time
            return {
                'task_id': task_id,
                'result': result,
                'execution_time': execution_time
            }

        # æµ‹è¯•é¡ºåºæ‰§è¡Œ
        sequential_start = time.perf_counter()
        sequential_results = []
        for i in range(tasks):
            sequential_results.append(worker_task(i))
        sequential_time = time.perf_counter() - sequential_start

        # æµ‹è¯•å¹¶è¡Œæ‰§è¡Œ
        parallel_start = time.perf_counter()
        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
            parallel_results = list(executor.map(worker_task, range(tasks)))
        parallel_time = time.perf_counter() - parallel_start

        # è®¡ç®—å¹¶è¡Œæ•ˆç‡
        speedup = sequential_time / parallel_time if parallel_time > 0 else 0
        efficiency = speedup / workers * 100

        return BenchmarkResult(
            test_name='parallel_execution',
            execution_time=parallel_time,
            memory_usage=0,
            cpu_usage=0,
            throughput=tasks / parallel_time if parallel_time > 0 else 0,
            error_count=0,
            success_rate=100.0,
            metadata={
                'sequential_time': sequential_time,
                'parallel_time': parallel_time,
                'speedup': speedup,
                'efficiency': efficiency,
                'workers': workers,
                'tasks': tasks
            }
        )

    def _benchmark_file_operations(self) -> BenchmarkResult:
        """æ–‡ä»¶æ“ä½œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        config = self.benchmark_configs['file_operations']
        file_count = config['file_count']
        file_size = config['file_size']

        temp_dir = Path('/tmp/perfect21_perf_test')
        temp_dir.mkdir(exist_ok=True)

        execution_times = []
        errors = 0

        try:
            # æ–‡ä»¶å†™å…¥æµ‹è¯•
            write_times = []
            for i in range(file_count):
                try:
                    start_time = time.perf_counter()

                    test_file = temp_dir / f'test_file_{i}.txt'
                    test_content = 'x' * file_size

                    with open(test_file, 'w') as f:
                        f.write(test_content)

                    end_time = time.perf_counter()
                    write_times.append(end_time - start_time)

                except Exception as e:
                    errors += 1
                    logger.error(f"File write error: {e}")

            # æ–‡ä»¶è¯»å–æµ‹è¯•
            read_times = []
            for i in range(file_count):
                try:
                    start_time = time.perf_counter()

                    test_file = temp_dir / f'test_file_{i}.txt'
                    with open(test_file, 'r') as f:
                        content = f.read()

                    end_time = time.perf_counter()
                    read_times.append(end_time - start_time)

                except Exception as e:
                    errors += 1
                    logger.error(f"File read error: {e}")

            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            for i in range(file_count):
                test_file = temp_dir / f'test_file_{i}.txt'
                if test_file.exists():
                    test_file.unlink()

            temp_dir.rmdir()

            execution_times = write_times + read_times

        except Exception as e:
            errors += 1
            logger.error(f"File operations benchmark error: {e}")

        if execution_times:
            avg_time = statistics.mean(execution_times)
            throughput = len(execution_times) / sum(execution_times)
            success_rate = (file_count * 2 - errors) / (file_count * 2) * 100
        else:
            avg_time = throughput = success_rate = 0

        return BenchmarkResult(
            test_name='file_operations',
            execution_time=avg_time,
            memory_usage=0,
            cpu_usage=0,
            throughput=throughput,
            error_count=errors,
            success_rate=success_rate,
            metadata={
                'file_count': file_count,
                'file_size': file_size,
                'write_times': write_times[:10] if write_times else [],
                'read_times': read_times[:10] if read_times else []
            }
        )

    def _benchmark_regex_performance(self) -> BenchmarkResult:
        """æ­£åˆ™è¡¨è¾¾å¼æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        config = self.benchmark_configs['regex_performance']
        patterns_count = config['patterns']
        text_size = config['text_size']

        # ç”Ÿæˆæµ‹è¯•æ–‡æœ¬
        test_text = 'The quick brown fox jumps over the lazy dog. ' * (text_size // 45)

        # æµ‹è¯•æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        patterns = [
            r'\b\w+\b',          # å•è¯åŒ¹é…
            r'\d+',              # æ•°å­—åŒ¹é…
            r'[A-Z][a-z]+',      # å¤§å†™å¼€å¤´çš„å•è¯
            r'\b\w{4,}\b',       # 4ä¸ªå­—ç¬¦ä»¥ä¸Šçš„å•è¯
            r'(?i)the',          # å¿½ç•¥å¤§å°å†™çš„"the"
            r'\w+@\w+\.\w+',     # é‚®ç®±æ¨¡å¼
            r'http[s]?://\S+',   # URLæ¨¡å¼
            r'\b[A-Z]{2,}\b',    # å…¨å¤§å†™ç¼©å†™
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}',  # IPåœ°å€
            r'(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)'  # å¤æ‚IP
        ]

        execution_times = []
        errors = 0
        total_matches = 0

        for i in range(patterns_count):
            try:
                pattern = patterns[i % len(patterns)]

                start_time = time.perf_counter()

                # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
                compiled_pattern = re.compile(pattern)

                # æ‰§è¡ŒåŒ¹é…
                matches = compiled_pattern.findall(test_text)
                total_matches += len(matches)

                end_time = time.perf_counter()
                execution_times.append(end_time - start_time)

            except Exception as e:
                errors += 1
                logger.error(f"Regex benchmark error: {e}")

        if execution_times:
            avg_time = statistics.mean(execution_times)
            throughput = patterns_count / sum(execution_times)
            success_rate = (patterns_count - errors) / patterns_count * 100
        else:
            avg_time = throughput = success_rate = 0

        return BenchmarkResult(
            test_name='regex_performance',
            execution_time=avg_time,
            memory_usage=0,
            cpu_usage=0,
            throughput=throughput,
            error_count=errors,
            success_rate=success_rate,
            metadata={
                'patterns_tested': patterns_count,
                'text_size': text_size,
                'total_matches': total_matches,
                'patterns_used': len(patterns)
            }
        )

    def _analyze_resource_usage(self) -> Dict[str, Any]:
        """åˆ†æèµ„æºä½¿ç”¨æƒ…å†µ"""
        log_info("å¼€å§‹èµ„æºä½¿ç”¨ç›‘æ§...")

        # å¯åŠ¨èµ„æºç›‘æ§
        monitoring_thread = threading.Thread(
            target=self._monitor_resources,
            args=(self.monitoring_duration,),
            daemon=True
        )
        monitoring_thread.start()

        # ç­‰å¾…ç›‘æ§å®Œæˆ
        monitoring_thread.join(timeout=self.monitoring_duration + 5)

        if not self.resource_samples:
            return {'error': 'No resource samples collected'}

        # åˆ†æèµ„æºä½¿ç”¨æ•°æ®
        cpu_usage = [sample.cpu_percent for sample in self.resource_samples]
        memory_usage = [sample.memory_percent for sample in self.resource_samples]
        memory_rss = [sample.memory_rss for sample in self.resource_samples]
        disk_usage = [sample.disk_usage for sample in self.resource_samples]

        return {
            'monitoring_duration': self.monitoring_duration,
            'samples_collected': len(self.resource_samples),
            'cpu_analysis': {
                'avg': statistics.mean(cpu_usage),
                'max': max(cpu_usage),
                'min': min(cpu_usage),
                'p95': statistics.quantiles(cpu_usage, n=20)[18] if len(cpu_usage) >= 20 else max(cpu_usage)
            },
            'memory_analysis': {
                'avg_percent': statistics.mean(memory_usage),
                'max_percent': max(memory_usage),
                'avg_rss_mb': statistics.mean(memory_rss),
                'max_rss_mb': max(memory_rss),
                'memory_growth': memory_rss[-1] - memory_rss[0] if len(memory_rss) > 1 else 0
            },
            'disk_analysis': {
                'avg_usage': statistics.mean(disk_usage),
                'max_usage': max(disk_usage)
            },
            'stability_metrics': {
                'cpu_volatility': statistics.stdev(cpu_usage) if len(cpu_usage) > 1 else 0,
                'memory_volatility': statistics.stdev(memory_usage) if len(memory_usage) > 1 else 0
            }
        }

    def _monitor_resources(self, duration: int):
        """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨"""
        process = psutil.Process()
        end_time = time.time() + duration

        while time.time() < end_time:
            try:
                # æ”¶é›†èµ„æºä½¿ç”¨æ•°æ®
                cpu_percent = process.cpu_percent()
                memory_info = process.memory_info()
                memory_percent = process.memory_percent()

                # ç³»ç»Ÿçº§æ•°æ®
                disk_usage = psutil.disk_usage('/').percent
                network_io = psutil.net_io_counters()._asdict()

                # è¿›ç¨‹çº§æ•°æ®
                try:
                    file_descriptors = process.num_fds()
                except AttributeError:
                    file_descriptors = 0  # Windowsä¸æ”¯æŒ

                threads = process.num_threads()

                sample = ResourceUsage(
                    timestamp=datetime.now(),
                    cpu_percent=cpu_percent,
                    memory_percent=memory_percent,
                    memory_rss=memory_info.rss / 1024 / 1024,  # MB
                    memory_vms=memory_info.vms / 1024 / 1024,  # MB
                    disk_usage=disk_usage,
                    network_io=network_io,
                    file_descriptors=file_descriptors,
                    threads=threads
                )

                self.resource_samples.append(sample)

                time.sleep(self.sampling_interval)

            except Exception as e:
                logger.error(f"Resource monitoring error: {e}")

    def _run_performance_profiling(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½å‰–æ"""
        profiling_results = {}

        # 1. å†…å­˜å‰–æ
        profiling_results['memory_profiling'] = self._profile_memory_usage()

        # 2. CPUå‰–æ
        profiling_results['cpu_profiling'] = self._profile_cpu_usage()

        # 3. å‡½æ•°è°ƒç”¨å‰–æ
        profiling_results['function_profiling'] = self._profile_function_calls()

        return profiling_results

    def _profile_memory_usage(self) -> Dict[str, Any]:
        """å†…å­˜ä½¿ç”¨å‰–æ"""
        tracemalloc.start()

        try:
            # æ‰§è¡Œä¸€äº›å†…å­˜å¯†é›†å‹æ“ä½œ
            orchestrator = WorkflowOrchestrator()

            # åˆ›å»ºå¤šä¸ªå·¥ä½œæµ
            for i in range(10):
                workflow_config = {
                    'name': f'Memory Test Workflow {i}',
                    'stages': [
                        {
                            'name': f'stage_{j}',
                            'description': f'Stage {j}',
                            'execution_mode': 'parallel'
                        }
                        for j in range(5)
                    ]
                }
                orchestrator.load_workflow(workflow_config)

            # è·å–å†…å­˜å¿«ç…§
            snapshot = tracemalloc.take_snapshot()
            top_stats = snapshot.statistics('lineno')

            memory_analysis = {
                'total_memory_mb': sum(stat.size for stat in top_stats) / 1024 / 1024,
                'top_memory_consumers': [
                    {
                        'filename': stat.traceback.format()[0].split(', ')[0],
                        'line_number': stat.traceback.format()[0].split(', ')[1],
                        'size_mb': stat.size / 1024 / 1024,
                        'count': stat.count
                    }
                    for stat in top_stats[:10]
                ]
            }

        except Exception as e:
            memory_analysis = {'error': str(e)}
        finally:
            tracemalloc.stop()

        return memory_analysis

    def _profile_cpu_usage(self) -> Dict[str, Any]:
        """CPUä½¿ç”¨å‰–æ"""
        profiler = cProfile.Profile()

        def cpu_intensive_task():
            """CPUå¯†é›†å‹ä»»åŠ¡"""
            orchestrator = WorkflowOrchestrator()

            # æ‰§è¡Œå¤æ‚çš„å·¥ä½œæµæ“ä½œ
            for i in range(50):
                workflow_config = {
                    'name': f'CPU Test Workflow {i}',
                    'stages': [
                        {
                            'name': f'analysis_{j}',
                            'description': 'Complex analysis',
                            'execution_mode': 'sequential'
                        }
                        for j in range(3)
                    ]
                }
                orchestrator.load_workflow(workflow_config)

                # æ¨¡æ‹Ÿå¤æ‚è®¡ç®—
                result = sum(i * j for i in range(100) for j in range(100))

        # å¼€å§‹å‰–æ
        profiler.enable()
        cpu_intensive_task()
        profiler.disable()

        # åˆ†æç»“æœ
        stats = pstats.Stats(profiler)
        stats.sort_stats('cumulative')

        # æå–çƒ­ç‚¹å‡½æ•°
        hot_spots = []
        for func_name, (cc, nc, tt, ct, callers) in stats.stats.items():
            if tt > 0.01:  # åªå…³æ³¨æ‰§è¡Œæ—¶é—´è¶…è¿‡10msçš„å‡½æ•°
                hot_spots.append({
                    'function': f"{func_name[0]}:{func_name[1]}({func_name[2]})",
                    'total_time': tt,
                    'cumulative_time': ct,
                    'call_count': cc,
                    'per_call_time': tt / cc if cc > 0 else 0
                })

        # æŒ‰æ€»æ—¶é—´æ’åº
        hot_spots.sort(key=lambda x: x['total_time'], reverse=True)

        return {
            'total_function_calls': stats.total_calls,
            'total_execution_time': sum(tt for _, (_, _, tt, _, _) in stats.stats.items()),
            'top_hot_spots': hot_spots[:10],
            'performance_bottlenecks': [
                spot for spot in hot_spots[:20]
                if spot['per_call_time'] > 0.001  # æ¯æ¬¡è°ƒç”¨è¶…è¿‡1ms
            ]
        }

    def _profile_function_calls(self) -> Dict[str, Any]:
        """å‡½æ•°è°ƒç”¨å‰–æ"""
        import sys
        from collections import defaultdict

        call_counts = defaultdict(int)
        call_times = defaultdict(float)

        def trace_calls(frame, event, arg):
            if event == 'call':
                func_name = f"{frame.f_code.co_filename}:{frame.f_code.co_name}"
                call_counts[func_name] += 1
                frame.f_locals['_start_time'] = time.perf_counter()
            elif event == 'return':
                func_name = f"{frame.f_code.co_filename}:{frame.f_code.co_name}"
                start_time = frame.f_locals.get('_start_time', 0)
                if start_time:
                    call_times[func_name] += time.perf_counter() - start_time

            return trace_calls

        # å¯åŠ¨è·Ÿè¸ª
        sys.settrace(trace_calls)

        try:
            # æ‰§è¡Œä¸€äº›æµ‹è¯•æ“ä½œ
            orchestrator = WorkflowOrchestrator()

            workflow_config = {
                'name': 'Function Profiling Test',
                'stages': [
                    {
                        'name': 'test_stage',
                        'description': 'Test stage',
                        'execution_mode': 'parallel'
                    }
                ]
            }

            orchestrator.load_workflow(workflow_config)

        finally:
            sys.settrace(None)

        # åˆ†æç»“æœ
        top_by_calls = sorted(call_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        top_by_time = sorted(call_times.items(), key=lambda x: x[1], reverse=True)[:10]

        return {
            'total_unique_functions': len(call_counts),
            'total_function_calls': sum(call_counts.values()),
            'total_execution_time': sum(call_times.values()),
            'most_called_functions': [
                {'function': func, 'call_count': count}
                for func, count in top_by_calls
            ],
            'most_time_consuming_functions': [
                {'function': func, 'total_time': time_spent}
                for func, time_spent in top_by_time
            ]
        }

    def _identify_bottlenecks(self) -> Dict[str, Any]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = {
            'cpu_bottlenecks': [],
            'memory_bottlenecks': [],
            'io_bottlenecks': [],
            'algorithmic_bottlenecks': []
        }

        # åˆ†æåŸºå‡†æµ‹è¯•ç»“æœä¸­çš„ç“¶é¢ˆ
        for test_name, result in self.benchmark_results:
            if hasattr(result, 'execution_time') and result.execution_time > 0.1:  # è¶…è¿‡100ms
                bottlenecks['cpu_bottlenecks'].append({
                    'component': test_name,
                    'issue': 'High execution time',
                    'value': result.execution_time,
                    'severity': 'high' if result.execution_time > 1.0 else 'medium'
                })

            if hasattr(result, 'memory_usage') and result.memory_usage > 50:  # è¶…è¿‡50MB
                bottlenecks['memory_bottlenecks'].append({
                    'component': test_name,
                    'issue': 'High memory usage',
                    'value': result.memory_usage,
                    'severity': 'high' if result.memory_usage > 100 else 'medium'
                })

            if hasattr(result, 'success_rate') and result.success_rate < 95:  # æˆåŠŸç‡ä½äº95%
                bottlenecks['algorithmic_bottlenecks'].append({
                    'component': test_name,
                    'issue': 'Low success rate',
                    'value': result.success_rate,
                    'severity': 'critical' if result.success_rate < 90 else 'high'
                })

        # åˆ†æèµ„æºä½¿ç”¨ä¸­çš„ç“¶é¢ˆ
        if self.resource_samples:
            max_cpu = max(sample.cpu_percent for sample in self.resource_samples)
            max_memory = max(sample.memory_percent for sample in self.resource_samples)

            if max_cpu > 80:
                bottlenecks['cpu_bottlenecks'].append({
                    'component': 'system',
                    'issue': 'High CPU usage',
                    'value': max_cpu,
                    'severity': 'critical' if max_cpu > 95 else 'high'
                })

            if max_memory > 80:
                bottlenecks['memory_bottlenecks'].append({
                    'component': 'system',
                    'issue': 'High memory usage',
                    'value': max_memory,
                    'severity': 'critical' if max_memory > 95 else 'high'
                })

        # è®¡ç®—ç“¶é¢ˆä¸¥é‡æ€§
        total_bottlenecks = sum(len(bottlenecks[category]) for category in bottlenecks)
        critical_bottlenecks = sum(
            len([b for b in bottlenecks[category] if b.get('severity') == 'critical'])
            for category in bottlenecks
        )

        return {
            'bottlenecks': bottlenecks,
            'summary': {
                'total_bottlenecks': total_bottlenecks,
                'critical_bottlenecks': critical_bottlenecks,
                'severity_distribution': {
                    'critical': critical_bottlenecks,
                    'high': sum(
                        len([b for b in bottlenecks[category] if b.get('severity') == 'high'])
                        for category in bottlenecks
                    ),
                    'medium': sum(
                        len([b for b in bottlenecks[category] if b.get('severity') == 'medium'])
                        for category in bottlenecks
                    )
                }
            }
        }

    def _generate_optimization_recommendations(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºåŸºå‡†æµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        for test_name, result in self.benchmark_results:
            if hasattr(result, 'execution_time') and result.execution_time > 0.5:
                recommendations.append({
                    'category': 'performance',
                    'priority': 'high',
                    'component': test_name,
                    'issue': f'Slow {test_name}',
                    'recommendation': f'Optimize {test_name} algorithm or implement caching',
                    'expected_improvement': '30-50% faster execution',
                    'implementation_effort': 'medium'
                })

            if hasattr(result, 'memory_usage') and result.memory_usage > 100:
                recommendations.append({
                    'category': 'memory',
                    'priority': 'medium',
                    'component': test_name,
                    'issue': f'High memory usage in {test_name}',
                    'recommendation': 'Implement memory pooling or reduce object creation',
                    'expected_improvement': '20-40% memory reduction',
                    'implementation_effort': 'medium'
                })

        # åŸºäºèµ„æºåˆ†æç”Ÿæˆå»ºè®®
        if self.resource_samples:
            cpu_usage = [sample.cpu_percent for sample in self.resource_samples]
            memory_growth = (
                self.resource_samples[-1].memory_rss - self.resource_samples[0].memory_rss
                if len(self.resource_samples) > 1 else 0
            )

            if statistics.mean(cpu_usage) > 50:
                recommendations.append({
                    'category': 'cpu',
                    'priority': 'high',
                    'component': 'system',
                    'issue': 'High average CPU usage',
                    'recommendation': 'Implement async processing or optimize hot code paths',
                    'expected_improvement': 'Reduce CPU usage by 25-40%',
                    'implementation_effort': 'high'
                })

            if memory_growth > 50:  # 50MBå¢é•¿
                recommendations.append({
                    'category': 'memory',
                    'priority': 'high',
                    'component': 'system',
                    'issue': 'Memory leak detected',
                    'recommendation': 'Implement proper cleanup and garbage collection',
                    'expected_improvement': 'Prevent memory leaks',
                    'implementation_effort': 'high'
                })

        # é€šç”¨ä¼˜åŒ–å»ºè®®
        recommendations.extend([
            {
                'category': 'caching',
                'priority': 'medium',
                'component': 'workflow_orchestrator',
                'issue': 'Repeated workflow loading',
                'recommendation': 'Implement workflow template caching',
                'expected_improvement': '60-80% faster workflow loading',
                'implementation_effort': 'low'
            },
            {
                'category': 'concurrency',
                'priority': 'medium',
                'component': 'task_execution',
                'issue': 'Sequential task processing',
                'recommendation': 'Increase parallelism in task execution',
                'expected_improvement': '2-4x faster task completion',
                'implementation_effort': 'medium'
            },
            {
                'category': 'database',
                'priority': 'low',
                'component': 'data_access',
                'issue': 'Database query optimization',
                'recommendation': 'Add database indexes and query optimization',
                'expected_improvement': '50-200% faster queries',
                'implementation_effort': 'low'
            }
        ])

        return recommendations

    def _calculate_performance_score(self) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        base_score = 100.0

        # åŸºäºåŸºå‡†æµ‹è¯•ç»“æœè®¡ç®—åˆ†æ•°
        benchmark_penalty = 0
        for test_name, result in self.benchmark_results:
            if hasattr(result, 'success_rate'):
                if result.success_rate < 95:
                    benchmark_penalty += (95 - result.success_rate) * 0.5

            if hasattr(result, 'execution_time'):
                if result.execution_time > 1.0:  # è¶…è¿‡1ç§’
                    benchmark_penalty += min(10, result.execution_time * 2)

        # åŸºäºèµ„æºä½¿ç”¨è®¡ç®—åˆ†æ•°
        resource_penalty = 0
        if self.resource_samples:
            cpu_usage = [sample.cpu_percent for sample in self.resource_samples]
            memory_usage = [sample.memory_percent for sample in self.resource_samples]

            avg_cpu = statistics.mean(cpu_usage)
            avg_memory = statistics.mean(memory_usage)

            if avg_cpu > 70:
                resource_penalty += (avg_cpu - 70) * 0.3

            if avg_memory > 80:
                resource_penalty += (avg_memory - 80) * 0.5

        # æœ€ç»ˆåˆ†æ•°
        final_score = max(0, base_score - benchmark_penalty - resource_penalty)

        # åˆ†æ•°ç­‰çº§
        if final_score >= 90:
            grade = 'A'
            status = 'Excellent'
        elif final_score >= 80:
            grade = 'B'
            status = 'Good'
        elif final_score >= 70:
            grade = 'C'
            status = 'Average'
        elif final_score >= 60:
            grade = 'D'
            status = 'Below Average'
        else:
            grade = 'F'
            status = 'Poor'

        return {
            'overall_score': round(final_score, 1),
            'grade': grade,
            'status': status,
            'score_breakdown': {
                'base_score': base_score,
                'benchmark_penalty': benchmark_penalty,
                'resource_penalty': resource_penalty
            },
            'improvement_potential': max(0, 100 - final_score)
        }

    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            'platform': os.name,
            'cpu_count': psutil.cpu_count(),
            'cpu_freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else {},
            'memory_total_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
            'disk_total_gb': psutil.disk_usage('/').total / 1024 / 1024 / 1024,
            'python_version': sys.version,
            'process_id': os.getpid()
        }

    def _save_results(self):
        """ä¿å­˜åˆ†æç»“æœ"""
        output_file = f"performance_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        try:
            with open(output_file, 'w') as f:
                json.dump(self.results, f, indent=2, default=str)

            log_info(f"æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

        except Exception as e:
            log_error(f"ä¿å­˜åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Perfect21 æ€§èƒ½åˆ†æå·¥å…·")
    print("=" * 50)

    analyzer = PerformanceAnalyzer()
    results = analyzer.run_complete_analysis()

    if 'error' in results:
        print(f"âŒ åˆ†æå¤±è´¥: {results['error']}")
        return

    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print("\nğŸ“Š æ€§èƒ½åˆ†æç»“æœæ‘˜è¦:")
    print("-" * 30)

    score_info = results.get('performance_score', {})
    print(f"ğŸ“ˆ æ€»ä½“è¯„åˆ†: {score_info.get('overall_score', 0)}/100 ({score_info.get('grade', 'N/A')})")
    print(f"ğŸ“Š çŠ¶æ€: {score_info.get('status', 'Unknown')}")

    benchmark_results = results.get('benchmark_results', {})
    print(f"\nğŸ§ª åŸºå‡†æµ‹è¯•ç»“æœ:")
    for test_name, result in benchmark_results.items():
        if hasattr(result, 'execution_time'):
            print(f"  â€¢ {test_name}: {result.execution_time:.3f}s (æˆåŠŸç‡: {result.success_rate:.1f}%)")

    resource_analysis = results.get('resource_analysis', {})
    if 'cpu_analysis' in resource_analysis:
        cpu_info = resource_analysis['cpu_analysis']
        print(f"\nğŸ’» CPUä½¿ç”¨: å¹³å‡ {cpu_info.get('avg', 0):.1f}%, æœ€å¤§ {cpu_info.get('max', 0):.1f}%")

    if 'memory_analysis' in resource_analysis:
        mem_info = resource_analysis['memory_analysis']
        print(f"ğŸ§  å†…å­˜ä½¿ç”¨: å¹³å‡ {mem_info.get('avg_rss_mb', 0):.1f}MB")

    bottlenecks = results.get('bottleneck_analysis', {}).get('summary', {})
    print(f"\nâš ï¸  æ€§èƒ½ç“¶é¢ˆ: {bottlenecks.get('total_bottlenecks', 0)} ä¸ª")
    print(f"   å…¶ä¸­ {bottlenecks.get('critical_bottlenecks', 0)} ä¸ªä¸¥é‡ç“¶é¢ˆ")

    recommendations = results.get('optimization_recommendations', [])
    high_priority_recs = [r for r in recommendations if r.get('priority') == 'high']
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®: {len(recommendations)} æ¡å»ºè®® ({len(high_priority_recs)} æ¡é«˜ä¼˜å…ˆçº§)")

    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°æ–‡ä»¶")
    print("=" * 50)

if __name__ == "__main__":
    main()