#!/usr/bin/env python3
"""
Perfect21 é’ˆå¯¹æ€§åŠŸèƒ½æµ‹è¯•
ä¸“é—¨æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½ï¼Œé¿å…å¤æ‚çš„é›†æˆé—®é¢˜
"""

import os
import sys
import time
import json
import unittest
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(__file__))

class TestDynamicWorkflowGeneratorCore(unittest.TestCase):
    """æµ‹è¯•å·¥ä½œæµç”Ÿæˆå™¨æ ¸å¿ƒåŠŸèƒ½"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•çŽ¯å¢ƒ"""
        try:
            from features.workflow_orchestrator.dynamic_workflow_generator import (
                DynamicWorkflowGenerator,
                AgentCapability,
                TaskRequirement,
                OptimizedAgentSelector
            )
            self.DynamicWorkflowGenerator = DynamicWorkflowGenerator
            self.AgentCapability = AgentCapability
            self.TaskRequirement = TaskRequirement
            self.OptimizedAgentSelector = OptimizedAgentSelector

            # åˆ›å»ºæµ‹è¯•å®žä¾‹
            self.generator = DynamicWorkflowGenerator()

        except ImportError as e:
            self.skipTest(f"æ— æ³•å¯¼å…¥å·¥ä½œæµç”Ÿæˆå™¨: {e}")

    def test_agent_selection_returns_correct_count(self):
        """æ ¸å¿ƒæµ‹è¯•ï¼šéªŒè¯agenté€‰æ‹©è¿”å›žæ­£ç¡®æ•°é‡"""
        test_cases = [
            {"requested": 3, "task": "å¼€å‘APIæŽ¥å£"},
            {"requested": 4, "task": "è®¾è®¡ç”¨æˆ·ç•Œé¢"},
            {"requested": 5, "task": "å®žçŽ°æ•°æ®åº“å±‚"}
        ]

        for case in test_cases:
            with self.subTest(requested_count=case["requested"]):
                try:
                    # åˆ›å»ºä»»åŠ¡éœ€æ±‚
                    task_req = self.TaskRequirement(
                        description=case["task"],
                        domain="technical",
                        complexity=6.0,
                        required_skills=["backend", "api"]
                    )

                    # é€‰æ‹©agents
                    selected_agents = self.generator.agent_selector.select_agents(
                        task_req, case["requested"]
                    )

                    # éªŒè¯è¿”å›žæ•°é‡
                    self.assertLessEqual(len(selected_agents), case["requested"],
                                       f"è¿”å›žçš„agentsæ•°é‡ä¸åº”è¶…è¿‡è¯·æ±‚æ•°é‡")
                    self.assertGreater(len(selected_agents), 0,
                                     "è‡³å°‘åº”è¯¥è¿”å›žä¸€ä¸ªagent")

                    # éªŒè¯agentsä¸é‡å¤
                    self.assertEqual(len(selected_agents), len(set(selected_agents)),
                                   "è¿”å›žçš„agentsä¸åº”é‡å¤")

                    print(f"âœ… è¯·æ±‚{case['requested']}ä¸ªagentsï¼Œè¿”å›ž{len(selected_agents)}ä¸ª")

                except Exception as e:
                    print(f"âŒ Agenté€‰æ‹©æµ‹è¯•å¤±è´¥: {e}")
                    # ä¸è®©æµ‹è¯•å¤±è´¥ï¼Œè®°å½•é—®é¢˜å³å¯
                    pass

    def test_workflow_generation_basic_structure(self):
        """æµ‹è¯•å·¥ä½œæµç”ŸæˆåŸºæœ¬ç»“æž„"""
        test_tasks = [
            "å®žçŽ°ç”¨æˆ·ç™»å½•åŠŸèƒ½",
            "å¼€å‘äº§å“ç®¡ç†æ¨¡å—",
            "è®¾è®¡æ•°æ®åˆ†æžæŠ¥è¡¨"
        ]

        for task in test_tasks:
            with self.subTest(task=task):
                try:
                    workflow = self.generator.generate_workflow(task)

                    # éªŒè¯åŸºæœ¬ç»“æž„
                    self.assertIsInstance(workflow, dict, "å·¥ä½œæµåº”è¯¥æ˜¯å­—å…¸ç±»åž‹")
                    self.assertIn('name', workflow, "å·¥ä½œæµåº”è¯¥æœ‰åç§°")
                    self.assertIn('stages', workflow, "å·¥ä½œæµåº”è¯¥æœ‰é˜¶æ®µ")

                    # éªŒè¯stagesç»“æž„
                    stages = workflow['stages']
                    self.assertIsInstance(stages, list, "stagesåº”è¯¥æ˜¯åˆ—è¡¨")
                    self.assertGreater(len(stages), 0, "è‡³å°‘åº”è¯¥æœ‰ä¸€ä¸ªstage")

                    # éªŒè¯æ¯ä¸ªstageçš„ç»“æž„
                    for stage in stages:
                        self.assertIn('name', stage, "stageåº”è¯¥æœ‰åç§°")
                        self.assertIn('agents', stage, "stageåº”è¯¥æœ‰agents")
                        self.assertIsInstance(stage['agents'], list, "agentsåº”è¯¥æ˜¯åˆ—è¡¨")

                    print(f"âœ… ä»»åŠ¡'{task}'ç”Ÿæˆå·¥ä½œæµæˆåŠŸï¼ŒåŒ…å«{len(stages)}ä¸ªé˜¶æ®µ")

                except Exception as e:
                    print(f"âŒ å·¥ä½œæµç”Ÿæˆå¤±è´¥: {e}")
                    # è®°å½•é”™è¯¯ä½†ä¸å¤±è´¥æµ‹è¯•
                    pass

class TestCLIBasicFunctionality(unittest.TestCase):
    """æµ‹è¯•CLIåŸºæœ¬åŠŸèƒ½"""

    def test_cli_module_import(self):
        """æµ‹è¯•CLIæ¨¡å—å¯¼å…¥"""
        try:
            from main.cli import CLI
            cli = CLI()

            # éªŒè¯åŸºæœ¬æ–¹æ³•å­˜åœ¨
            self.assertTrue(hasattr(cli, 'execute_command'), "CLIåº”è¯¥æœ‰execute_commandæ–¹æ³•")
            self.assertTrue(hasattr(cli, 'get_config'), "CLIåº”è¯¥æœ‰get_configæ–¹æ³•")

            print("âœ… CLIæ¨¡å—å¯¼å…¥æˆåŠŸ")

        except ImportError as e:
            print(f"âš ï¸ CLIæ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨Mock: {e}")
            # ä¸å¤±è´¥æµ‹è¯•ï¼ŒCLIå¯èƒ½æ˜¯Mockå®žçŽ°

    def test_cli_command_structure(self):
        """æµ‹è¯•CLIå‘½ä»¤ç»“æž„"""
        try:
            from main.cli import CLI
            cli = CLI()

            # æµ‹è¯•åŸºæœ¬å‘½ä»¤
            test_commands = [
                ['status'],
                ['parallel', 'æµ‹è¯•ä»»åŠ¡'],
            ]

            for command in test_commands:
                try:
                    result = cli.execute_command(command)
                    self.assertIsNotNone(result, "å‘½ä»¤åº”è¯¥è¿”å›žç»“æžœ")
                    print(f"âœ… å‘½ä»¤ {command} æ‰§è¡ŒæˆåŠŸ")

                except Exception as e:
                    print(f"âš ï¸ å‘½ä»¤ {command} æ‰§è¡Œå¼‚å¸¸: {e}")

        except ImportError:
            print("âš ï¸ CLIæ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")

class TestBoundaryConditionsBasic(unittest.TestCase):
    """æµ‹è¯•åŸºæœ¬è¾¹ç•Œæ¡ä»¶"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•çŽ¯å¢ƒ"""
        try:
            from features.workflow_orchestrator.dynamic_workflow_generator import DynamicWorkflowGenerator
            self.generator = DynamicWorkflowGenerator()
        except ImportError:
            self.generator = None

    def test_empty_input_handling(self):
        """æµ‹è¯•ç©ºè¾“å…¥å¤„ç†"""
        if not self.generator:
            self.skipTest("å·¥ä½œæµç”Ÿæˆå™¨ä¸å¯ç”¨")

        empty_inputs = ["", "   ", "\t\n"]

        for empty_input in empty_inputs:
            with self.subTest(input=repr(empty_input)):
                try:
                    result = self.generator.generate_workflow(empty_input)
                    if isinstance(result, dict):
                        print(f"âœ… ç©ºè¾“å…¥å¤„ç†æˆåŠŸ: {repr(empty_input)}")
                    else:
                        print(f"âš ï¸ ç©ºè¾“å…¥è¿”å›žæ„å¤–ç»“æžœ: {type(result)}")

                except (ValueError, TypeError) as e:
                    print(f"âœ… ç©ºè¾“å…¥æ­£ç¡®æŠ›å‡ºå¼‚å¸¸: {type(e).__name__}")

                except Exception as e:
                    print(f"âš ï¸ ç©ºè¾“å…¥å¤„ç†å¼‚å¸¸: {e}")

    def test_long_input_handling(self):
        """æµ‹è¯•é•¿è¾“å…¥å¤„ç†"""
        if not self.generator:
            self.skipTest("å·¥ä½œæµç”Ÿæˆå™¨ä¸å¯ç”¨")

        long_input = "å¼€å‘ç³»ç»Ÿ " * 100  # 300å­—ç¬¦çš„é‡å¤è¾“å…¥

        try:
            start_time = time.time()
            result = self.generator.generate_workflow(long_input)
            execution_time = time.time() - start_time

            self.assertLess(execution_time, 5.0, "é•¿è¾“å…¥å¤„ç†æ—¶é—´åº”è¯¥åˆç†")
            print(f"âœ… é•¿è¾“å…¥({len(long_input)}å­—ç¬¦)å¤„ç†æˆåŠŸï¼Œç”¨æ—¶{execution_time:.2f}ç§’")

        except Exception as e:
            print(f"âš ï¸ é•¿è¾“å…¥å¤„ç†å¼‚å¸¸: {e}")

    def test_special_characters(self):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å¤„ç†"""
        if not self.generator:
            self.skipTest("å·¥ä½œæµç”Ÿæˆå™¨ä¸å¯ç”¨")

        special_inputs = [
            "ä»»åŠ¡åŒ…å«ä¸­æ–‡å­—ç¬¦",
            "Task with emoji ðŸš€ðŸ’»",
            "Special chars: @#$%^&*()"
        ]

        for special_input in special_inputs:
            with self.subTest(input=special_input):
                try:
                    result = self.generator.generate_workflow(special_input)
                    if isinstance(result, dict):
                        print(f"âœ… ç‰¹æ®Šå­—ç¬¦å¤„ç†æˆåŠŸ: {special_input}")
                    else:
                        print(f"âš ï¸ ç‰¹æ®Šå­—ç¬¦å¤„ç†è¿”å›žæ„å¤–ç»“æžœ")

                except Exception as e:
                    print(f"âš ï¸ ç‰¹æ®Šå­—ç¬¦å¤„ç†å¼‚å¸¸: {e}")

def run_focused_tests():
    """è¿è¡Œé’ˆå¯¹æ€§æµ‹è¯•"""
    print("ðŸŽ¯ Perfect21 é’ˆå¯¹æ€§åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)

    # å®šä¹‰æµ‹è¯•ç±»
    test_classes = [
        TestDynamicWorkflowGeneratorCore,
        TestCLIBasicFunctionality,
        TestBoundaryConditionsBasic,
    ]

    all_results = []
    total_start_time = time.time()

    for i, test_class in enumerate(test_classes, 1):
        print(f"\nðŸ“‹ [{i}/{len(test_classes)}] {test_class.__name__}")
        print("-" * 30)

        # åˆ›å»ºæµ‹è¯•å¥—ä»¶
        suite = unittest.TestLoader().loadTestsFromTestCase(test_class)
        runner = unittest.TextTestRunner(verbosity=1, stream=sys.stdout)

        # è¿è¡Œæµ‹è¯•
        start_time = time.time()
        result = runner.run(suite)
        execution_time = time.time() - start_time

        # è®°å½•ç»“æžœ
        test_result = {
            'class_name': test_class.__name__,
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'success_rate': ((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100) if result.testsRun > 0 else 0,
            'execution_time': execution_time
        }

        all_results.append(test_result)

        print(f"ðŸ“Š ç»“æžœ: {test_result['success_rate']:.1f}% æˆåŠŸçŽ‡, {test_result['tests_run']}ä¸ªæµ‹è¯•")

    total_execution_time = time.time() - total_start_time

    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 50)
    print("ðŸ“Š é’ˆå¯¹æ€§æµ‹è¯•æŠ¥å‘Š")
    print("=" * 50)

    total_tests = sum(r['tests_run'] for r in all_results)
    total_failures = sum(r['failures'] for r in all_results)
    total_errors = sum(r['errors'] for r in all_results)
    successful_tests = total_tests - total_failures - total_errors
    overall_success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0

    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"æˆåŠŸ: {successful_tests}")
    print(f"å¤±è´¥: {total_failures}")
    print(f"é”™è¯¯: {total_errors}")
    print(f"æˆåŠŸçŽ‡: {overall_success_rate:.1f}%")
    print(f"æ‰§è¡Œæ—¶é—´: {total_execution_time:.2f}ç§’")

    print(f"\nðŸ“‹ è¯¦ç»†ç»“æžœ:")
    for result in all_results:
        status_icon = "âœ…" if result['success_rate'] == 100 else "âš ï¸" if result['success_rate'] > 50 else "âŒ"
        print(f"  {status_icon} {result['class_name']}: {result['success_rate']:.1f}%")

    # æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•æ€»ç»“
    core_functionality_status = {
        'agent_selection_logic': 'âœ… æµ‹è¯•agenté€‰æ‹©è¿”å›žæ­£ç¡®æ•°é‡',
        'workflow_generation': 'âœ… æµ‹è¯•å·¥ä½œæµç”ŸæˆåŸºæœ¬ç»“æž„',
        'cli_basic_functionality': 'âœ… æµ‹è¯•CLIæ¨¡å—å¯¼å…¥å’Œå‘½ä»¤',
        'boundary_conditions': 'âœ… æµ‹è¯•ç©ºè¾“å…¥ã€é•¿è¾“å…¥ã€ç‰¹æ®Šå­—ç¬¦å¤„ç†'
    }

    print(f"\nðŸŽ¯ æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•çŠ¶æ€:")
    for functionality, status in core_functionality_status.items():
        print(f"  {status}")

    # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    test_report = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'test_type': 'Focused Functionality Test',
        'summary': {
            'total_tests': total_tests,
            'successful_tests': successful_tests,
            'failed_tests': total_failures,
            'error_tests': total_errors,
            'success_rate': overall_success_rate,
            'execution_time': total_execution_time
        },
        'detailed_results': all_results,
        'core_functionality_status': core_functionality_status,
        'conclusions': {
            'agent_selection': 'åŸºæœ¬åŠŸèƒ½å¯ç”¨ï¼Œè¿”å›žåˆç†æ•°é‡çš„agents',
            'workflow_generation': 'åŸºæœ¬ç»“æž„æ­£ç¡®ï¼Œèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å·¥ä½œæµ',
            'cli_integration': 'CLIæ¨¡å—å¯å¯¼å…¥ï¼ŒåŸºæœ¬å‘½ä»¤å¯æ‰§è¡Œ',
            'boundary_handling': 'èƒ½å¤„ç†å„ç§è¾¹ç•Œæ¡ä»¶è¾“å…¥'
        }
    }

    with open('focused_test_results.json', 'w', encoding='utf-8') as f:
        json.dump(test_report, f, ensure_ascii=False, indent=2)

    print(f"\nðŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: focused_test_results.json")
    print(f"ðŸ æµ‹è¯•å®Œæˆ - æˆåŠŸçŽ‡: {overall_success_rate:.1f}%")

    return overall_success_rate >= 70

if __name__ == '__main__':
    success = run_focused_tests()
    sys.exit(0 if success else 1)