#!/usr/bin/env python3
"""
Perfect21 Opus41 æ™ºèƒ½å¹¶è¡Œä¼˜åŒ–å™¨é›†æˆæµ‹è¯•
æµ‹è¯•Opus41çš„å„é¡¹åŠŸèƒ½ï¼Œç¡®ä¿ç³»ç»Ÿæ­£å¸¸å·¥ä½œ
"""

import unittest
import time
import json
import os
import sys
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__)))

from features.opus41_optimizer import (
    get_opus41_optimizer,
    OptimizationLevel,
    QualityThreshold,
    QualityLevel
)
from features.opus41_visualizer import get_opus41_visualizer

class TestOpus41Integration(unittest.TestCase):
    """Opus41é›†æˆæµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        self.optimizer = get_opus41_optimizer()
        self.visualizer = get_opus41_visualizer()
        self.test_task = "å¼€å‘ä¸€ä¸ªé«˜æ€§èƒ½çš„å¾®æœåŠ¡ç”µå•†å¹³å°"

    def test_optimizer_initialization(self):
        """æµ‹è¯•ä¼˜åŒ–å™¨åˆå§‹åŒ–"""
        print("\nğŸ§ª æµ‹è¯•ä¼˜åŒ–å™¨åˆå§‹åŒ–...")

        # æ£€æŸ¥ä¼˜åŒ–å™¨å®ä¾‹
        self.assertIsNotNone(self.optimizer)
        self.assertIsNotNone(self.optimizer.decomposer)
        self.assertIsNotNone(self.optimizer.agent_metrics)

        # æ£€æŸ¥åŸºç¡€é…ç½®
        self.assertEqual(self.optimizer.max_parallel_agents, 20)
        self.assertEqual(self.optimizer.quality_threshold, 0.95)
        self.assertEqual(self.optimizer.max_refinement_rounds, 5)

        # æ£€æŸ¥Agentåˆ†ç±»
        self.assertIn('business', self.optimizer.agent_categories)
        self.assertIn('development', self.optimizer.agent_categories)
        self.assertIn('quality', self.optimizer.agent_categories)

        print("âœ… ä¼˜åŒ–å™¨åˆå§‹åŒ–æµ‹è¯•é€šè¿‡")

    def test_agent_selection(self):
        """æµ‹è¯•æ™ºèƒ½Agenté€‰æ‹©"""
        print("\nğŸ§ª æµ‹è¯•æ™ºèƒ½Agenté€‰æ‹©...")

        test_cases = [
            ("åˆ›å»ºReactå‰ç«¯åº”ç”¨", QualityLevel.PREMIUM),
            ("å¼€å‘REST API", QualityLevel.BALANCED),
            ("AIæ¨èç³»ç»Ÿ", QualityLevel.ULTIMATE),
            ("éƒ¨ç½²å®¹å™¨åŒ–åº”ç”¨", QualityLevel.FAST)
        ]

        for task, quality_level in test_cases:
            with self.subTest(task=task, quality=quality_level):
                agents = self.optimizer.select_optimal_agents(task, quality_level)

                # åŸºæœ¬éªŒè¯
                self.assertIsInstance(agents, list)
                self.assertGreater(len(agents), 0)

                # è´¨é‡çº§åˆ«å¯¹åº”çš„æ•°é‡éªŒè¯
                expected_ranges = {
                    QualityLevel.FAST: (3, 6),
                    QualityLevel.BALANCED: (5, 10),
                    QualityLevel.PREMIUM: (8, 15),
                    QualityLevel.ULTIMATE: (12, 20)
                }

                min_agents, max_agents = expected_ranges[quality_level]
                self.assertGreaterEqual(len(agents), min_agents)
                self.assertLessEqual(len(agents), max_agents)

                # éªŒè¯æ²¡æœ‰é‡å¤çš„agents
                self.assertEqual(len(agents), len(set(agents)))

                print(f"  âœ… {task}: {len(agents)} agents ({quality_level.name})")

        print("âœ… Agenté€‰æ‹©æµ‹è¯•é€šè¿‡")

    def test_optimization_planning(self):
        """æµ‹è¯•ä¼˜åŒ–è®¡åˆ’ç”Ÿæˆ"""
        print("\nğŸ§ª æµ‹è¯•ä¼˜åŒ–è®¡åˆ’ç”Ÿæˆ...")

        quality_levels = [
            QualityThreshold.MINIMUM,
            QualityThreshold.GOOD,
            QualityThreshold.EXCELLENT,
            QualityThreshold.PERFECT
        ]

        for quality in quality_levels:
            with self.subTest(quality=quality):
                plan = self.optimizer.optimize_execution(
                    task_description=self.test_task,
                    target_quality=quality,
                    optimization_level=OptimizationLevel.OPUS41
                )

                # åŸºæœ¬éªŒè¯
                self.assertIsNotNone(plan)
                self.assertEqual(plan.task_description, self.test_task)
                self.assertEqual(plan.target_quality, quality)
                self.assertEqual(plan.optimization_level, OptimizationLevel.OPUS41)

                # æ‰§è¡Œå±‚éªŒè¯
                self.assertGreater(len(plan.execution_layers), 0)
                self.assertLessEqual(len(plan.execution_layers), 5)

                # èµ„æºéœ€æ±‚éªŒè¯
                self.assertIn('total_agents', plan.resource_requirements)
                self.assertIn('concurrent_agents', plan.resource_requirements)
                self.assertGreater(plan.resource_requirements['total_agents'], 0)

                # æˆåŠŸæ¦‚ç‡éªŒè¯
                self.assertGreaterEqual(plan.success_probability, 0.1)
                self.assertLessEqual(plan.success_probability, 1.0)

                # æ—¶é—´ä¼°ç®—éªŒè¯
                self.assertGreater(plan.estimated_total_time, 0)

                print(f"  âœ… {quality.name}: {plan.resource_requirements['total_agents']} agents, "
                      f"{plan.estimated_total_time}min, {plan.success_probability:.1%} success")

        print("âœ… ä¼˜åŒ–è®¡åˆ’ç”Ÿæˆæµ‹è¯•é€šè¿‡")

    def test_task_calls_generation(self):
        """æµ‹è¯•Taskè°ƒç”¨ç”Ÿæˆ"""
        print("\nğŸ§ª æµ‹è¯•Taskè°ƒç”¨ç”Ÿæˆ...")

        plan = self.optimizer.optimize_execution(
            task_description=self.test_task,
            target_quality=QualityThreshold.EXCELLENT,
            optimization_level=OptimizationLevel.OPUS41
        )

        task_calls = self.optimizer.generate_task_calls(plan)

        # åŸºæœ¬éªŒè¯
        self.assertIsInstance(task_calls, list)
        self.assertGreater(len(task_calls), 0)

        # éªŒè¯æ¯ä¸ªTaskè°ƒç”¨çš„ç»“æ„
        for i, call in enumerate(task_calls):
            with self.subTest(task_call=i):
                self.assertIn('tool_name', call)
                self.assertEqual(call['tool_name'], 'Task')

                self.assertIn('parameters', call)
                params = call['parameters']

                self.assertIn('subagent_type', params)
                self.assertIn('description', params)
                self.assertIn('prompt', params)

                # éªŒè¯promptä¸ä¸ºç©º
                self.assertGreater(len(params['prompt'].strip()), 0)

                # éªŒè¯layerä¿¡æ¯
                self.assertIn('layer_id', call)
                self.assertIn('layer_name', call)

        print(f"âœ… ç”Ÿæˆäº† {len(task_calls)} ä¸ªTaskè°ƒç”¨")

    def test_execution_simulation(self):
        """æµ‹è¯•æ‰§è¡Œæ¨¡æ‹Ÿ"""
        print("\nğŸ§ª æµ‹è¯•æ‰§è¡Œæ¨¡æ‹Ÿ...")

        plan = self.optimizer.optimize_execution(
            task_description=self.test_task,
            target_quality=QualityThreshold.GOOD,
            optimization_level=OptimizationLevel.OPUS41
        )

        # æ‰§è¡Œè®¡åˆ’
        result = self.optimizer.execute_optimized_plan(plan)

        # éªŒè¯æ‰§è¡Œç»“æœ
        self.assertIsInstance(result, dict)
        self.assertIn('success', result)
        self.assertIn('execution_time', result)
        self.assertIn('final_quality', result)
        self.assertIn('layers_completed', result)

        # å¦‚æœæ‰§è¡ŒæˆåŠŸï¼ŒéªŒè¯è´¨é‡æŒ‡æ ‡
        if result['success']:
            self.assertGreaterEqual(result['final_quality'], 0.0)
            self.assertLessEqual(result['final_quality'], 1.0)
            self.assertGreater(result['layers_completed'], 0)

            print(f"âœ… æ‰§è¡ŒæˆåŠŸ: è´¨é‡={result['final_quality']:.1%}, "
                  f"æ—¶é—´={result['execution_time']:.2f}s, "
                  f"å±‚æ•°={result['layers_completed']}")
        else:
            print(f"âš ï¸ æ‰§è¡Œå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    def test_visualization_system(self):
        """æµ‹è¯•å¯è§†åŒ–ç³»ç»Ÿ"""
        print("\nğŸ§ª æµ‹è¯•å¯è§†åŒ–ç³»ç»Ÿ...")

        # å¯åŠ¨ç›‘æ§
        self.visualizer.start_real_time_monitoring({
            "task": "æµ‹è¯•ä»»åŠ¡",
            "start_time": datetime.now().isoformat()
        })

        # æ›´æ–°å‡ æ¬¡æŒ‡æ ‡
        test_metrics = [
            (0.5, 0.6, 30, 3, {1: 0.5}, {"agent1": "running"}),
            (0.7, 0.8, 60, 5, {1: 1.0, 2: 0.5}, {"agent1": "completed", "agent2": "running"}),
            (0.85, 0.9, 90, 4, {1: 1.0, 2: 1.0, 3: 0.3}, {"agent1": "completed", "agent2": "completed"})
        ]

        for quality, success, time, agents, layers, agent_status in test_metrics:
            self.visualizer.update_metrics(
                quality_score=quality,
                success_rate=success,
                execution_time=time,
                active_agents=agents,
                layer_progress=layers,
                agent_status=agent_status
            )

        # éªŒè¯æŒ‡æ ‡å†å²
        self.assertEqual(len(self.visualizer.metrics_history), len(test_metrics))

        # ç”ŸæˆæŠ¥å‘Š
        report = self.visualizer.generate_performance_report()
        self.assertIsInstance(report, dict)
        self.assertIn('summary', report)
        self.assertIn('quality_metrics', report)
        self.assertIn('performance_metrics', report)

        # åœæ­¢ç›‘æ§
        self.visualizer.stop_monitoring()

        print("âœ… å¯è§†åŒ–ç³»ç»Ÿæµ‹è¯•é€šè¿‡")

    def test_html_dashboard_generation(self):
        """æµ‹è¯•HTML Dashboardç”Ÿæˆ"""
        print("\nğŸ§ª æµ‹è¯•HTML Dashboardç”Ÿæˆ...")

        # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
        self.visualizer.start_real_time_monitoring({"test": "data"})
        self.visualizer.update_metrics(
            quality_score=0.85,
            success_rate=0.9,
            execution_time=120,
            active_agents=8,
            layer_progress={1: 1.0, 2: 0.8, 3: 0.5},
            agent_status={"test-agent": "running"}
        )

        # ç”ŸæˆDashboard
        dashboard_file = self.visualizer.generate_html_dashboard("test_dashboard.html")

        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        self.assertTrue(os.path.exists(dashboard_file))

        # éªŒè¯æ–‡ä»¶å†…å®¹
        with open(dashboard_file, 'r', encoding='utf-8') as f:
            content = f.read()
            self.assertIn('<html', content)
            self.assertIn('Opus41', content)
            self.assertIn('è´¨é‡åˆ†æ•°', content)

        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if os.path.exists(dashboard_file):
            os.remove(dashboard_file)

        self.visualizer.stop_monitoring()

        print("âœ… HTML Dashboardç”Ÿæˆæµ‹è¯•é€šè¿‡")

    def test_status_and_metrics(self):
        """æµ‹è¯•çŠ¶æ€å’ŒæŒ‡æ ‡"""
        print("\nğŸ§ª æµ‹è¯•çŠ¶æ€å’ŒæŒ‡æ ‡...")

        # è·å–ä¼˜åŒ–å™¨çŠ¶æ€
        status = self.optimizer.get_optimization_status()

        # éªŒè¯çŠ¶æ€ç»“æ„
        self.assertIsInstance(status, dict)
        self.assertIn('agent_count', status)
        self.assertIn('execution_history_count', status)
        self.assertIn('system_status', status)
        self.assertIn('max_parallel_agents', status)
        self.assertIn('quality_threshold', status)

        # éªŒè¯å€¼çš„åˆç†æ€§
        self.assertGreaterEqual(status['agent_count'], 0)
        self.assertGreaterEqual(status['execution_history_count'], 0)
        self.assertEqual(status['system_status'], 'operational')

        print(f"âœ… ç³»ç»ŸçŠ¶æ€æ­£å¸¸: {status['agent_count']} agents, "
              f"å†å²è®°å½• {status['execution_history_count']} æ¡")

    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        print("\nğŸ§ª æµ‹è¯•é”™è¯¯å¤„ç†...")

        # æµ‹è¯•ç©ºä»»åŠ¡
        with self.assertRaises(Exception):
            self.optimizer.optimize_execution(
                task_description="",
                target_quality=QualityThreshold.EXCELLENT,
                optimization_level=OptimizationLevel.OPUS41
            )

        # æµ‹è¯•æ— æ•ˆå‚æ•°ï¼ˆè¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿä¸€ä¸‹ï¼Œå®é™…å®ç°å¯èƒ½ä¸ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
        try:
            plan = self.optimizer.optimize_execution(
                task_description="ç®€å•æµ‹è¯•",
                target_quality=QualityThreshold.MINIMUM,
                optimization_level=OptimizationLevel.BASIC
            )
            # å¦‚æœæ²¡æœ‰æŠ›å‡ºå¼‚å¸¸ï¼ŒéªŒè¯è¿”å›çš„è®¡åˆ’æ˜¯æœ‰æ•ˆçš„
            self.assertIsNotNone(plan)
        except Exception as e:
            print(f"  å¤„ç†äº†å¼‚å¸¸: {e}")

        print("âœ… é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")

    def test_performance_benchmark(self):
        """æµ‹è¯•æ€§èƒ½åŸºå‡†"""
        print("\nğŸ§ª æµ‹è¯•æ€§èƒ½åŸºå‡†...")

        # æµ‹è¯•ä¸åŒè§„æ¨¡çš„ä»»åŠ¡
        test_tasks = [
            ("ç®€å•ä»»åŠ¡", QualityLevel.FAST),
            ("ä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡", QualityLevel.BALANCED),
            ("å¤æ‚ä¼ä¸šçº§ä»»åŠ¡", QualityLevel.PREMIUM)
        ]

        for task, quality in test_tasks:
            start_time = time.time()

            # Agenté€‰æ‹©æ€§èƒ½
            agents = self.optimizer.select_optimal_agents(task, quality)
            selection_time = time.time() - start_time

            # ä¼˜åŒ–è§„åˆ’æ€§èƒ½
            plan_start = time.time()
            plan = self.optimizer.optimize_execution(
                task_description=task,
                target_quality=QualityThreshold.GOOD,
                optimization_level=OptimizationLevel.OPUS41
            )
            planning_time = time.time() - plan_start

            # éªŒè¯æ€§èƒ½æŒ‡æ ‡
            self.assertLess(selection_time, 1.0)  # Agenté€‰æ‹©åº”åœ¨1ç§’å†…å®Œæˆ
            self.assertLess(planning_time, 5.0)   # è§„åˆ’åº”åœ¨5ç§’å†…å®Œæˆ

            print(f"  âœ… {task}: é€‰æ‹©={selection_time:.3f}s, è§„åˆ’={planning_time:.3f}s, "
                  f"agents={len(agents)}")

        print("âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡")

    def test_data_export(self):
        """æµ‹è¯•æ•°æ®å¯¼å‡º"""
        print("\nğŸ§ª æµ‹è¯•æ•°æ®å¯¼å‡º...")

        # ç”Ÿæˆä¸€äº›æµ‹è¯•æ•°æ®
        self.visualizer.start_real_time_monitoring({"test": "export"})
        self.visualizer.update_metrics(
            quality_score=0.9,
            success_rate=0.95,
            execution_time=150,
            active_agents=10,
            layer_progress={1: 1.0, 2: 1.0, 3: 0.8},
            agent_status={"agent1": "completed", "agent2": "running"}
        )

        # å¯¼å‡ºJSONæ•°æ®
        export_file = self.visualizer.export_metrics_to_json("test_export.json")

        # éªŒè¯å¯¼å‡ºæ–‡ä»¶
        self.assertTrue(os.path.exists(export_file))

        with open(export_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.assertIn('export_time', data)
            self.assertIn('metrics_count', data)
            self.assertIn('dashboard_data', data)
            self.assertIn('metrics_history', data)

        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if os.path.exists(export_file):
            os.remove(export_file)

        self.visualizer.stop_monitoring()

        print("âœ… æ•°æ®å¯¼å‡ºæµ‹è¯•é€šè¿‡")

class TestOpus41CLI(unittest.TestCase):
    """CLIæµ‹è¯•"""

    def test_cli_import(self):
        """æµ‹è¯•CLIæ¨¡å—å¯¼å…¥"""
        print("\nğŸ§ª æµ‹è¯•CLIæ¨¡å—å¯¼å…¥...")

        try:
            from main.cli_opus41 import (
                create_opus41_parser,
                handle_opus41_command
            )
            print("âœ… CLIæ¨¡å—å¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            self.fail(f"CLIæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("ğŸš€ Perfect21 Opus41 æ™ºèƒ½å¹¶è¡Œä¼˜åŒ–å™¨ - ç»¼åˆæµ‹è¯•")
    print("=" * 80)

    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()

    # æ·»åŠ é›†æˆæµ‹è¯•
    integration_tests = [
        'test_optimizer_initialization',
        'test_agent_selection',
        'test_optimization_planning',
        'test_task_calls_generation',
        'test_execution_simulation',
        'test_visualization_system',
        'test_html_dashboard_generation',
        'test_status_and_metrics',
        'test_error_handling',
        'test_performance_benchmark',
        'test_data_export'
    ]

    for test_method in integration_tests:
        test_suite.addTest(TestOpus41Integration(test_method))

    # æ·»åŠ CLIæµ‹è¯•
    test_suite.addTest(TestOpus41CLI('test_cli_import'))

    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)

    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    test_report = {
        "test_time": datetime.now().isoformat(),
        "total_tests": result.testsRun,
        "failures": len(result.failures),
        "errors": len(result.errors),
        "success_rate": (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun if result.testsRun > 0 else 0,
        "test_results": {
            "passed": result.testsRun - len(result.failures) - len(result.errors),
            "failed": len(result.failures),
            "errors": len(result.errors)
        }
    }

    # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    report_file = f"opus41_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(test_report, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“Š æµ‹è¯•å®Œæˆ!")
    print(f"æ€»æµ‹è¯•æ•°: {test_report['total_tests']}")
    print(f"é€šè¿‡: {test_report['test_results']['passed']}")
    print(f"å¤±è´¥: {test_report['test_results']['failed']}")
    print(f"é”™è¯¯: {test_report['test_results']['errors']}")
    print(f"æˆåŠŸç‡: {test_report['success_rate']:.1%}")
    print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    return result.wasSuccessful()

if __name__ == '__main__':
    success = run_comprehensive_test()
    sys.exit(0 if success else 1)