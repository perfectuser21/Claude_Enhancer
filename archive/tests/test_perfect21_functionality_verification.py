#!/usr/bin/env python3
"""
Perfect21 åŠŸèƒ½æµ‹è¯•éªŒè¯è„šæœ¬
æµ‹è¯•Perfect21æ ¸å¿ƒåŠŸèƒ½æ¨¡å—çš„è¿è¡ŒçŠ¶æ€å’Œé›†æˆæ•ˆæœ

æµ‹è¯•ç›®æ ‡ï¼š
1. 13ä¸ªå·¥ä½œæµæ¨¡æ¿çš„æ­£å¸¸å·¥ä½œçŠ¶æ€
2. åŒæ­¥ç‚¹æœºåˆ¶çš„æŒ‰é¢„æœŸæ‰§è¡Œ
3. è´¨é‡é—¨æ£€æŸ¥çš„æœ‰æ•ˆæ€§
4. é”™è¯¯æ¢å¤æœºåˆ¶çš„å¯é æ€§
5. Git Hooksé›†æˆçš„å®Œæ•´æ€§
6. èƒ½åŠ›å‘ç°ç³»ç»Ÿçš„å‡†ç¡®æ€§
"""

import sys
import os
import json
import time
import asyncio
import logging
import traceback
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# æµ‹è¯•ç»“æœæ•°æ®ç±»
@dataclass
class TestResult:
    test_name: str
    success: bool
    duration: float
    details: Dict[str, Any]
    error: Optional[str] = None

class Perfect21FunctionalityValidator:
    """Perfect21åŠŸèƒ½éªŒè¯å™¨"""

    def __init__(self):
        self.logger = self._setup_logger()
        self.test_results: List[TestResult] = []
        self.start_time = time.time()

        # æµ‹è¯•æŠ¥å‘Šé…ç½®
        self.report_file = project_root / "perfect21_functionality_test_report.json"
        self.dashboard_file = project_root / "perfect21_functionality_dashboard.html"

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger("Perfect21Validator")
        logger.setLevel(logging.INFO)

        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)

        return logger

    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰åŠŸèƒ½æµ‹è¯•"""
        self.logger.info("ğŸš€ å¼€å§‹Perfect21åŠŸèƒ½éªŒè¯æµ‹è¯•")
        self.logger.info("=" * 80)

        # å®šä¹‰æµ‹è¯•å¥—ä»¶
        test_suite = [
            ("å·¥ä½œæµæ¨¡æ¿æµ‹è¯•", self.test_workflow_templates),
            ("åŒæ­¥ç‚¹æœºåˆ¶æµ‹è¯•", self.test_sync_point_mechanism),
            ("è´¨é‡é—¨æ£€æŸ¥æµ‹è¯•", self.test_quality_gate_checks),
            ("é”™è¯¯æ¢å¤æœºåˆ¶æµ‹è¯•", self.test_error_recovery_mechanism),
            ("Git Hooksé›†æˆæµ‹è¯•", self.test_git_hooks_integration),
            ("èƒ½åŠ›å‘ç°ç³»ç»Ÿæµ‹è¯•", self.test_capability_discovery_system),
            ("å·¥ä½œæµç¼–æ’å™¨æµ‹è¯•", self.test_workflow_orchestrator),
            ("å¹¶è¡Œæ‰§è¡Œæµ‹è¯•", self.test_parallel_execution),
            ("å†³ç­–è®°å½•æµ‹è¯•", self.test_decision_recording),
            ("è´¨é‡å®ˆæŠ¤æµ‹è¯•", self.test_quality_guardian),
            ("å¤šå·¥ä½œç©ºé—´æµ‹è¯•", self.test_multi_workspace),
            ("å­¦ä¹ åé¦ˆæµ‹è¯•", self.test_learning_feedback)
        ]

        # æ‰§è¡Œæµ‹è¯•
        total_tests = len(test_suite)
        passed_tests = 0

        for i, (test_name, test_func) in enumerate(test_suite, 1):
            self.logger.info(f"\nğŸ“‹ [{i}/{total_tests}] æ‰§è¡Œæµ‹è¯•: {test_name}")
            self.logger.info("-" * 60)

            start_time = time.time()

            try:
                result = await test_func()
                duration = time.time() - start_time

                if result.get('success', False):
                    passed_tests += 1
                    self.logger.info(f"âœ… {test_name} é€šè¿‡ (è€—æ—¶: {duration:.2f}s)")
                else:
                    self.logger.error(f"âŒ {test_name} å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                # è®°å½•æµ‹è¯•ç»“æœ
                test_result = TestResult(
                    test_name=test_name,
                    success=result.get('success', False),
                    duration=duration,
                    details=result,
                    error=result.get('error') if not result.get('success', False) else None
                )
                self.test_results.append(test_result)

            except Exception as e:
                duration = time.time() - start_time
                error_msg = f"æµ‹è¯•å¼‚å¸¸: {str(e)}"
                self.logger.error(f"ğŸ’¥ {test_name} å¼‚å¸¸: {error_msg}")

                test_result = TestResult(
                    test_name=test_name,
                    success=False,
                    duration=duration,
                    details={"exception": str(e), "traceback": traceback.format_exc()},
                    error=error_msg
                )
                self.test_results.append(test_result)

        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        total_duration = time.time() - self.start_time
        overall_success = passed_tests == total_tests

        summary = {
            'overall_success': overall_success,
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': total_tests - passed_tests,
            'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0,
            'total_duration': total_duration,
            'timestamp': datetime.now().isoformat()
        }

        self.logger.info(f"\nğŸ¯ æµ‹è¯•å®Œæˆæ€»ç»“:")
        self.logger.info(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        self.logger.info(f"   é€šè¿‡æµ‹è¯•: {passed_tests}")
        self.logger.info(f"   å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
        self.logger.info(f"   æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        self.logger.info(f"   æ€»è€—æ—¶: {total_duration:.2f}s")

        # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        await self.generate_test_report(summary)

        return summary

    async def test_workflow_templates(self) -> Dict[str, Any]:
        """æµ‹è¯•å·¥ä½œæµæ¨¡æ¿"""
        try:
            from features.workflow_orchestrator.orchestrator import WorkflowOrchestrator

            orchestrator = WorkflowOrchestrator(self.logger)

            # æµ‹è¯•ä¸åŒç±»å‹çš„å·¥ä½œæµæ¨¡æ¿
            workflow_templates = [
                {
                    'name': 'Premium Quality Workflow',
                    'stages': [
                        {
                            'name': 'analysis',
                            'description': 'æ·±åº¦åˆ†æé˜¶æ®µ',
                            'execution_mode': 'parallel',
                            'sync_point': {
                                'type': 'consensus_check',
                                'validation_criteria': {
                                    'tasks_completed': '> 0'
                                }
                            }
                        },
                        {
                            'name': 'design',
                            'description': 'è®¾è®¡é˜¶æ®µ',
                            'execution_mode': 'sequential',
                            'depends_on': ['analysis'],
                            'quality_gate': {
                                'checklist': 'architecture_review,security_check'
                            }
                        }
                    ]
                },
                {
                    'name': 'Quick Development Workflow',
                    'stages': [
                        {
                            'name': 'implementation',
                            'description': 'å¿«é€Ÿå®ç°',
                            'execution_mode': 'parallel'
                        }
                    ]
                }
            ]

            template_results = []

            for template in workflow_templates:
                load_result = orchestrator.load_workflow(template)

                if load_result['success']:
                    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
                    task_result = orchestrator.create_task(
                        agent='test-agent',
                        description='æµ‹è¯•ä»»åŠ¡',
                        stage=template['stages'][0]['name']
                    )

                    template_results.append({
                        'template_name': template['name'],
                        'loaded': load_result['success'],
                        'task_created': task_result.get('success', False),
                        'stages_count': load_result.get('stages_count', 0)
                    })
                else:
                    template_results.append({
                        'template_name': template['name'],
                        'loaded': False,
                        'error': load_result.get('error', 'Unknown error')
                    })

            success = all(result.get('loaded', False) for result in template_results)

            return {
                'success': success,
                'templates_tested': len(workflow_templates),
                'templates_passed': len([r for r in template_results if r.get('loaded', False)]),
                'template_results': template_results
            }

        except Exception as e:
            return {
                'success': False,
                'error': f"å·¥ä½œæµæ¨¡æ¿æµ‹è¯•å¼‚å¸¸: {str(e)}",
                'traceback': traceback.format_exc()
            }

    async def test_sync_point_mechanism(self) -> Dict[str, Any]:
        """æµ‹è¯•åŒæ­¥ç‚¹æœºåˆ¶"""
        try:
            from features.sync_point_manager.sync_manager import SyncPointManager

            sync_manager = SyncPointManager()

            # æµ‹è¯•ä¸åŒç±»å‹çš„åŒæ­¥ç‚¹
            sync_points = [
                {
                    'type': 'consensus_check',
                    'validation_criteria': {
                        'agreement_percentage': '> 80',
                        'conflicts_resolved': 'true'
                    }
                },
                {
                    'type': 'quality_verification',
                    'validation_criteria': {
                        'test_coverage': '> 90',
                        'code_quality_score': '> 8.5'
                    }
                },
                {
                    'type': 'integration_checkpoint',
                    'validation_criteria': {
                        'api_tests_passed': 'true',
                        'dependencies_resolved': 'true'
                    }
                }
            ]

            sync_results = []

            for sync_point in sync_points:
                # æ¨¡æ‹ŸéªŒè¯æ•°æ®
                mock_validation_data = {
                    'agreement_percentage': 85,
                    'conflicts_resolved': True,
                    'test_coverage': 95,
                    'code_quality_score': 9.2,
                    'api_tests_passed': True,
                    'dependencies_resolved': True
                }

                result = sync_manager.validate_sync_point(sync_point, mock_validation_data)

                sync_results.append({
                    'sync_type': sync_point['type'],
                    'validation_passed': result.get('success', False),
                    'criteria_met': result.get('all_criteria_met', False),
                    'failed_criteria_count': len(result.get('failed_criteria', []))
                })

            success = all(result.get('validation_passed', False) for result in sync_results)

            return {
                'success': success,
                'sync_points_tested': len(sync_points),
                'sync_points_passed': len([r for r in sync_results if r.get('validation_passed', False)]),
                'sync_results': sync_results
            }

        except Exception as e:
            return {
                'success': False,
                'error': f"åŒæ­¥ç‚¹æœºåˆ¶æµ‹è¯•å¼‚å¸¸: {str(e)}",
                'traceback': traceback.format_exc()
            }

    async def test_quality_gate_checks(self) -> Dict[str, Any]:
        """æµ‹è¯•è´¨é‡é—¨æ£€æŸ¥"""
        try:
            from features.quality_gates.quality_guardian import QualityGuardian

            quality_guardian = QualityGuardian()

            # æµ‹è¯•ä¸åŒçš„è´¨é‡é—¨
            quality_gates = [
                {
                    'name': 'code_quality',
                    'checks': [
                        {'type': 'code_coverage', 'threshold': 90},
                        {'type': 'complexity', 'max_value': 10},
                        {'type': 'security_scan', 'required': True}
                    ]
                },
                {
                    'name': 'performance',
                    'checks': [
                        {'type': 'response_time', 'max_ms': 200},
                        {'type': 'memory_usage', 'max_mb': 512},
                        {'type': 'cpu_usage', 'max_percent': 80}
                    ]
                }
            ]

            quality_results = []

            for gate in quality_gates:
                # æ¨¡æ‹Ÿè´¨é‡æ£€æŸ¥æ•°æ®
                mock_quality_data = {
                    'code_coverage': 95,
                    'complexity': 8,
                    'security_scan': True,
                    'response_time': 150,
                    'memory_usage': 256,
                    'cpu_usage': 60
                }

                result = quality_guardian.check_quality_gate(gate, mock_quality_data)

                quality_results.append({
                    'gate_name': gate['name'],
                    'passed': result.get('passed', False),
                    'checks_total': len(gate['checks']),
                    'checks_passed': result.get('checks_passed', 0),
                    'quality_score': result.get('quality_score', 0)
                })

            success = all(result.get('passed', False) for result in quality_results)

            return {
                'success': success,
                'quality_gates_tested': len(quality_gates),
                'quality_gates_passed': len([r for r in quality_results if r.get('passed', False)]),
                'quality_results': quality_results
            }

        except Exception as e:
            return {
                'success': False,
                'error': f"è´¨é‡é—¨æ£€æŸ¥æµ‹è¯•å¼‚å¸¸: {str(e)}",
                'traceback': traceback.format_exc()
            }

    async def test_error_recovery_mechanism(self) -> Dict[str, Any]:
        """æµ‹è¯•é”™è¯¯æ¢å¤æœºåˆ¶"""
        try:
            from features.workflow_orchestrator.orchestrator import WorkflowOrchestrator

            orchestrator = WorkflowOrchestrator(self.logger)

            # åˆ›å»ºæµ‹è¯•å·¥ä½œæµ
            test_workflow = {
                'name': 'Error Recovery Test',
                'stages': [
                    {
                        'name': 'test_stage',
                        'description': 'é”™è¯¯æ¢å¤æµ‹è¯•é˜¶æ®µ',
                        'execution_mode': 'parallel'
                    }
                ]
            }

            load_result = orchestrator.load_workflow(test_workflow)
            if not load_result['success']:
                return {
                    'success': False,
                    'error': f"æ— æ³•åŠ è½½æµ‹è¯•å·¥ä½œæµ: {load_result.get('error')}"
                }

            # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
            task_result = orchestrator.create_task(
                agent='error-test-agent',
                description='é”™è¯¯æµ‹è¯•ä»»åŠ¡',
                stage='test_stage'
            )

            if not task_result['success']:
                return {
                    'success': False,
                    'error': f"æ— æ³•åˆ›å»ºæµ‹è¯•ä»»åŠ¡: {task_result.get('error')}"
                }

            task_id = task_result['task_id']

            # æµ‹è¯•ä¸åŒç±»å‹çš„é”™è¯¯æ¢å¤
            error_scenarios = [
                {
                    'error_type': 'timeout',
                    'expected_action': 'retry'
                },
                {
                    'error_type': 'validation_error',
                    'expected_action': 'fail'
                },
                {
                    'error_type': 'network_error',
                    'expected_action': 'retry'
                }
            ]

            recovery_results = []

            for scenario in error_scenarios:
                error_result = {
                    'success': False,
                    'error_type': scenario['error_type'],
                    'error': f"æ¨¡æ‹Ÿ{scenario['error_type']}é”™è¯¯"
                }

                recovery_result = orchestrator.handle_task_error(task_id, error_result)

                recovery_results.append({
                    'error_type': scenario['error_type'],
                    'expected_action': scenario['expected_action'],
                    'actual_action': recovery_result.get('action', 'unknown'),
                    'recovery_successful': recovery_result.get('success', False),
                    'action_matches_expected': recovery_result.get('action') == scenario['expected_action']
                })

            success = all(result.get('action_matches_expected', False) for result in recovery_results)

            return {
                'success': success,
                'error_scenarios_tested': len(error_scenarios),
                'recovery_strategies_correct': len([r for r in recovery_results if r.get('action_matches_expected', False)]),
                'recovery_results': recovery_results
            }

        except Exception as e:
            return {
                'success': False,
                'error': f"é”™è¯¯æ¢å¤æœºåˆ¶æµ‹è¯•å¼‚å¸¸: {str(e)}",
                'traceback': traceback.format_exc()
            }

    async def test_git_hooks_integration(self) -> Dict[str, Any]:
        """æµ‹è¯•Git Hooksé›†æˆ"""
        try:
            from features.git_workflow.hooks import GitHookManager

            hook_manager = GitHookManager()

            # æ£€æŸ¥Git Hooksé…ç½®
            hooks_status = hook_manager.get_hooks_status()

            # æµ‹è¯•ç‰¹å®šé’©å­
            test_hooks = [
                'pre-commit',
                'commit-msg',
                'pre-push',
                'post-commit',
                'post-merge'
            ]

            hooks_results = []

            for hook_name in test_hooks:
                # æ£€æŸ¥é’©å­æ˜¯å¦å®‰è£…å’Œé…ç½®
                hook_installed = hook_manager.is_hook_installed(hook_name)
                hook_enabled = hook_manager.is_hook_enabled(hook_name)

                # æµ‹è¯•é’©å­é…ç½®
                hook_config = hook_manager.get_hook_config(hook_name)

                hooks_results.append({
                    'hook_name': hook_name,
                    'installed': hook_installed,
                    'enabled': hook_enabled,
                    'configured': bool(hook_config),
                    'config_valid': self._validate_hook_config(hook_config)
                })

            # æ£€æŸ¥hooksçš„æ•´ä½“çŠ¶æ€
            hooks_working = all(
                result.get('installed', False) and result.get('enabled', False)
                for result in hooks_results
            )

            return {
                'success': hooks_working,
                'total_hooks': len(test_hooks),
                'working_hooks': len([r for r in hooks_results if r.get('installed', False) and r.get('enabled', False)]),
                'hooks_status': hooks_status,
                'hooks_results': hooks_results,
                'integration_complete': len(hooks_results) >= 5  # è‡³å°‘5ä¸ªé’©å­å·¥ä½œ
            }

        except Exception as e:
            return {
                'success': False,
                'error': f"Git Hooksé›†æˆæµ‹è¯•å¼‚å¸¸: {str(e)}",
                'traceback': traceback.format_exc()
            }

    async def test_capability_discovery_system(self) -> Dict[str, Any]:
        """æµ‹è¯•èƒ½åŠ›å‘ç°ç³»ç»Ÿ"""
        try:
            from features.capability_discovery.registry import CapabilityRegistry

            registry = CapabilityRegistry()

            # æµ‹è¯•èƒ½åŠ›å‘ç°
            discovery_result = registry.discover_capabilities()

            # æ£€æŸ¥æ ¸å¿ƒèƒ½åŠ›æ˜¯å¦è¢«å‘ç°
            expected_capabilities = [
                'workflow_orchestration',
                'sync_point_management',
                'quality_gates',
                'error_recovery',
                'git_integration',
                'parallel_execution'
            ]

            discovered_capabilities = discovery_result.get('capabilities', [])
            capability_names = [cap.get('name', '') for cap in discovered_capabilities]

            capability_results = []

            for expected_cap in expected_capabilities:
                found = any(expected_cap in name.lower() for name in capability_names)

                if found:
                    # è·å–è¯¦ç»†ä¿¡æ¯
                    cap_details = next(
                        (cap for cap in discovered_capabilities
                         if expected_cap in cap.get('name', '').lower()),
                        {}
                    )

                    capability_results.append({
                        'capability_name': expected_cap,
                        'discovered': True,
                        'version': cap_details.get('version', 'unknown'),
                        'status': cap_details.get('status', 'unknown'),
                        'features_count': len(cap_details.get('features', []))
                    })
                else:
                    capability_results.append({
                        'capability_name': expected_cap,
                        'discovered': False
                    })

            # æµ‹è¯•èƒ½åŠ›æ³¨å†Œ
            test_capability = {
                'name': 'test_capability',
                'version': '1.0.0',
                'status': 'active',
                'features': ['test_feature_1', 'test_feature_2']
            }

            registration_result = registry.register_capability(test_capability)

            success = (
                discovery_result.get('success', False) and
                len(capability_results) >= len(expected_capabilities) * 0.8 and  # 80%çš„èƒ½åŠ›è¢«å‘ç°
                registration_result.get('success', False)
            )

            return {
                'success': success,
                'total_expected_capabilities': len(expected_capabilities),
                'discovered_capabilities': len([r for r in capability_results if r.get('discovered', False)]),
                'discovery_rate': len([r for r in capability_results if r.get('discovered', False)]) / len(expected_capabilities) * 100,
                'capability_results': capability_results,
                'registration_working': registration_result.get('success', False),
                'total_capabilities_in_registry': len(discovered_capabilities)
            }

        except Exception as e:
            return {
                'success': False,
                'error': f"èƒ½åŠ›å‘ç°ç³»ç»Ÿæµ‹è¯•å¼‚å¸¸: {str(e)}",
                'traceback': traceback.format_exc()
            }

    async def test_workflow_orchestrator(self) -> Dict[str, Any]:
        """æµ‹è¯•å·¥ä½œæµç¼–æ’å™¨æ ¸å¿ƒåŠŸèƒ½"""
        try:
            from features.workflow_orchestrator.orchestrator import WorkflowOrchestrator

            orchestrator = WorkflowOrchestrator(self.logger)

            # æµ‹è¯•ç¼–æ’å™¨æ ¸å¿ƒåŠŸèƒ½
            test_workflow = {
                'name': 'Orchestrator Core Test',
                'stages': [
                    {
                        'name': 'parallel_stage',
                        'description': 'å¹¶è¡Œæµ‹è¯•é˜¶æ®µ',
                        'execution_mode': 'parallel'
                    },
                    {
                        'name': 'sequential_stage',
                        'description': 'é¡ºåºæµ‹è¯•é˜¶æ®µ',
                        'execution_mode': 'sequential',
                        'depends_on': ['parallel_stage']
                    }
                ]
            }

            # æµ‹è¯•å·¥ä½œæµåŠ è½½
            load_result = orchestrator.load_workflow(test_workflow)
            if not load_result['success']:
                return {
                    'success': False,
                    'error': f"å·¥ä½œæµåŠ è½½å¤±è´¥: {load_result.get('error')}"
                }

            # æµ‹è¯•ä»»åŠ¡åˆ›å»º
            task_results = []
            for i in range(3):
                task_result = orchestrator.create_task(
                    agent=f'test-agent-{i}',
                    description=f'æµ‹è¯•ä»»åŠ¡ {i}',
                    stage='parallel_stage'
                )
                task_results.append(task_result.get('success', False))

            # æµ‹è¯•é˜¶æ®µæ‰§è¡Œè®¡åˆ’
            plan_result = orchestrator.plan_stage_execution('parallel_stage')

            # æµ‹è¯•è¿›åº¦è·Ÿè¸ª
            progress = orchestrator.get_workflow_progress()

            # æµ‹è¯•å·¥ä½œæµçŠ¶æ€
            metrics = orchestrator.get_execution_metrics()
            execution_log = orchestrator.get_execution_log()

            success = (
                load_result['success'] and
                all(task_results) and
                plan_result.get('success', False) and
                progress.get('completion_percentage') is not None
            )

            return {
                'success': success,
                'workflow_loaded': load_result['success'],
                'tasks_created': len([r for r in task_results if r]),
                'execution_plan_created': plan_result.get('success', False),
                'progress_tracking_working': progress.get('completion_percentage') is not None,
                'metrics_available': bool(metrics),
                'execution_log_working': isinstance(execution_log, list),
                'stages_configured': load_result.get('stages_count', 0)
            }

        except Exception as e:
            return {
                'success': False,
                'error': f"å·¥ä½œæµç¼–æ’å™¨æµ‹è¯•å¼‚å¸¸: {str(e)}",
                'traceback': traceback.format_exc()
            }

    async def test_parallel_execution(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶è¡Œæ‰§è¡ŒåŠŸèƒ½"""
        try:
            # è¿™é‡Œåº”è¯¥æµ‹è¯•å¹¶è¡Œæ‰§è¡Œèƒ½åŠ›
            # ç”±äºéœ€è¦çœŸå®çš„agentæ‰§è¡Œç¯å¢ƒï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿæµ‹è¯•

            from features.parallel_executor import ParallelExecutor

            executor = ParallelExecutor()

            # åˆ›å»ºæ¨¡æ‹Ÿä»»åŠ¡
            test_tasks = [
                {'id': f'task_{i}', 'agent': f'agent_{i}', 'description': f'å¹¶è¡Œä»»åŠ¡ {i}'}
                for i in range(5)
            ]

            # æµ‹è¯•å¹¶è¡Œæ‰§è¡Œ
            start_time = time.time()
            execution_result = await executor.execute_parallel_tasks(test_tasks)
            execution_time = time.time() - start_time

            # éªŒè¯å¹¶è¡Œæ‰§è¡Œæ•ˆæœ
            sequential_time_estimate = len(test_tasks) * 1.0  # å‡è®¾æ¯ä¸ªä»»åŠ¡1ç§’
            parallel_efficiency = sequential_time_estimate / execution_time if execution_time > 0 else 0

            return {
                'success': execution_result.get('success', False),
                'tasks_executed': execution_result.get('completed_tasks', 0),
                'total_tasks': len(test_tasks),
                'execution_time': execution_time,
                'parallel_efficiency': parallel_efficiency,
                'efficiency_good': parallel_efficiency > 2.0,  # è‡³å°‘2å€åŠ é€Ÿ
                'all_tasks_completed': execution_result.get('completed_tasks', 0) == len(test_tasks)
            }

        except Exception as e:
            return {
                'success': False,
                'error': f"å¹¶è¡Œæ‰§è¡Œæµ‹è¯•å¼‚å¸¸: {str(e)}",
                'traceback': traceback.format_exc()
            }

    async def test_decision_recording(self) -> Dict[str, Any]:
        """æµ‹è¯•å†³ç­–è®°å½•åŠŸèƒ½"""
        try:
            from features.decision_recorder.recorder import DecisionRecorder

            recorder = DecisionRecorder()

            # æµ‹è¯•å†³ç­–è®°å½•
            test_decision = {
                'title': 'æµ‹è¯•æ¶æ„å†³ç­–',
                'context': 'é€‰æ‹©æ•°æ®åº“æŠ€æœ¯',
                'options': ['PostgreSQL', 'MongoDB', 'Redis'],
                'decision': 'PostgreSQL',
                'rationale': 'å…³ç³»å‹æ•°æ®éœ€æ±‚ï¼ŒACIDç‰¹æ€§é‡è¦',
                'consequences': ['æ›´å¥½çš„æ•°æ®ä¸€è‡´æ€§', 'æˆç†Ÿçš„ç”Ÿæ€ç³»ç»Ÿ']
            }

            # è®°å½•å†³ç­–
            record_result = recorder.record_decision(test_decision)

            # æŸ¥è¯¢å†³ç­–
            decisions = recorder.get_decisions()

            # æœç´¢å†³ç­–
            search_result = recorder.search_decisions('æ•°æ®åº“')

            success = (
                record_result.get('success', False) and
                len(decisions) > 0 and
                len(search_result) > 0
            )

            return {
                'success': success,
                'decision_recorded': record_result.get('success', False),
                'decisions_count': len(decisions),
                'search_working': len(search_result) > 0,
                'decision_id': record_result.get('decision_id')
            }

        except Exception as e:
            return {
                'success': False,
                'error': f"å†³ç­–è®°å½•æµ‹è¯•å¼‚å¸¸: {str(e)}",
                'traceback': traceback.format_exc()
            }

    async def test_quality_guardian(self) -> Dict[str, Any]:
        """æµ‹è¯•è´¨é‡å®ˆæŠ¤åŠŸèƒ½"""
        try:
            from features.quality_gates.quality_guardian import QualityGuardian

            guardian = QualityGuardian()

            # æµ‹è¯•é¢„é˜²æ€§è´¨é‡æ£€æŸ¥
            code_sample = """
            def calculate_total(items):
                total = 0
                for item in items:
                    total += item.price
                return total
            """

            quality_check = guardian.analyze_code_quality(code_sample)

            # æµ‹è¯•è´¨é‡è§„åˆ™
            quality_rules = guardian.get_quality_rules()

            # æµ‹è¯•è´¨é‡åº¦é‡
            metrics = {
                'complexity': 5,
                'coverage': 85,
                'duplication': 10,
                'maintainability': 8.5
            }

            quality_score = guardian.calculate_quality_score(metrics)

            success = (
                quality_check.get('success', False) and
                len(quality_rules) > 0 and
                quality_score.get('score', 0) > 0
            )

            return {
                'success': success,
                'code_analysis_working': quality_check.get('success', False),
                'quality_rules_loaded': len(quality_rules),
                'quality_score_calculated': quality_score.get('score', 0) > 0,
                'overall_quality_score': quality_score.get('score', 0)
            }

        except Exception as e:
            return {
                'success': False,
                'error': f"è´¨é‡å®ˆæŠ¤æµ‹è¯•å¼‚å¸¸: {str(e)}",
                'traceback': traceback.format_exc()
            }

    async def test_multi_workspace(self) -> Dict[str, Any]:
        """æµ‹è¯•å¤šå·¥ä½œç©ºé—´åŠŸèƒ½"""
        try:
            from features.multi_workspace.manager import MultiWorkspaceManager

            manager = MultiWorkspaceManager()

            # åˆ›å»ºæµ‹è¯•å·¥ä½œç©ºé—´
            workspace_config = {
                'name': 'test-workspace',
                'path': '/tmp/test-workspace',
                'type': 'development'
            }

            create_result = manager.create_workspace(workspace_config)

            # åˆ‡æ¢å·¥ä½œç©ºé—´
            switch_result = manager.switch_workspace('test-workspace')

            # è·å–å·¥ä½œç©ºé—´åˆ—è¡¨
            workspaces = manager.list_workspaces()

            # è·å–å½“å‰å·¥ä½œç©ºé—´
            current = manager.get_current_workspace()

            success = (
                create_result.get('success', False) and
                switch_result.get('success', False) and
                len(workspaces) > 0 and
                current is not None
            )

            return {
                'success': success,
                'workspace_created': create_result.get('success', False),
                'workspace_switched': switch_result.get('success', False),
                'workspaces_count': len(workspaces),
                'current_workspace_detected': current is not None
            }

        except Exception as e:
            return {
                'success': False,
                'error': f"å¤šå·¥ä½œç©ºé—´æµ‹è¯•å¼‚å¸¸: {str(e)}",
                'traceback': traceback.format_exc()
            }

    async def test_learning_feedback(self) -> Dict[str, Any]:
        """æµ‹è¯•å­¦ä¹ åé¦ˆåŠŸèƒ½"""
        try:
            from features.learning_feedback.learning_loop import LearningLoop

            loop = LearningLoop()

            # è®°å½•æ‰§è¡Œç»éªŒ
            experience = {
                'workflow': 'development_workflow',
                'stage': 'implementation',
                'action': 'parallel_execution',
                'outcome': 'success',
                'duration': 120,
                'quality_score': 8.5,
                'lessons': ['å¹¶è¡Œæ‰§è¡Œæé«˜æ•ˆç‡', 'éœ€è¦æ›´å¥½çš„åŒæ­¥æœºåˆ¶']
            }

            record_result = loop.record_experience(experience)

            # è·å–å­¦ä¹ å»ºè®®
            context = {
                'workflow': 'development_workflow',
                'stage': 'implementation'
            }

            suggestions = loop.get_suggestions(context)

            # æµ‹è¯•æ¨¡å¼è¯†åˆ«
            patterns = loop.identify_patterns()

            success = (
                record_result.get('success', False) and
                len(suggestions) > 0 and
                len(patterns) >= 0
            )

            return {
                'success': success,
                'experience_recorded': record_result.get('success', False),
                'suggestions_generated': len(suggestions),
                'patterns_identified': len(patterns),
                'learning_active': record_result.get('success', False) and len(suggestions) > 0
            }

        except Exception as e:
            return {
                'success': False,
                'error': f"å­¦ä¹ åé¦ˆæµ‹è¯•å¼‚å¸¸: {str(e)}",
                'traceback': traceback.format_exc()
            }

    def _validate_hook_config(self, config: Dict[str, Any]) -> bool:
        """éªŒè¯é’©å­é…ç½®"""
        if not config:
            return False

        required_fields = ['enabled', 'script_path']
        return all(field in config for field in required_fields)

    async def generate_test_report(self, summary: Dict[str, Any]) -> None:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        # ç”ŸæˆJSONæŠ¥å‘Š
        report_data = {
            'summary': summary,
            'test_results': [
                {
                    'test_name': result.test_name,
                    'success': result.success,
                    'duration': result.duration,
                    'details': result.details,
                    'error': result.error
                }
                for result in self.test_results
            ],
            'generated_at': datetime.now().isoformat(),
            'total_duration': summary['total_duration']
        }

        with open(self.report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        self.logger.info(f"ğŸ“„ JSONæŠ¥å‘Šå·²ä¿å­˜: {self.report_file}")

        # ç”ŸæˆHTMLä»ªè¡¨æ¿
        await self.generate_html_dashboard(summary)

    async def generate_html_dashboard(self, summary: Dict[str, Any]) -> None:
        """ç”ŸæˆHTMLæµ‹è¯•ä»ªè¡¨æ¿"""

        html_content = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Perfect21 åŠŸèƒ½éªŒè¯æŠ¥å‘Š</title>
            <style>
                body {{
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                    margin: 0;
                    padding: 20px;
                    background-color: #f5f5f5;
                }}
                .container {{
                    max-width: 1200px;
                    margin: 0 auto;
                    background: white;
                    border-radius: 10px;
                    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
                    overflow: hidden;
                }}
                .header {{
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 30px;
                    text-align: center;
                }}
                .header h1 {{
                    margin: 0;
                    font-size: 2.5em;
                    font-weight: 300;
                }}
                .header p {{
                    margin: 10px 0 0 0;
                    opacity: 0.9;
                }}
                .summary {{
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                    gap: 20px;
                    padding: 30px;
                    background: #f8f9fa;
                }}
                .metric-card {{
                    background: white;
                    padding: 20px;
                    border-radius: 8px;
                    text-align: center;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    border-left: 4px solid #007bff;
                }}
                .metric-card.success {{
                    border-left-color: #28a745;
                }}
                .metric-card.warning {{
                    border-left-color: #ffc107;
                }}
                .metric-card.danger {{
                    border-left-color: #dc3545;
                }}
                .metric-value {{
                    font-size: 2em;
                    font-weight: bold;
                    color: #2c3e50;
                    margin-bottom: 5px;
                }}
                .metric-label {{
                    color: #6c757d;
                    font-size: 0.9em;
                }}
                .test-results {{
                    padding: 30px;
                }}
                .test-item {{
                    display: flex;
                    align-items: center;
                    padding: 15px;
                    margin-bottom: 10px;
                    border-radius: 8px;
                    background: #f8f9fa;
                    border-left: 4px solid #28a745;
                }}
                .test-item.failed {{
                    border-left-color: #dc3545;
                    background: #fff5f5;
                }}
                .test-status {{
                    width: 24px;
                    height: 24px;
                    border-radius: 50%;
                    margin-right: 15px;
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    color: white;
                    font-weight: bold;
                }}
                .test-status.passed {{
                    background: #28a745;
                }}
                .test-status.failed {{
                    background: #dc3545;
                }}
                .test-info {{
                    flex: 1;
                }}
                .test-name {{
                    font-weight: 600;
                    color: #2c3e50;
                    margin-bottom: 5px;
                }}
                .test-duration {{
                    color: #6c757d;
                    font-size: 0.9em;
                }}
                .test-error {{
                    color: #dc3545;
                    font-size: 0.9em;
                    margin-top: 5px;
                }}
                .progress-bar {{
                    width: 100%;
                    height: 8px;
                    background: #e9ecef;
                    border-radius: 4px;
                    overflow: hidden;
                    margin: 20px 0;
                }}
                .progress-fill {{
                    height: 100%;
                    background: linear-gradient(90deg, #28a745, #20c997);
                    transition: width 0.3s ease;
                }}
                .footer {{
                    background: #f8f9fa;
                    padding: 20px;
                    text-align: center;
                    color: #6c757d;
                    border-top: 1px solid #dee2e6;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>ğŸš€ Perfect21 åŠŸèƒ½éªŒè¯æŠ¥å‘Š</h1>
                    <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>

                <div class="summary">
                    <div class="metric-card {'success' if summary['overall_success'] else 'danger'}">
                        <div class="metric-value">{'âœ…' if summary['overall_success'] else 'âŒ'}</div>
                        <div class="metric-label">æ•´ä½“çŠ¶æ€</div>
                    </div>

                    <div class="metric-card">
                        <div class="metric-value">{summary['total_tests']}</div>
                        <div class="metric-label">æ€»æµ‹è¯•æ•°</div>
                    </div>

                    <div class="metric-card success">
                        <div class="metric-value">{summary['passed_tests']}</div>
                        <div class="metric-label">é€šè¿‡æµ‹è¯•</div>
                    </div>

                    <div class="metric-card {'success' if summary['failed_tests'] == 0 else 'danger'}">
                        <div class="metric-value">{summary['failed_tests']}</div>
                        <div class="metric-label">å¤±è´¥æµ‹è¯•</div>
                    </div>

                    <div class="metric-card">
                        <div class="metric-value">{summary['success_rate']:.1f}%</div>
                        <div class="metric-label">æˆåŠŸç‡</div>
                    </div>

                    <div class="metric-card">
                        <div class="metric-value">{summary['total_duration']:.1f}s</div>
                        <div class="metric-label">æ€»è€—æ—¶</div>
                    </div>
                </div>

                <div class="progress-bar">
                    <div class="progress-fill" style="width: {summary['success_rate']}%"></div>
                </div>

                <div class="test-results">
                    <h2>ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ</h2>
        """

        for result in self.test_results:
            status_class = "passed" if result.success else "failed"
            item_class = "failed" if not result.success else ""
            status_icon = "âœ“" if result.success else "âœ—"

            html_content += f"""
                    <div class="test-item {item_class}">
                        <div class="test-status {status_class}">{status_icon}</div>
                        <div class="test-info">
                            <div class="test-name">{result.test_name}</div>
                            <div class="test-duration">è€—æ—¶: {result.duration:.2f}s</div>
                            {f'<div class="test-error">é”™è¯¯: {result.error}</div>' if result.error else ''}
                        </div>
                    </div>
            """

        html_content += f"""
                </div>

                <div class="footer">
                    <p>Perfect21 åŠŸèƒ½éªŒè¯æµ‹è¯• - è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š</p>
                    <p>è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: {self.report_file.name}</p>
                </div>
            </div>
        </body>
        </html>
        """

        with open(self.dashboard_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        self.logger.info(f"ğŸ¨ HTMLä»ªè¡¨æ¿å·²ç”Ÿæˆ: {self.dashboard_file}")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨Perfect21åŠŸèƒ½éªŒè¯æµ‹è¯•")
    print("=" * 80)

    validator = Perfect21FunctionalityValidator()

    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        summary = await validator.run_all_tests()

        # è¾“å‡ºæœ€ç»ˆç»“æœ
        print("\n" + "=" * 80)
        print("ğŸ¯ Perfect21åŠŸèƒ½éªŒè¯å®Œæˆ")
        print(f"ğŸ“Š æˆåŠŸç‡: {summary['success_rate']:.1f}% ({summary['passed_tests']}/{summary['total_tests']})")
        print(f"â±ï¸  æ€»è€—æ—¶: {summary['total_duration']:.2f}ç§’")

        if summary['overall_success']:
            print("âœ… æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
            return 0
        else:
            print("âŒ éƒ¨åˆ†åŠŸèƒ½å­˜åœ¨é—®é¢˜ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")
            return 1

    except Exception as e:
        print(f"ğŸ’¥ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)