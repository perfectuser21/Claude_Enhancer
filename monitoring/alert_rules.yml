# =============================================================================
# Prometheus Alert Rules for Perfect21 Claude Enhancer
# Comprehensive alerting for production monitoring
# =============================================================================

groups:
# Application Performance Alerts
- name: claude-enhancer-application
  rules:
  - alert: ApplicationDown
    expr: up{job="claude-enhancer"} == 0
    for: 1m
    labels:
      severity: critical
      service: claude-enhancer
    annotations:
      summary: "Claude Enhancer application is down"
      description: "Claude Enhancer application has been down for more than 1 minute. Instance: {{ $labels.instance }}"

  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="claude-enhancer"}[5m])) > 2
    for: 5m
    labels:
      severity: warning
      service: claude-enhancer
    annotations:
      summary: "High response time detected"
      description: "95th percentile response time is {{ $value }}s for instance {{ $labels.instance }}"

  - alert: HighErrorRate
    expr: rate(http_requests_total{job="claude-enhancer",status=~"5.."}[5m]) / rate(http_requests_total{job="claude-enhancer"}[5m]) > 0.05
    for: 5m
    labels:
      severity: critical
      service: claude-enhancer
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value | humanizePercentage }} for instance {{ $labels.instance }}"

  - alert: LowThroughput
    expr: rate(http_requests_total{job="claude-enhancer"}[5m]) < 10
    for: 10m
    labels:
      severity: warning
      service: claude-enhancer
    annotations:
      summary: "Low request throughput"
      description: "Request rate is {{ $value }} requests/sec for instance {{ $labels.instance }}"

# Database Alerts
- name: postgresql-alerts
  rules:
  - alert: PostgreSQLDown
    expr: pg_up == 0
    for: 1m
    labels:
      severity: critical
      service: postgresql
    annotations:
      summary: "PostgreSQL is down"
      description: "PostgreSQL database is down. Instance: {{ $labels.instance }}"

  - alert: PostgreSQLHighConnections
    expr: pg_stat_database_numbackends / pg_settings_max_connections > 0.8
    for: 5m
    labels:
      severity: warning
      service: postgresql
    annotations:
      summary: "PostgreSQL high connection usage"
      description: "PostgreSQL connection usage is {{ $value | humanizePercentage }}. Instance: {{ $labels.instance }}"

  - alert: PostgreSQLSlowQueries
    expr: pg_stat_activity_max_tx_duration > 300
    for: 5m
    labels:
      severity: warning
      service: postgresql
    annotations:
      summary: "PostgreSQL slow queries detected"
      description: "Longest running query is {{ $value }}s. Instance: {{ $labels.instance }}"

  - alert: PostgreSQLHighReplicationLag
    expr: pg_replication_lag > 10
    for: 5m
    labels:
      severity: warning
      service: postgresql
    annotations:
      summary: "PostgreSQL high replication lag"
      description: "Replication lag is {{ $value }}s. Instance: {{ $labels.instance }}"

  - alert: PostgreSQLLowDiskSpace
    expr: pg_database_size_bytes / (1024*1024*1024) > 15
    for: 5m
    labels:
      severity: warning
      service: postgresql
    annotations:
      summary: "PostgreSQL database size growing"
      description: "Database size is {{ $value }}GB. Instance: {{ $labels.instance }}"

# Redis Alerts
- name: redis-alerts
  rules:
  - alert: RedisDown
    expr: redis_up == 0
    for: 1m
    labels:
      severity: critical
      service: redis
    annotations:
      summary: "Redis is down"
      description: "Redis server is down. Instance: {{ $labels.instance }}"

  - alert: RedisHighMemoryUsage
    expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
    for: 5m
    labels:
      severity: warning
      service: redis
    annotations:
      summary: "Redis high memory usage"
      description: "Redis memory usage is {{ $value | humanizePercentage }}. Instance: {{ $labels.instance }}"

  - alert: RedisHighConnections
    expr: redis_connected_clients > 100
    for: 5m
    labels:
      severity: warning
      service: redis
    annotations:
      summary: "Redis high client connections"
      description: "Redis has {{ $value }} client connections. Instance: {{ $labels.instance }}"

  - alert: RedisSlowOperations
    expr: redis_slowlog_length > 10
    for: 5m
    labels:
      severity: warning
      service: redis
    annotations:
      summary: "Redis slow operations detected"
      description: "Redis slow log has {{ $value }} entries. Instance: {{ $labels.instance }}"

# Infrastructure Alerts
- name: kubernetes-alerts
  rules:
  - alert: PodCrashLooping
    expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
    for: 5m
    labels:
      severity: warning
      service: kubernetes
    annotations:
      summary: "Pod is crash looping"
      description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"

  - alert: PodNotReady
    expr: kube_pod_status_ready{condition="false"} == 1
    for: 5m
    labels:
      severity: warning
      service: kubernetes
    annotations:
      summary: "Pod not ready"
      description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been not ready for more than 5 minutes"

  - alert: NodeNotReady
    expr: kube_node_status_ready{condition="false"} == 1
    for: 5m
    labels:
      severity: critical
      service: kubernetes
    annotations:
      summary: "Node not ready"
      description: "Node {{ $labels.node }} has been not ready for more than 5 minutes"

  - alert: HighPodCPUUsage
    expr: rate(container_cpu_usage_seconds_total{namespace="claude-enhancer"}[5m]) > 0.8
    for: 10m
    labels:
      severity: warning
      service: kubernetes
    annotations:
      summary: "High CPU usage in pod"
      description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has high CPU usage: {{ $value | humanizePercentage }}"

  - alert: HighPodMemoryUsage
    expr: container_memory_usage_bytes{namespace="claude-enhancer"} / container_spec_memory_limit_bytes > 0.9
    for: 10m
    labels:
      severity: warning
      service: kubernetes
    annotations:
      summary: "High memory usage in pod"
      description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has high memory usage: {{ $value | humanizePercentage }}"

# Load Balancer Alerts
- name: nginx-alerts
  rules:
  - alert: NginxDown
    expr: nginx_up == 0
    for: 1m
    labels:
      severity: critical
      service: nginx
    annotations:
      summary: "Nginx is down"
      description: "Nginx load balancer is down. Instance: {{ $labels.instance }}"

  - alert: NginxHighErrorRate
    expr: rate(nginx_http_requests_total{status=~"5.."}[5m]) / rate(nginx_http_requests_total[5m]) > 0.05
    for: 5m
    labels:
      severity: warning
      service: nginx
    annotations:
      summary: "Nginx high error rate"
      description: "Nginx error rate is {{ $value | humanizePercentage }}. Instance: {{ $labels.instance }}"

  - alert: NginxHighLatency
    expr: histogram_quantile(0.95, rate(nginx_http_request_duration_seconds_bucket[5m])) > 1
    for: 5m
    labels:
      severity: warning
      service: nginx
    annotations:
      summary: "Nginx high latency"
      description: "Nginx 95th percentile latency is {{ $value }}s. Instance: {{ $labels.instance }}"

# Security Alerts
- name: security-alerts
  rules:
  - alert: SuspiciousLoginActivity
    expr: increase(auth_failed_attempts_total[5m]) > 10
    for: 1m
    labels:
      severity: warning
      service: security
    annotations:
      summary: "Suspicious login activity detected"
      description: "{{ $value }} failed login attempts in the last 5 minutes from {{ $labels.ip }}"

  - alert: UnauthorizedAPIAccess
    expr: increase(http_requests_total{status="401"}[5m]) > 20
    for: 2m
    labels:
      severity: warning
      service: security
    annotations:
      summary: "High number of unauthorized API requests"
      description: "{{ $value }} unauthorized requests in the last 5 minutes"

  - alert: DDoSAttackSuspected
    expr: rate(http_requests_total[1m]) > 1000
    for: 2m
    labels:
      severity: critical
      service: security
    annotations:
      summary: "Possible DDoS attack detected"
      description: "Request rate is {{ $value }} requests/second, which may indicate a DDoS attack"

# Business Logic Alerts
- name: business-alerts
  rules:
  - alert: LowAgentUtilization
    expr: claude_enhancer_active_agents / claude_enhancer_total_agents < 0.3
    for: 15m
    labels:
      severity: info
      service: business
    annotations:
      summary: "Low agent utilization"
      description: "Agent utilization is {{ $value | humanizePercentage }}. Consider scaling down or optimizing agent allocation."

  - alert: HighTaskFailureRate
    expr: rate(claude_enhancer_task_failures_total[10m]) / rate(claude_enhancer_tasks_total[10m]) > 0.1
    for: 5m
    labels:
      severity: warning
      service: business
    annotations:
      summary: "High task failure rate"
      description: "Task failure rate is {{ $value | humanizePercentage }}. Investigation required."

  - alert: LowTaskCompletionRate
    expr: rate(claude_enhancer_tasks_completed_total[10m]) < 5
    for: 10m
    labels:
      severity: info
      service: business
    annotations:
      summary: "Low task completion rate"
      description: "Task completion rate is {{ $value }} tasks/minute. Check for system issues or low demand."

# External Dependencies
- name: external-alerts
  rules:
  - alert: ExternalAPIDown
    expr: probe_success == 0
    for: 2m
    labels:
      severity: warning
      service: external
    annotations:
      summary: "External API is down"
      description: "External API {{ $labels.instance }} is not responding to health checks"

  - alert: ExternalAPIHighLatency
    expr: probe_duration_seconds > 5
    for: 5m
    labels:
      severity: warning
      service: external
    annotations:
      summary: "External API high latency"
      description: "External API {{ $labels.instance }} response time is {{ $value }}s"

# Certificate Expiration
- name: certificate-alerts
  rules:
  - alert: SSLCertificateExpiringSoon
    expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 7
    for: 1h
    labels:
      severity: warning
      service: security
    annotations:
      summary: "SSL certificate expiring soon"
      description: "SSL certificate for {{ $labels.instance }} expires in {{ $value | humanizeDuration }}"

  - alert: SSLCertificateExpired
    expr: probe_ssl_earliest_cert_expiry - time() <= 0
    for: 1m
    labels:
      severity: critical
      service: security
    annotations:
      summary: "SSL certificate expired"
      description: "SSL certificate for {{ $labels.instance }} has expired"