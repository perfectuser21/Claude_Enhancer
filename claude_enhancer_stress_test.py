#!/usr/bin/env python3
"""
Claude Enhancer å…¨é¢å‹åŠ›æµ‹è¯•å¥—ä»¶ v2.0
================================================

ä¸“ä¸šæ€§èƒ½å·¥ç¨‹å‹åŠ›æµ‹è¯•ï¼Œæ¶µç›–ï¼š
1. æ€§èƒ½å‹åŠ›æµ‹è¯• - Hookæ‰§è¡Œé€Ÿåº¦ã€Agentå¹¶å‘è°ƒç”¨ã€æ–‡æ¡£åŠ è½½æ€§èƒ½
2. å¹¶å‘å‹åŠ›æµ‹è¯• - å¤šHookåŒæ—¶è§¦å‘ã€å¤šAgentå¹¶è¡Œæ‰§è¡Œã€èµ„æºç«äº‰
3. å†…å­˜å‹åŠ›æµ‹è¯• - å¤§æ–‡ä»¶å¤„ç†ã€å†…å­˜æ³„æ¼æ£€æµ‹ã€ç¼“å­˜æœºåˆ¶
4. ç¨³å®šæ€§æµ‹è¯• - é•¿æ—¶é—´è¿è¡Œã€é”™è¯¯æ¢å¤ã€è¾¹ç•Œæ¡ä»¶

åŸºäºç°æœ‰ä»£ç çš„ä¸“ä¸šå¢å¼ºç‰ˆæœ¬
ä½œè€…: Claude Code (Performance Engineering Expert)
ç‰ˆæœ¬: 2.0.0
è®¸å¯: MIT
"""

import os
import sys
import time
import json
import yaml
import subprocess
import threading
import multiprocessing
import tracemalloc
import gc
import signal
import psutil
import statistics
import random
import string
import tempfile
import shutil
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import logging

# è®¾ç½®ä¸“ä¸šçº§æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
    handlers=[
        logging.FileHandler("/tmp/claude_enhancer_stress_test.log"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""

    timestamp: float
    operation: str
    duration_ms: float
    cpu_percent: float
    memory_mb: float
    success: bool
    error_message: Optional[str] = None
    additional_data: Optional[Dict] = None


@dataclass
class StressTestResult:
    """å‹åŠ›æµ‹è¯•ç»“æœ"""

    test_name: str
    start_time: float
    end_time: float
    total_operations: int
    successful_operations: int
    failed_operations: int
    metrics: List[PerformanceMetrics]
    peak_memory_mb: float
    avg_cpu_percent: float
    error_summary: Dict[str, int]
    performance_percentiles: Dict[str, float]


class SystemMonitor:
    """ç³»ç»Ÿæ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.monitoring = False
        self.monitor_thread = None
        self.metrics_history = deque(maxlen=10000)
        self.peak_memory = 0
        self.peak_cpu = 0

    def start_monitoring(self):
        """å¼€å§‹ç³»ç»Ÿç›‘æ§"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        tracemalloc.start()

    def stop_monitoring(self):
        """åœæ­¢ç³»ç»Ÿç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1)
        tracemalloc.stop()

    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024
                cpu_percent = process.cpu_percent()

                self.peak_memory = max(self.peak_memory, memory_mb)
                self.peak_cpu = max(self.peak_cpu, cpu_percent)

                self.metrics_history.append(
                    {
                        "timestamp": time.time(),
                        "memory_mb": memory_mb,
                        "cpu_percent": cpu_percent,
                        "threads": process.num_threads(),
                        "open_files": len(process.open_files()),
                    }
                )

                time.sleep(0.1)
            except Exception as e:
                logger.error(f"Monitor error: {e}")

    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.metrics_history:
            return {}

        memory_values = [m["memory_mb"] for m in self.metrics_history]
        cpu_values = [m["cpu_percent"] for m in self.metrics_history]

        return {
            "peak_memory_mb": self.peak_memory,
            "peak_cpu_percent": self.peak_cpu,
            "avg_memory_mb": statistics.mean(memory_values),
            "avg_cpu_percent": statistics.mean(cpu_values),
            "current_threads": self.metrics_history[-1]["threads"],
            "current_open_files": self.metrics_history[-1]["open_files"],
        }


class ClaudeEnhancerStressTest:
    """Claude Enhancerå…¨é¢å‹åŠ›æµ‹è¯•å™¨"""

    def __init__(self):
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "system_info": self._get_system_info(),
            "tests": {},
            "summary": {},
            "bottlenecks": [],
            "recommendations": [],
        }

        # é¡¹ç›®è·¯å¾„é…ç½®
        self.project_root = Path("/home/xx/dev/Claude_Enhancer")
        self.hook_dir = self.project_root / ".claude" / "hooks"
        self.config_dir = self.project_root / ".claude" / "config"
        self.agents_dir = self.project_root / ".claude" / "agents"

        # ç³»ç»Ÿç›‘æ§å™¨
        self.system_monitor = SystemMonitor()

        # æµ‹è¯•é…ç½®
        self.test_config = {
            "hook_iterations": 100,
            "concurrent_levels": [5, 10, 20, 50],
            "memory_test_size_mb": [1, 10, 50, 100],
            "stability_duration_minutes": 5,
            "agent_test_scenarios": ["simple", "standard", "complex"],
        }

    def _get_system_info(self):
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "python_version": sys.version,
            "platform": sys.platform,
            "timestamp": datetime.now().isoformat(),
        }

    def run_command(self, cmd, timeout=10, input_data=""):
        """æ‰§è¡Œå‘½ä»¤å¹¶æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        start_time = time.time()
        process = psutil.Process()
        start_memory = process.memory_info().rss / 1024 / 1024

        try:
            result = subprocess.run(
                cmd,
                shell=True,
                capture_output=True,
                text=True,
                timeout=timeout,
                input=input_data,
            )

            end_time = time.time()
            duration_ms = (end_time - start_time) * 1000
            end_memory = process.memory_info().rss / 1024 / 1024

            return PerformanceMetrics(
                timestamp=start_time,
                operation=cmd[:50],  # é™åˆ¶å‘½ä»¤é•¿åº¦
                duration_ms=duration_ms,
                cpu_percent=psutil.cpu_percent(),
                memory_mb=end_memory - start_memory,
                success=result.returncode == 0,
                error_message=result.stderr if result.returncode != 0 else None,
                additional_data={
                    "stdout_length": len(result.stdout),
                    "stderr_length": len(result.stderr),
                    "return_code": result.returncode,
                },
            )
        except subprocess.TimeoutExpired:
            return PerformanceMetrics(
                timestamp=start_time,
                operation=cmd[:50],
                duration_ms=timeout * 1000,
                cpu_percent=0,
                memory_mb=0,
                success=False,
                error_message="Command timeout",
            )
        except Exception as e:
            return PerformanceMetrics(
                timestamp=start_time,
                operation=cmd[:50],
                duration_ms=0,
                cpu_percent=0,
                memory_mb=0,
                success=False,
                error_message=str(e),
            )

    def test_hook_performance(self):
        """æµ‹è¯•Hookç³»ç»Ÿæ€§èƒ½ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        logger.info("ğŸ”§ Testing Hook System Performance...")
        self.system_monitor.start_monitoring()

        hook_files = list(self.hook_dir.glob("*.sh"))
        all_metrics = []

        for hook_path in hook_files:
            if hook_path.name.startswith("."):
                continue

            logger.info(f"Testing hook: {hook_path.name}")
            hook_metrics = []

            # æµ‹è¯•å¤šæ¬¡è·å–ç»Ÿè®¡æ•°æ®
            for i in range(self.test_config["hook_iterations"]):
                # æ¨¡æ‹Ÿä¸åŒçš„è¾“å…¥æ•°æ®
                test_inputs = [
                    '{"tool": "test", "prompt": "simple test"}',
                    '{"tool": "Task", "prompt": "complex development task with multiple requirements"}',
                    '{"tool": "Write", "prompt": "write complex code"}',
                    '{"phase": "3", "task": "implementation"}',
                    "",  # ç©ºè¾“å…¥æµ‹è¯•
                ]

                input_data = test_inputs[i % len(test_inputs)]
                metric = self.run_command(
                    f"bash {hook_path}", timeout=10, input_data=input_data
                )
                hook_metrics.append(metric)

                # é¿å…è¿‡åº¦é¢‘ç¹è°ƒç”¨
                if i % 10 == 0:
                    time.sleep(0.1)

            all_metrics.extend(hook_metrics)

            # è®¡ç®—è¯¥Hookçš„ç»Ÿè®¡ä¿¡æ¯
            successful_metrics = [m for m in hook_metrics if m.success]
            if successful_metrics:
                durations = [m.duration_ms for m in successful_metrics]
                logger.info(
                    f"  {hook_path.name}: avg={statistics.mean(durations):.2f}ms, "
                    f"p95={sorted(durations)[int(len(durations)*0.95)]:.2f}ms, "
                    f"success_rate={len(successful_metrics)/len(hook_metrics)*100:.1f}%"
                )

        self.system_monitor.stop_monitoring()

        # åˆ†æç»“æœ
        result = self._analyze_performance_metrics(
            all_metrics, "Hook Performance Tests"
        )
        self.results["tests"]["hook_performance"] = asdict(result)

        return result

    def test_agent_concurrency(self):
        """æµ‹è¯•Agentå¹¶å‘æ€§èƒ½ï¼ˆ4-6-8ç­–ç•¥ï¼‰"""
        logger.info("ğŸ¤– Testing Agent Concurrency (4-6-8 Strategy)...")
        self.system_monitor.start_monitoring()

        all_metrics = []

        # æ¨¡æ‹Ÿ4-6-8ç­–ç•¥çš„Agentæ‰§è¡Œ
        strategies = {
            "simple": {"agents": 4, "complexity": 0.1, "duration_range": (0.1, 0.3)},
            "standard": {"agents": 6, "complexity": 0.5, "duration_range": (0.3, 0.6)},
            "complex": {"agents": 8, "complexity": 1.0, "duration_range": (0.6, 1.2)},
        }

        for strategy_name, config in strategies.items():
            logger.info(
                f"Testing {strategy_name} strategy with {config['agents']} agents"
            )

            for iteration in range(20):  # æ¯ç§ç­–ç•¥æµ‹è¯•20æ¬¡
                # å¹¶å‘æ‰§è¡Œæ¨¡æ‹Ÿçš„Agentä»»åŠ¡
                with ThreadPoolExecutor(max_workers=config["agents"]) as executor:
                    futures = []

                    for agent_id in range(config["agents"]):
                        future = executor.submit(
                            self._simulate_agent_execution,
                            f"{strategy_name}_agent_{agent_id}",
                            config["complexity"],
                            config["duration_range"],
                        )
                        futures.append(future)

                    # æ”¶é›†ç»“æœ
                    for future in as_completed(futures):
                        try:
                            metric = future.result(timeout=5)
                            all_metrics.append(metric)
                        except Exception as e:
                            logger.error(f"Agent execution failed: {e}")

        self.system_monitor.stop_monitoring()

        result = self._analyze_performance_metrics(
            all_metrics, "Agent Concurrency Tests"
        )
        self.results["tests"]["agent_concurrency"] = asdict(result)

        return result

    def _simulate_agent_execution(
        self, agent_name: str, complexity: float, duration_range: Tuple[float, float]
    ) -> PerformanceMetrics:
        """æ¨¡æ‹ŸAgentæ‰§è¡Œ"""
        start_time = time.time()
        process = psutil.Process()
        start_memory = process.memory_info().rss / 1024 / 1024

        # æ ¹æ®å¤æ‚åº¦æ¨¡æ‹Ÿä¸åŒçš„å·¥ä½œè´Ÿè½½
        work_amount = int(1000 * complexity + 100)
        duration = random.uniform(*duration_range)

        # æ¨¡æ‹ŸCPUå¯†é›†å‹å·¥ä½œ
        for _ in range(work_amount):
            _ = sum(range(random.randint(50, 200)))

        # æ¨¡æ‹ŸI/Oç­‰å¾…
        time.sleep(duration * 0.1)  # 10%çš„æ—¶é—´ç”¨äºI/Oç­‰å¾…

        # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨
        temp_data = [random.random() for _ in range(int(work_amount * 0.1))]
        del temp_data  # ç«‹å³é‡Šæ”¾

        end_time = time.time()
        end_memory = process.memory_info().rss / 1024 / 1024

        return PerformanceMetrics(
            timestamp=start_time,
            operation=f"agent_{agent_name}",
            duration_ms=(end_time - start_time) * 1000,
            cpu_percent=psutil.cpu_percent(),
            memory_mb=end_memory - start_memory,
            success=True,
            additional_data={
                "complexity": complexity,
                "agent_name": agent_name,
                "work_amount": work_amount,
            },
        )

    def test_concurrent_hook_execution(self):
        """æµ‹è¯•Hookå¹¶å‘æ‰§è¡Œèƒ½åŠ›ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        logger.info("ğŸ”€ Testing Concurrent Hook Execution...")
        self.system_monitor.start_monitoring()

        hook_files = list(self.hook_dir.glob("*.sh"))[:5]  # é€‰æ‹©å‰5ä¸ªHook
        all_metrics = []

        for concurrency in self.test_config["concurrent_levels"]:
            logger.info(f"Testing concurrency level: {concurrency}")

            # å¹¶å‘æ‰§è¡ŒHook
            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = []

                for _ in range(concurrency):
                    hook_path = random.choice(hook_files)
                    input_data = '{"tool": "test", "concurrent": true}'

                    future = executor.submit(
                        self.run_command, f"bash {hook_path}", 5, input_data
                    )
                    futures.append(future)

                # æ”¶é›†ç»“æœ
                for future in as_completed(futures):
                    try:
                        metric = future.result(timeout=10)
                        metric.additional_data = {
                            **(metric.additional_data or {}),
                            "concurrency_level": concurrency,
                        }
                        all_metrics.append(metric)
                    except Exception as e:
                        logger.error(f"Concurrent hook execution failed: {e}")

        self.system_monitor.stop_monitoring()

        result = self._analyze_performance_metrics(
            all_metrics, "Concurrent Hook Execution"
        )
        self.results["tests"]["concurrent_execution"] = asdict(result)

        return result

    def test_memory_pressure(self):
        """å†…å­˜å‹åŠ›æµ‹è¯•"""
        logger.info("ğŸ’¾ Testing Memory Pressure...")
        self.system_monitor.start_monitoring()

        all_metrics = []

        # æµ‹è¯•å¤§æ–‡ä»¶å¤„ç†
        for size_mb in self.test_config["memory_test_size_mb"]:
            logger.info(f"Testing {size_mb}MB file processing...")

            # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
            test_file = self._create_test_file(size_mb)

            try:
                # æµ‹è¯•æ–‡ä»¶è¯»å–
                metric = self._test_file_processing(test_file, size_mb)
                all_metrics.append(metric)

                # æµ‹è¯•å†…å­˜æ³„æ¼æ£€æµ‹
                leak_metrics = self._test_memory_leak_simulation(100)
                all_metrics.extend(leak_metrics)

            finally:
                # æ¸…ç†æµ‹è¯•æ–‡ä»¶
                if test_file.exists():
                    test_file.unlink()

        self.system_monitor.stop_monitoring()

        result = self._analyze_performance_metrics(all_metrics, "Memory Pressure Tests")
        self.results["tests"]["memory_pressure"] = asdict(result)

        return result

    def _create_test_file(self, size_mb: int) -> Path:
        """åˆ›å»ºæµ‹è¯•æ–‡ä»¶"""
        test_file = Path(f"/tmp/claude_test_{size_mb}mb_{int(time.time())}.txt")

        content_size = size_mb * 1024 * 1024
        chunk_size = 8192

        with open(test_file, "w") as f:
            written = 0
            base_content = (
                "This is test content for Claude Enhancer stress testing. " * 100
            )

            while written < content_size:
                chunk = base_content[: min(chunk_size, content_size - written)]
                f.write(chunk)
                written += len(chunk)

        return test_file

    def _test_file_processing(
        self, file_path: Path, size_mb: int
    ) -> PerformanceMetrics:
        """æµ‹è¯•æ–‡ä»¶å¤„ç†æ€§èƒ½"""
        start_time = time.time()
        process = psutil.Process()
        start_memory = process.memory_info().rss / 1024 / 1024

        try:
            # æ¨¡æ‹Ÿæ–‡æ¡£åŠ è½½å’Œå¤„ç†
            with open(file_path, "r") as f:
                content = f.read()

            # æ¨¡æ‹Ÿå†…å®¹åˆ†æ
            lines = content.split("\n")
            words = content.split()

            # æ¨¡æ‹Ÿä¸€äº›å¤„ç†æ“ä½œ
            _ = len(lines)
            _ = len(words)

            end_time = time.time()
            end_memory = process.memory_info().rss / 1024 / 1024

            return PerformanceMetrics(
                timestamp=start_time,
                operation=f"file_processing_{size_mb}mb",
                duration_ms=(end_time - start_time) * 1000,
                cpu_percent=psutil.cpu_percent(),
                memory_mb=end_memory - start_memory,
                success=True,
                additional_data={
                    "file_size_mb": size_mb,
                    "lines_count": len(lines),
                    "words_count": len(words),
                },
            )
        except Exception as e:
            return PerformanceMetrics(
                timestamp=start_time,
                operation=f"file_processing_{size_mb}mb",
                duration_ms=0,
                cpu_percent=0,
                memory_mb=0,
                success=False,
                error_message=str(e),
            )

    def _test_memory_leak_simulation(self, iterations: int) -> List[PerformanceMetrics]:
        """æ¨¡æ‹Ÿå†…å­˜æ³„æ¼æµ‹è¯•"""
        metrics = []
        memory_snapshots = []

        gc.collect()  # æ¸…ç†åƒåœ¾å›æ”¶
        process = psutil.Process()
        baseline_memory = process.memory_info().rss / 1024 / 1024

        for i in range(iterations):
            start_time = time.time()

            # åˆ›å»ºä¸€äº›å¯¹è±¡
            data = []
            for j in range(100):
                item = {
                    "id": j,
                    "data": "".join(random.choices(string.ascii_letters, k=1000)),
                    "timestamp": time.time(),
                    "nested": {"value": random.random()},
                }
                data.append(item)

            # æ•…æ„ä¿ç•™ä¸€äº›å¼•ç”¨ï¼ˆæ¨¡æ‹Ÿæ½œåœ¨æ³„æ¼ï¼‰
            if i % 20 == 0:
                memory_snapshots.append(data[:5])

            current_memory = process.memory_info().rss / 1024 / 1024
            memory_delta = current_memory - baseline_memory

            metrics.append(
                PerformanceMetrics(
                    timestamp=start_time,
                    operation="memory_leak_test",
                    duration_ms=(time.time() - start_time) * 1000,
                    cpu_percent=psutil.cpu_percent(),
                    memory_mb=memory_delta,
                    success=True,
                    additional_data={
                        "iteration": i,
                        "current_memory_mb": current_memory,
                        "baseline_memory_mb": baseline_memory,
                        "snapshots_retained": len(memory_snapshots),
                    },
                )
            )

        return metrics

    def test_stability_long_running(self):
        """é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•"""
        logger.info(
            f"ğŸ”„ Testing Long-running Stability ({self.test_config['stability_duration_minutes']} minutes)..."
        )
        self.system_monitor.start_monitoring()

        all_metrics = []
        start_time = time.time()
        end_time = start_time + (self.test_config["stability_duration_minutes"] * 60)

        operations = [
            self._simulate_hook_operation,
            self._simulate_config_operation,
            self._simulate_agent_operation,
            self._simulate_file_operation,
        ]

        while time.time() < end_time:
            operation = random.choice(operations)

            try:
                metric = operation()
                all_metrics.append(metric)
            except Exception as e:
                logger.error(f"Stability test operation failed: {e}")

            # éšæœºä¼‘çœ æ¨¡æ‹ŸçœŸå®ä½¿ç”¨æ¨¡å¼
            time.sleep(random.uniform(0.1, 2.0))

        self.system_monitor.stop_monitoring()

        result = self._analyze_performance_metrics(
            all_metrics, "Long-running Stability Test"
        )
        self.results["tests"]["stability"] = asdict(result)

        logger.info(f"Stability test completed: {len(all_metrics)} operations")
        return result

    def _simulate_hook_operation(self) -> PerformanceMetrics:
        """æ¨¡æ‹ŸHookæ“ä½œ"""
        hook_files = list(self.hook_dir.glob("*.sh"))
        if not hook_files:
            raise Exception("No hook files found")

        hook_path = random.choice(hook_files)
        return self.run_command(
            f"bash {hook_path}", timeout=5, input_data='{"test": true}'
        )

    def _simulate_config_operation(self) -> PerformanceMetrics:
        """æ¨¡æ‹Ÿé…ç½®æ“ä½œ"""
        config_file = self.config_dir / "unified_main.yaml"
        if not config_file.exists():
            raise Exception("Config file not found")

        return self.run_command(
            f"python3 -c 'import yaml; yaml.safe_load(open(\"{config_file}\"))'",
            timeout=3,
        )

    def _simulate_agent_operation(self) -> PerformanceMetrics:
        """æ¨¡æ‹ŸAgentæ“ä½œ"""
        return self._simulate_agent_execution(
            "stability_agent", random.uniform(0.1, 0.5), (0.1, 0.3)
        )

    def _simulate_file_operation(self) -> PerformanceMetrics:
        """æ¨¡æ‹Ÿæ–‡ä»¶æ“ä½œ"""
        start_time = time.time()

        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_file = Path(f"/tmp/claude_stability_{random.randint(1000, 9999)}.tmp")

            with open(temp_file, "w") as f:
                f.write(f"Stability test data: {time.time()}")

            with open(temp_file, "r") as f:
                content = f.read()

            temp_file.unlink()

            return PerformanceMetrics(
                timestamp=start_time,
                operation="file_operation",
                duration_ms=(time.time() - start_time) * 1000,
                cpu_percent=psutil.cpu_percent(),
                memory_mb=0,
                success=True,
            )
        except Exception as e:
            return PerformanceMetrics(
                timestamp=start_time,
                operation="file_operation",
                duration_ms=0,
                cpu_percent=0,
                memory_mb=0,
                success=False,
                error_message=str(e),
            )

    def test_error_recovery(self):
        """é”™è¯¯æ¢å¤æµ‹è¯•"""
        logger.info("ğŸ”§ Testing Error Recovery...")

        all_metrics = []
        error_scenarios = [
            self._test_timeout_recovery,
            self._test_file_not_found_recovery,
            self._test_invalid_input_recovery,
            self._test_memory_pressure_recovery,
        ]

        for scenario in error_scenarios:
            try:
                metric = scenario()
                all_metrics.append(metric)
            except Exception as e:
                logger.error(f"Error recovery test failed: {e}")

        result = self._analyze_performance_metrics(all_metrics, "Error Recovery Tests")
        self.results["tests"]["error_recovery"] = asdict(result)

        return result

    def _test_timeout_recovery(self) -> PerformanceMetrics:
        """æµ‹è¯•è¶…æ—¶æ¢å¤"""
        start_time = time.time()

        try:
            # æ•…æ„è§¦å‘è¶…æ—¶
            result = subprocess.run(["sleep", "3"], timeout=1, capture_output=True)
        except subprocess.TimeoutExpired:
            # é¢„æœŸçš„è¶…æ—¶ï¼Œè¿™æ˜¯æ­£å¸¸çš„æ¢å¤è¡Œä¸º
            pass

        return PerformanceMetrics(
            timestamp=start_time,
            operation="timeout_recovery",
            duration_ms=(time.time() - start_time) * 1000,
            cpu_percent=psutil.cpu_percent(),
            memory_mb=0,
            success=True,  # æˆåŠŸå¤„ç†äº†è¶…æ—¶
            additional_data={"recovery_type": "timeout"},
        )

    def _test_file_not_found_recovery(self) -> PerformanceMetrics:
        """æµ‹è¯•æ–‡ä»¶æœªæ‰¾åˆ°æ¢å¤"""
        start_time = time.time()

        try:
            with open("/nonexistent/path/file.txt", "r") as f:
                _ = f.read()
        except FileNotFoundError:
            # é¢„æœŸçš„é”™è¯¯ï¼Œæ­£å¸¸æ¢å¤
            pass

        return PerformanceMetrics(
            timestamp=start_time,
            operation="file_not_found_recovery",
            duration_ms=(time.time() - start_time) * 1000,
            cpu_percent=psutil.cpu_percent(),
            memory_mb=0,
            success=True,
            additional_data={"recovery_type": "file_not_found"},
        )

    def _test_invalid_input_recovery(self) -> PerformanceMetrics:
        """æµ‹è¯•æ— æ•ˆè¾“å…¥æ¢å¤"""
        start_time = time.time()

        # æµ‹è¯•Hookå¯¹æ— æ•ˆè¾“å…¥çš„å¤„ç†
        hook_files = list(self.hook_dir.glob("*.sh"))
        if hook_files:
            hook_path = hook_files[0]
            # å‘é€æ— æ•ˆJSON
            metric = self.run_command(
                f"bash {hook_path}", timeout=5, input_data="invalid json data"
            )
            metric.operation = "invalid_input_recovery"
            metric.additional_data = {"recovery_type": "invalid_input"}
            return metric

        return PerformanceMetrics(
            timestamp=start_time,
            operation="invalid_input_recovery",
            duration_ms=0,
            cpu_percent=0,
            memory_mb=0,
            success=False,
            error_message="No hooks available for testing",
        )

    def _test_memory_pressure_recovery(self) -> PerformanceMetrics:
        """æµ‹è¯•å†…å­˜å‹åŠ›æ¢å¤"""
        start_time = time.time()

        try:
            # åˆ›å»ºå†…å­˜å‹åŠ›
            data = []
            for i in range(1000):
                data.append([random.random() for _ in range(1000)])

            # ç«‹å³é‡Šæ”¾å†…å­˜
            del data
            gc.collect()

            return PerformanceMetrics(
                timestamp=start_time,
                operation="memory_pressure_recovery",
                duration_ms=(time.time() - start_time) * 1000,
                cpu_percent=psutil.cpu_percent(),
                memory_mb=0,
                success=True,
                additional_data={"recovery_type": "memory_pressure"},
            )
        except MemoryError:
            # å¦‚æœçœŸçš„å‡ºç°å†…å­˜ä¸è¶³ï¼Œä¹Ÿç®—æ˜¯æˆåŠŸæ£€æµ‹åˆ°äº†
            gc.collect()
            return PerformanceMetrics(
                timestamp=start_time,
                operation="memory_pressure_recovery",
                duration_ms=(time.time() - start_time) * 1000,
                cpu_percent=psutil.cpu_percent(),
                memory_mb=0,
                success=True,
                additional_data={"recovery_type": "memory_error_handled"},
            )

    def _analyze_performance_metrics(
        self, metrics: List[PerformanceMetrics], test_name: str
    ) -> StressTestResult:
        """åˆ†ææ€§èƒ½æŒ‡æ ‡"""
        if not metrics:
            return StressTestResult(
                test_name=test_name,
                start_time=time.time(),
                end_time=time.time(),
                total_operations=0,
                successful_operations=0,
                failed_operations=0,
                metrics=[],
                peak_memory_mb=0,
                avg_cpu_percent=0,
                error_summary={},
                performance_percentiles={},
            )

        successful_metrics = [m for m in metrics if m.success]
        failed_metrics = [m for m in metrics if not m.success]

        # è®¡ç®—æ€§èƒ½ç™¾åˆ†ä½æ•°
        durations = [m.duration_ms for m in successful_metrics]
        percentiles = {}
        if durations:
            sorted_durations = sorted(durations)
            n = len(sorted_durations)
            percentiles = {
                "p50": sorted_durations[int(n * 0.5)] if n > 0 else 0,
                "p90": sorted_durations[int(n * 0.9)] if n > 0 else 0,
                "p95": sorted_durations[int(n * 0.95)] if n > 0 else 0,
                "p99": sorted_durations[int(n * 0.99)]
                if n >= 100
                else (sorted_durations[-1] if n > 0 else 0),
                "min": min(durations) if durations else 0,
                "max": max(durations) if durations else 0,
                "avg": statistics.mean(durations) if durations else 0,
            }

        # é”™è¯¯ç»Ÿè®¡
        error_summary = defaultdict(int)
        for metric in failed_metrics:
            if metric.error_message:
                error_summary[metric.error_message] += 1

        # ç³»ç»Ÿç»Ÿè®¡
        system_stats = self.system_monitor.get_stats()

        return StressTestResult(
            test_name=test_name,
            start_time=metrics[0].timestamp if metrics else time.time(),
            end_time=metrics[-1].timestamp if metrics else time.time(),
            total_operations=len(metrics),
            successful_operations=len(successful_metrics),
            failed_operations=len(failed_metrics),
            metrics=metrics,
            peak_memory_mb=system_stats.get("peak_memory_mb", 0),
            avg_cpu_percent=system_stats.get("avg_cpu_percent", 0),
            error_summary=dict(error_summary),
            performance_percentiles=percentiles,
        )

    def test_config_loading_advanced(self):
        """é«˜çº§é…ç½®åŠ è½½æµ‹è¯•"""
        logger.info("ğŸ“‹ Testing Advanced Config Loading...")
        self.system_monitor.start_monitoring()

        all_metrics = []

        # æµ‹è¯•ä¸åŒé…ç½®æ–‡ä»¶
        config_files = [
            self.config_dir / "unified_main.yaml",
            self.config_dir / "main.yaml",
            self.project_root / ".claude" / "settings.json",
        ]

        for config_file in config_files:
            if config_file.exists():
                logger.info(f"Testing config file: {config_file.name}")

                # æµ‹è¯•å¤šæ¬¡åŠ è½½
                for i in range(50):
                    if config_file.suffix == ".yaml":
                        cmd = f"python3 -c 'import yaml; yaml.safe_load(open(\"{config_file}\"))'"
                    else:
                        cmd = f"python3 -c 'import json; json.load(open(\"{config_file}\"))'"

                    metric = self.run_command(cmd, timeout=3)
                    metric.additional_data = {
                        **(metric.additional_data or {}),
                        "config_file": config_file.name,
                        "file_type": config_file.suffix,
                    }
                    all_metrics.append(metric)

        # æµ‹è¯•é…ç½®éªŒè¯
        config_manager = self.config_dir / "config_manager.py"
        if config_manager.exists():
            logger.info("Testing config validation...")
            for i in range(10):
                metric = self.run_command(
                    f"python3 {config_manager} validate", timeout=10
                )
                metric.additional_data = {
                    **(metric.additional_data or {}),
                    "operation_type": "validation",
                }
                all_metrics.append(metric)

        self.system_monitor.stop_monitoring()

        result = self._analyze_performance_metrics(
            all_metrics, "Advanced Config Loading"
        )
        self.results["tests"]["config_loading"] = asdict(result)

        return result

    def analyze_bottlenecks(self):
        """åˆ†ææ€§èƒ½ç“¶é¢ˆï¼ˆä¸“ä¸šç‰ˆï¼‰"""
        logger.info("ğŸ” Analyzing Performance Bottlenecks...")

        bottlenecks = []

        # åˆ†æå„ä¸ªæµ‹è¯•çš„æ€§èƒ½æŒ‡æ ‡
        for test_name, test_data in self.results["tests"].items():
            if isinstance(test_data, dict) and "performance_percentiles" in test_data:
                perf = test_data["performance_percentiles"]

                # é«˜å»¶è¿Ÿæ£€æµ‹
                if perf.get("p95", 0) > 5000:  # P95 > 5ç§’
                    bottlenecks.append(
                        {
                            "type": "high_latency",
                            "test": test_name,
                            "severity": "critical",
                            "metric": f"P95 latency: {perf['p95']:.2f}ms",
                            "recommendation": "ä¼˜åŒ–ç®—æ³•å’Œå‡å°‘I/Oæ“ä½œ",
                        }
                    )
                elif perf.get("p95", 0) > 1000:  # P95 > 1ç§’
                    bottlenecks.append(
                        {
                            "type": "moderate_latency",
                            "test": test_name,
                            "severity": "warning",
                            "metric": f"P95 latency: {perf['p95']:.2f}ms",
                            "recommendation": "è€ƒè™‘æ€§èƒ½ä¼˜åŒ–",
                        }
                    )

                # æˆåŠŸç‡æ£€æµ‹
                success_rate = (
                    (
                        test_data["successful_operations"]
                        / test_data["total_operations"]
                        * 100
                    )
                    if test_data["total_operations"] > 0
                    else 0
                )
                if success_rate < 95:
                    bottlenecks.append(
                        {
                            "type": "low_success_rate",
                            "test": test_name,
                            "severity": "critical" if success_rate < 90 else "warning",
                            "metric": f"Success rate: {success_rate:.1f}%",
                            "recommendation": "å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶",
                        }
                    )

                # å†…å­˜ä½¿ç”¨æ£€æµ‹
                if test_data.get("peak_memory_mb", 0) > 500:
                    bottlenecks.append(
                        {
                            "type": "high_memory_usage",
                            "test": test_name,
                            "severity": "warning",
                            "metric": f"Peak memory: {test_data['peak_memory_mb']:.2f}MB",
                            "recommendation": "ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œå®ç°å¯¹è±¡æ± ",
                        }
                    )

                # CPUä½¿ç”¨æ£€æµ‹
                if test_data.get("avg_cpu_percent", 0) > 80:
                    bottlenecks.append(
                        {
                            "type": "high_cpu_usage",
                            "test": test_name,
                            "severity": "warning",
                            "metric": f"Avg CPU: {test_data['avg_cpu_percent']:.1f}%",
                            "recommendation": "å®ç°å¼‚æ­¥å¤„ç†å’Œè´Ÿè½½å‡è¡¡",
                        }
                    )

        self.results["bottlenecks"] = bottlenecks

        # è¾“å‡ºåˆ†æç»“æœ
        if bottlenecks:
            logger.info("Performance bottlenecks identified:")
            for bottleneck in bottlenecks:
                severity_icon = "ğŸ”´" if bottleneck["severity"] == "critical" else "ğŸŸ¡"
                logger.info(
                    f"  {severity_icon} [{bottleneck['type']}] {bottleneck['test']}: {bottleneck['metric']}"
                )
        else:
            logger.info("  âœ… No significant bottlenecks identified")

        return bottlenecks

    def generate_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®ï¼ˆä¸“ä¸šç‰ˆï¼‰"""
        recommendations = []
        bottlenecks = self.results.get("bottlenecks", [])

        # åŸºäºç“¶é¢ˆç”Ÿæˆå»ºè®®
        bottleneck_types = set(b["type"] for b in bottlenecks)

        if "high_latency" in bottleneck_types or "moderate_latency" in bottleneck_types:
            recommendations.extend(
                ["å®æ–½Hookç»“æœç¼“å­˜æœºåˆ¶", "ä¼˜åŒ–æ–‡ä»¶I/Oæ“ä½œï¼Œä½¿ç”¨æ‰¹é‡å¤„ç†", "è€ƒè™‘å¼‚æ­¥æ‰§è¡Œéå…³é”®Hook", "å®ç°Hookæ‰§è¡Œä¼˜å…ˆçº§é˜Ÿåˆ—"]
            )

        if "low_success_rate" in bottleneck_types:
            recommendations.extend(
                ["å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶", "å®ç°æ–­è·¯å™¨æ¨¡å¼é˜²æ­¢çº§è”å¤±è´¥", "æ·»åŠ è¯¦ç»†çš„é”™è¯¯æ—¥å¿—å’Œç›‘æ§", "è®¾ç½®åˆç†çš„è¶…æ—¶å’Œå›é€€ç­–ç•¥"]
            )

        if "high_memory_usage" in bottleneck_types:
            recommendations.extend(
                ["å®ç°å†…å­˜æ± å’Œå¯¹è±¡é‡ç”¨", "æ·»åŠ å†…å­˜ç›‘æ§å’Œè‡ªåŠ¨åƒåœ¾å›æ”¶", "ä¼˜åŒ–å¤§æ–‡ä»¶å¤„ç†ï¼Œä½¿ç”¨æµå¼å¤„ç†", "å®ç°å†…å­˜ä½¿ç”¨é™åˆ¶å’Œè­¦å‘Šæœºåˆ¶"]
            )

        if "high_cpu_usage" in bottleneck_types:
            recommendations.extend(
                ["å®ç°CPUå¯†é›†å‹æ“ä½œçš„è´Ÿè½½å‡è¡¡", "ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†å¹¶è¡Œä»»åŠ¡", "ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦", "è€ƒè™‘ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„"]
            )

        # é€šç”¨å»ºè®®
        recommendations.extend(
            ["å»ºç«‹æ€§èƒ½ç›‘æ§Dashboard", "å®æ–½æŒç»­æ€§èƒ½å›å½’æµ‹è¯•", "è®¾ç½®æ€§èƒ½åŸºå‡†å’ŒSLA", "å®šæœŸè¿›è¡Œæ€§èƒ½è°ƒä¼˜", "è€ƒè™‘å®æ–½A/Bæµ‹è¯•ä¼˜åŒ–ç­–ç•¥"]
        )

        self.results["recommendations"] = recommendations
        return recommendations

    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_operations = sum(
            test.get("total_operations", 0)
            for test in self.results["tests"].values()
            if isinstance(test, dict)
        )
        total_successful = sum(
            test.get("successful_operations", 0)
            for test in self.results["tests"].values()
            if isinstance(test, dict)
        )
        total_failed = sum(
            test.get("failed_operations", 0)
            for test in self.results["tests"].values()
            if isinstance(test, dict)
        )

        overall_success_rate = (
            (total_successful / total_operations * 100) if total_operations > 0 else 0
        )

        self.results["summary"] = {
            "total_tests": len(self.results["tests"]),
            "total_operations": total_operations,
            "successful_operations": total_successful,
            "failed_operations": total_failed,
            "overall_success_rate": overall_success_rate,
            "bottlenecks_found": len(self.results["bottlenecks"]),
            "critical_issues": sum(
                1 for b in self.results["bottlenecks"] if b["severity"] == "critical"
            ),
            "warning_issues": sum(
                1 for b in self.results["bottlenecks"] if b["severity"] == "warning"
            ),
            "test_duration_minutes": (
                time.time()
                - datetime.fromisoformat(self.results["timestamp"]).timestamp()
            )
            / 60,
        }

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = (
            self.project_root / f"claude_enhancer_stress_report_{timestamp}.json"
        )

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)

        # ä¿å­˜ç®€åŒ–æ‘˜è¦
        summary_file = (
            self.project_root / f"claude_enhancer_stress_summary_{timestamp}.txt"
        )
        with open(summary_file, "w", encoding="utf-8") as f:
            self._write_text_summary(f)

        logger.info(f"ğŸ“Š Comprehensive report saved to: {report_file}")
        logger.info(f"ğŸ“‹ Summary saved to: {summary_file}")

        return self.results

    def _write_text_summary(self, f):
        """å†™å…¥æ–‡æœ¬æ‘˜è¦"""
        f.write("Claude Enhancer Stress Test Report\n")
        f.write("=" * 50 + "\n\n")

        summary = self.results["summary"]
        f.write(f"Test Duration: {summary['test_duration_minutes']:.2f} minutes\n")
        f.write(f"Total Operations: {summary['total_operations']}\n")
        f.write(f"Success Rate: {summary['overall_success_rate']:.2f}%\n")
        f.write(f"Bottlenecks Found: {summary['bottlenecks_found']}\n")
        f.write(f"Critical Issues: {summary['critical_issues']}\n")
        f.write(f"Warning Issues: {summary['warning_issues']}\n\n")

        # æµ‹è¯•ç»“æœè¯¦æƒ…
        f.write("Test Results:\n")
        f.write("-" * 20 + "\n")
        for test_name, test_data in self.results["tests"].items():
            if isinstance(test_data, dict):
                f.write(f"{test_name}:\n")
                f.write(f"  Operations: {test_data.get('total_operations', 0)}\n")
                f.write(
                    f"  Success Rate: {(test_data.get('successful_operations', 0) / test_data.get('total_operations', 1) * 100):.1f}%\n"
                )

                perf = test_data.get("performance_percentiles", {})
                if perf:
                    f.write(f"  P50: {perf.get('p50', 0):.2f}ms\n")
                    f.write(f"  P95: {perf.get('p95', 0):.2f}ms\n")
                f.write("\n")

        # ç“¶é¢ˆåˆ†æ
        if self.results["bottlenecks"]:
            f.write("Performance Bottlenecks:\n")
            f.write("-" * 25 + "\n")
            for bottleneck in self.results["bottlenecks"]:
                f.write(
                    f"[{bottleneck['severity'].upper()}] {bottleneck['test']}: {bottleneck['metric']}\n"
                )
                f.write(f"  Recommendation: {bottleneck['recommendation']}\n\n")

        # å»ºè®®
        if self.results["recommendations"]:
            f.write("Recommendations:\n")
            f.write("-" * 15 + "\n")
            for i, rec in enumerate(self.results["recommendations"][:10], 1):  # å‰10ä¸ªå»ºè®®
                f.write(f"{i}. {rec}\n")

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("=" * 60)
        logger.info("ğŸš€ Claude Enhancer Comprehensive Stress Test Suite v2.0")
        logger.info("=" * 60)

        try:
            # è¿è¡Œå„é¡¹æµ‹è¯•
            self.test_hook_performance()
            self.test_agent_concurrency()
            self.test_concurrent_hook_execution()
            self.test_config_loading_advanced()
            self.test_memory_pressure()
            self.test_stability_long_running()
            self.test_error_recovery()

            # åˆ†æå’ŒæŠ¥å‘Š
            self.analyze_bottlenecks()
            self.generate_recommendations()
            report = self.generate_comprehensive_report()

            # è¾“å‡ºå…³é”®ç»“æœ
            summary = self.results["summary"]
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ¯ TEST RESULTS SUMMARY")
            logger.info("=" * 60)
            logger.info(f"â±ï¸  Duration: {summary['test_duration_minutes']:.2f} minutes")
            logger.info(f"ğŸ”¢ Total Operations: {summary['total_operations']}")
            logger.info(f"âœ… Success Rate: {summary['overall_success_rate']:.2f}%")
            logger.info(f"âš ï¸  Bottlenecks: {summary['bottlenecks_found']}")
            logger.info(f"ğŸ”´ Critical Issues: {summary['critical_issues']}")
            logger.info(f"ğŸŸ¡ Warnings: {summary['warning_issues']}")

            # æ€§èƒ½æ€»ç»“
            if self.results["tests"]:
                all_p95s = []
                for test_data in self.results["tests"].values():
                    if (
                        isinstance(test_data, dict)
                        and "performance_percentiles" in test_data
                    ):
                        p95 = test_data["performance_percentiles"].get("p95", 0)
                        if p95 > 0:
                            all_p95s.append(p95)

                if all_p95s:
                    avg_p95 = statistics.mean(all_p95s)
                    max_p95 = max(all_p95s)
                    logger.info(f"ğŸ“Š Avg P95 Latency: {avg_p95:.2f}ms")
                    logger.info(f"ğŸ“ˆ Max P95 Latency: {max_p95:.2f}ms")

            logger.info("=" * 60)

            # æ ¹æ®ç»“æœç¡®å®šé€€å‡ºçŠ¶æ€
            if summary["critical_issues"] > 0:
                logger.error("âŒ Test completed with CRITICAL issues!")
                return 1
            elif summary["overall_success_rate"] < 95:
                logger.warning("âš ï¸  Test completed with low success rate!")
                return 2
            elif summary["bottlenecks_found"] > 0:
                logger.warning("âš ï¸  Test completed with performance bottlenecks!")
                return 3
            else:
                logger.info("ğŸ‰ All tests passed successfully!")
                return 0

        except Exception as e:
            logger.error(f"Test suite failed with error: {e}")
            import traceback

            logger.debug(traceback.format_exc())
            return 1


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Claude Enhancer Comprehensive Stress Test Suite v2.0"
    )
    parser.add_argument(
        "--quick", action="store_true", help="Run quick test suite (reduced iterations)"
    )
    parser.add_argument(
        "--stability-duration",
        type=int,
        default=5,
        help="Stability test duration in minutes",
    )
    parser.add_argument(
        "--hook-iterations",
        type=int,
        default=100,
        help="Number of hook test iterations",
    )
    parser.add_argument(
        "--concurrent-levels",
        nargs="+",
        type=int,
        default=[5, 10, 20, 50],
        help="Concurrency levels to test",
    )

    args = parser.parse_args()

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ClaudeEnhancerStressTest()

    # è°ƒæ•´æµ‹è¯•é…ç½®
    if args.quick:
        tester.test_config.update(
            {
                "hook_iterations": 20,
                "concurrent_levels": [5, 10],
                "memory_test_size_mb": [1, 10],
                "stability_duration_minutes": 1,
            }
        )
        logger.info("Running in quick test mode")
    else:
        tester.test_config.update(
            {
                "hook_iterations": args.hook_iterations,
                "concurrent_levels": args.concurrent_levels,
                "stability_duration_minutes": args.stability_duration,
            }
        )

    # è¿è¡Œæµ‹è¯•å¹¶è¿”å›é€€å‡ºç 
    try:
        exit_code = tester.run_all_tests()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        logger.info("Test interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
